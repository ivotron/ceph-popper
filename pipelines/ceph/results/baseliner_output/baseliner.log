
2018-06-04 19:20:00,460 p=10 u=root |  PLAY [all] *********************************************************************
2018-06-04 19:20:00,466 p=10 u=root |  TASK [Gathering Facts] *********************************************************
2018-06-04 19:20:00,466 p=10 u=root |  Monday 04 June 2018  19:20:00 +0000 (0:00:00.041)       0:00:00.041 *********** 
2018-06-04 19:20:03,446 p=10 u=root |  ok: [node1]
2018-06-04 19:20:03,455 p=10 u=root |  ok: [node3]
2018-06-04 19:20:04,439 p=10 u=root |  ok: [node2]
2018-06-04 19:20:04,445 p=10 u=root |  TASK [baseliner : include_tasks] ***********************************************
2018-06-04 19:20:04,445 p=10 u=root |  Monday 04 June 2018  19:20:04 +0000 (0:00:03.979)       0:00:04.020 *********** 
2018-06-04 19:20:04,507 p=10 u=root |  included: /etc/ansible/roles/baseliner/tasks/check_preconditions.yml for node1, node2, node3
2018-06-04 19:20:04,515 p=10 u=root |  TASK [baseliner : ensure expected variables are defined] ***********************
2018-06-04 19:20:04,516 p=10 u=root |  Monday 04 June 2018  19:20:04 +0000 (0:00:00.070)       0:00:04.091 *********** 
2018-06-04 19:20:04,570 p=10 u=root |  ok: [node1] => {
    "changed": false, 
    "failed": false, 
    "msg": "All assertions passed"
}
2018-06-04 19:20:04,581 p=10 u=root |  ok: [node2] => {
    "changed": false, 
    "failed": false, 
    "msg": "All assertions passed"
}
2018-06-04 19:20:04,589 p=10 u=root |  ok: [node3] => {
    "changed": false, 
    "failed": false, 
    "msg": "All assertions passed"
}
2018-06-04 19:20:04,595 p=10 u=root |  TASK [baseliner : set remote results path if not defined] **********************
2018-06-04 19:20:04,596 p=10 u=root |  Monday 04 June 2018  19:20:04 +0000 (0:00:00.080)       0:00:04.171 *********** 
2018-06-04 19:20:04,653 p=10 u=root |  ok: [node1]
2018-06-04 19:20:04,661 p=10 u=root |  ok: [node2]
2018-06-04 19:20:04,669 p=10 u=root |  ok: [node3]
2018-06-04 19:20:04,678 p=10 u=root |  TASK [baseliner : ensure results folder exists] ********************************
2018-06-04 19:20:04,679 p=10 u=root |  Monday 04 June 2018  19:20:04 +0000 (0:00:00.082)       0:00:04.254 *********** 
2018-06-04 19:20:04,900 p=10 u=root |  ok: [node1 -> localhost]
2018-06-04 19:20:04,910 p=10 u=root |  TASK [baseliner : check that pre-tasks file exists] ****************************
2018-06-04 19:20:04,911 p=10 u=root |  Monday 04 June 2018  19:20:04 +0000 (0:00:00.231)       0:00:04.486 *********** 
2018-06-04 19:20:04,933 p=10 u=root |  TASK [baseliner : check that post-tasks file exists] ***************************
2018-06-04 19:20:04,933 p=10 u=root |  Monday 04 June 2018  19:20:04 +0000 (0:00:00.022)       0:00:04.508 *********** 
2018-06-04 19:20:04,969 p=10 u=root |  TASK [baseliner : ensure docker-engine is installed] ***************************
2018-06-04 19:20:04,969 p=10 u=root |  Monday 04 June 2018  19:20:04 +0000 (0:00:00.036)       0:00:04.545 *********** 
2018-06-04 19:20:05,004 p=10 u=root |  TASK [baseliner : install statically linked docker] ****************************
2018-06-04 19:20:05,004 p=10 u=root |  Monday 04 June 2018  19:20:05 +0000 (0:00:00.034)       0:00:04.579 *********** 
2018-06-04 19:20:05,037 p=10 u=root |  TASK [baseliner : ensure the docker daemon is running] *************************
2018-06-04 19:20:05,038 p=10 u=root |  Monday 04 June 2018  19:20:05 +0000 (0:00:00.033)       0:00:04.613 *********** 
2018-06-04 19:20:05,071 p=10 u=root |  TASK [baseliner : stop any running container] **********************************
2018-06-04 19:20:05,072 p=10 u=root |  Monday 04 June 2018  19:20:05 +0000 (0:00:00.034)       0:00:04.647 *********** 
2018-06-04 19:20:06,037 p=10 u=root |  fatal: [node2]: FAILED! => {"changed": true, "cmd": "docker stop `docker ps -q`", "delta": "0:00:00.033956", "end": "2018-06-04 13:20:05.996345", "failed": true, "msg": "non-zero return code", "rc": 1, "start": "2018-06-04 13:20:05.962389", "stderr": "\"docker stop\" requires at least 1 argument(s).\nSee 'docker stop --help'.\n\nUsage:  docker stop [OPTIONS] CONTAINER [CONTAINER...]\n\nStop one or more running containers", "stderr_lines": ["\"docker stop\" requires at least 1 argument(s).", "See 'docker stop --help'.", "", "Usage:  docker stop [OPTIONS] CONTAINER [CONTAINER...]", "", "Stop one or more running containers"], "stdout": "", "stdout_lines": []}
2018-06-04 19:20:06,037 p=10 u=root |  ...ignoring
2018-06-04 19:20:06,044 p=10 u=root |  fatal: [node1]: FAILED! => {"changed": true, "cmd": "docker stop `docker ps -q`", "delta": "0:00:00.042777", "end": "2018-06-04 13:20:06.004495", "failed": true, "msg": "non-zero return code", "rc": 1, "start": "2018-06-04 13:20:05.961718", "stderr": "\"docker stop\" requires at least 1 argument(s).\nSee 'docker stop --help'.\n\nUsage:  docker stop [OPTIONS] CONTAINER [CONTAINER...]\n\nStop one or more running containers", "stderr_lines": ["\"docker stop\" requires at least 1 argument(s).", "See 'docker stop --help'.", "", "Usage:  docker stop [OPTIONS] CONTAINER [CONTAINER...]", "", "Stop one or more running containers"], "stdout": "", "stdout_lines": []}
2018-06-04 19:20:06,045 p=10 u=root |  ...ignoring
2018-06-04 19:20:06,045 p=10 u=root |  fatal: [node3]: FAILED! => {"changed": true, "cmd": "docker stop `docker ps -q`", "delta": "0:00:00.039976", "end": "2018-06-04 13:20:06.005179", "failed": true, "msg": "non-zero return code", "rc": 1, "start": "2018-06-04 13:20:05.965203", "stderr": "\"docker stop\" requires at least 1 argument(s).\nSee 'docker stop --help'.\n\nUsage:  docker stop [OPTIONS] CONTAINER [CONTAINER...]\n\nStop one or more running containers", "stderr_lines": ["\"docker stop\" requires at least 1 argument(s).", "See 'docker stop --help'.", "", "Usage:  docker stop [OPTIONS] CONTAINER [CONTAINER...]", "", "Stop one or more running containers"], "stdout": "", "stdout_lines": []}
2018-06-04 19:20:06,045 p=10 u=root |  ...ignoring
2018-06-04 19:20:06,054 p=10 u=root |  TASK [baseliner : remove containers to avoid name clashes] *********************
2018-06-04 19:20:06,054 p=10 u=root |  Monday 04 June 2018  19:20:06 +0000 (0:00:00.982)       0:00:05.629 *********** 
2018-06-04 19:20:06,932 p=10 u=root |  fatal: [node2]: FAILED! => {"changed": true, "cmd": "docker rm `docker ps -qa`", "delta": "0:00:00.039514", "end": "2018-06-04 13:20:06.887926", "failed": true, "msg": "non-zero return code", "rc": 1, "start": "2018-06-04 13:20:06.848412", "stderr": "\"docker rm\" requires at least 1 argument(s).\nSee 'docker rm --help'.\n\nUsage:  docker rm [OPTIONS] CONTAINER [CONTAINER...]\n\nRemove one or more containers", "stderr_lines": ["\"docker rm\" requires at least 1 argument(s).", "See 'docker rm --help'.", "", "Usage:  docker rm [OPTIONS] CONTAINER [CONTAINER...]", "", "Remove one or more containers"], "stdout": "", "stdout_lines": []}
2018-06-04 19:20:06,933 p=10 u=root |  ...ignoring
2018-06-04 19:20:06,934 p=10 u=root |  fatal: [node3]: FAILED! => {"changed": true, "cmd": "docker rm `docker ps -qa`", "delta": "0:00:00.040450", "end": "2018-06-04 13:20:06.893449", "failed": true, "msg": "non-zero return code", "rc": 1, "start": "2018-06-04 13:20:06.852999", "stderr": "\"docker rm\" requires at least 1 argument(s).\nSee 'docker rm --help'.\n\nUsage:  docker rm [OPTIONS] CONTAINER [CONTAINER...]\n\nRemove one or more containers", "stderr_lines": ["\"docker rm\" requires at least 1 argument(s).", "See 'docker rm --help'.", "", "Usage:  docker rm [OPTIONS] CONTAINER [CONTAINER...]", "", "Remove one or more containers"], "stdout": "", "stdout_lines": []}
2018-06-04 19:20:06,935 p=10 u=root |  ...ignoring
2018-06-04 19:20:06,936 p=10 u=root |  fatal: [node1]: FAILED! => {"changed": true, "cmd": "docker rm `docker ps -qa`", "delta": "0:00:00.042708", "end": "2018-06-04 13:20:06.891993", "failed": true, "msg": "non-zero return code", "rc": 1, "start": "2018-06-04 13:20:06.849285", "stderr": "\"docker rm\" requires at least 1 argument(s).\nSee 'docker rm --help'.\n\nUsage:  docker rm [OPTIONS] CONTAINER [CONTAINER...]\n\nRemove one or more containers", "stderr_lines": ["\"docker rm\" requires at least 1 argument(s).", "See 'docker rm --help'.", "", "Usage:  docker rm [OPTIONS] CONTAINER [CONTAINER...]", "", "Remove one or more containers"], "stdout": "", "stdout_lines": []}
2018-06-04 19:20:06,936 p=10 u=root |  ...ignoring
2018-06-04 19:20:06,944 p=10 u=root |  TASK [baseliner : store facts] *************************************************
2018-06-04 19:20:06,944 p=10 u=root |  Monday 04 June 2018  19:20:06 +0000 (0:00:00.890)       0:00:06.520 *********** 
2018-06-04 19:20:07,001 p=10 u=root |  included: /etc/ansible/roles/baseliner/tasks/store-facts.yml for node1, node2, node3
2018-06-04 19:20:07,016 p=10 u=root |  TASK [baseliner : install facter] **********************************************
2018-06-04 19:20:07,017 p=10 u=root |  Monday 04 June 2018  19:20:07 +0000 (0:00:00.072)       0:00:06.592 *********** 
2018-06-04 19:20:08,536 p=10 u=root |  ok: [node1]
2018-06-04 19:20:08,543 p=10 u=root |  ok: [node2]
2018-06-04 19:20:08,543 p=10 u=root |  ok: [node3]
2018-06-04 19:20:08,555 p=10 u=root |  TASK [baseliner : create facts folder if it doesn't exist] *********************
2018-06-04 19:20:08,555 p=10 u=root |  Monday 04 June 2018  19:20:08 +0000 (0:00:01.538)       0:00:08.130 *********** 
2018-06-04 19:20:08,661 p=10 u=root |  changed: [node1 -> localhost]
2018-06-04 19:20:08,671 p=10 u=root |  TASK [baseliner : store facts about remotes] ***********************************
2018-06-04 19:20:08,671 p=10 u=root |  Monday 04 June 2018  19:20:08 +0000 (0:00:00.116)       0:00:08.247 *********** 
2018-06-04 19:20:09,046 p=10 u=root |  changed: [node1 -> localhost] => (item=node1)
2018-06-04 19:20:09,240 p=10 u=root |  changed: [node1 -> localhost] => (item=node2)
2018-06-04 19:20:09,446 p=10 u=root |  changed: [node1 -> localhost] => (item=node3)
2018-06-04 19:20:09,455 p=10 u=root |  TASK [baseliner : start monitoring] ********************************************
2018-06-04 19:20:09,455 p=10 u=root |  Monday 04 June 2018  19:20:09 +0000 (0:00:00.783)       0:00:09.030 *********** 
2018-06-04 19:20:09,493 p=10 u=root |  TASK [baseliner : include_tasks] ***********************************************
2018-06-04 19:20:09,493 p=10 u=root |  Monday 04 June 2018  19:20:09 +0000 (0:00:00.038)       0:00:09.069 *********** 
2018-06-04 19:20:09,529 p=10 u=root |  TASK [baseliner : get number of repetitions] ***********************************
2018-06-04 19:20:09,529 p=10 u=root |  Monday 04 June 2018  19:20:09 +0000 (0:00:00.035)       0:00:09.105 *********** 
2018-06-04 19:20:09,584 p=10 u=root |  ok: [node1] => (item=1)
2018-06-04 19:20:09,592 p=10 u=root |  ok: [node2] => (item=1)
2018-06-04 19:20:09,601 p=10 u=root |  ok: [node3] => (item=1)
2018-06-04 19:20:09,607 p=10 u=root |  TASK [baseliner : execute each benchmark] **************************************
2018-06-04 19:20:09,607 p=10 u=root |  Monday 04 June 2018  19:20:09 +0000 (0:00:00.077)       0:00:09.182 *********** 
2018-06-04 19:20:09,668 p=10 u=root |  included: /etc/ansible/roles/baseliner/tasks/run-bench.yml for node1, node2, node3
2018-06-04 19:20:09,691 p=10 u=root |  TASK [baseliner : upload files] ************************************************
2018-06-04 19:20:09,691 p=10 u=root |  Monday 04 June 2018  19:20:09 +0000 (0:00:00.083)       0:00:09.266 *********** 
2018-06-04 19:20:11,197 p=10 u=root |  changed: [node3] => (item={u'dest': u'/tmp/etc/ceph.conf', u'src': u'/experiment/scripts/docker-bench/ceph/etcceph/ceph.conf'})
2018-06-04 19:20:11,199 p=10 u=root |  changed: [node2] => (item={u'dest': u'/tmp/etc/ceph.conf', u'src': u'/experiment/scripts/docker-bench/ceph/etcceph/ceph.conf'})
2018-06-04 19:20:11,202 p=10 u=root |  changed: [node1] => (item={u'dest': u'/tmp/etc/ceph.conf', u'src': u'/experiment/scripts/docker-bench/ceph/etcceph/ceph.conf'})
2018-06-04 19:20:11,208 p=10 u=root |  TASK [baseliner : initialize parameters and add number of repetitions] *********
2018-06-04 19:20:11,209 p=10 u=root |  Monday 04 June 2018  19:20:11 +0000 (0:00:01.517)       0:00:10.784 *********** 
2018-06-04 19:20:11,261 p=10 u=root |  ok: [node1]
2018-06-04 19:20:11,269 p=10 u=root |  ok: [node2]
2018-06-04 19:20:11,278 p=10 u=root |  ok: [node3]
2018-06-04 19:20:11,285 p=10 u=root |  TASK [baseliner : unnest parameters when parameters for benchmark were passed] ***
2018-06-04 19:20:11,286 p=10 u=root |  Monday 04 June 2018  19:20:11 +0000 (0:00:00.077)       0:00:10.861 *********** 
2018-06-04 19:20:11,320 p=10 u=root |  TASK [baseliner : parametrized execution] **************************************
2018-06-04 19:20:11,320 p=10 u=root |  Monday 04 June 2018  19:20:11 +0000 (0:00:00.034)       0:00:10.895 *********** 
2018-06-04 19:20:11,394 p=10 u=root |  included: /etc/ansible/roles/baseliner/tasks/run-bench-parametrized.yml for node1, node2, node3
2018-06-04 19:20:11,402 p=10 u=root |  TASK [baseliner : remove remote results folder] ********************************
2018-06-04 19:20:11,402 p=10 u=root |  Monday 04 June 2018  19:20:11 +0000 (0:00:00.082)       0:00:10.977 *********** 
2018-06-04 19:20:12,236 p=10 u=root |  ok: [node2]
2018-06-04 19:20:12,237 p=10 u=root |  ok: [node3]
2018-06-04 19:20:12,237 p=10 u=root |  ok: [node1]
2018-06-04 19:20:12,247 p=10 u=root |  TASK [baseliner : create remote results folder] ********************************
2018-06-04 19:20:12,247 p=10 u=root |  Monday 04 June 2018  19:20:12 +0000 (0:00:00.844)       0:00:11.822 *********** 
2018-06-04 19:20:13,049 p=10 u=root |  changed: [node1]
2018-06-04 19:20:13,054 p=10 u=root |  changed: [node2]
2018-06-04 19:20:13,057 p=10 u=root |  changed: [node3]
2018-06-04 19:20:13,064 p=10 u=root |  TASK [baseliner : initialize parameter dictionary] *****************************
2018-06-04 19:20:13,064 p=10 u=root |  Monday 04 June 2018  19:20:13 +0000 (0:00:00.816)       0:00:12.639 *********** 
2018-06-04 19:20:13,125 p=10 u=root |  ok: [node1]
2018-06-04 19:20:13,129 p=10 u=root |  ok: [node2]
2018-06-04 19:20:13,141 p=10 u=root |  ok: [node3]
2018-06-04 19:20:13,147 p=10 u=root |  TASK [baseliner : populate parameter dictionary] *******************************
2018-06-04 19:20:13,147 p=10 u=root |  Monday 04 June 2018  19:20:13 +0000 (0:00:00.083)       0:00:12.723 *********** 
2018-06-04 19:20:13,207 p=10 u=root |  ok: [node1] => (item=[u'repetition', u'1'])
2018-06-04 19:20:13,215 p=10 u=root |  ok: [node2] => (item=[u'repetition', u'1'])
2018-06-04 19:20:13,230 p=10 u=root |  ok: [node3] => (item=[u'repetition', u'1'])
2018-06-04 19:20:13,242 p=10 u=root |  TASK [baseliner : merge default variables to the benchmark-specific options] ***
2018-06-04 19:20:13,242 p=10 u=root |  Monday 04 June 2018  19:20:13 +0000 (0:00:00.094)       0:00:12.817 *********** 
2018-06-04 19:20:13,295 p=10 u=root |  ok: [node1]
2018-06-04 19:20:13,309 p=10 u=root |  ok: [node2]
2018-06-04 19:20:13,314 p=10 u=root |  ok: [node3]
2018-06-04 19:20:13,323 p=10 u=root |  TASK [baseliner : initialize string for path] **********************************
2018-06-04 19:20:13,324 p=10 u=root |  Monday 04 June 2018  19:20:13 +0000 (0:00:00.081)       0:00:12.899 *********** 
2018-06-04 19:20:13,376 p=10 u=root |  ok: [node1]
2018-06-04 19:20:13,416 p=10 u=root |  ok: [node2]
2018-06-04 19:20:13,424 p=10 u=root |  ok: [node3]
2018-06-04 19:20:13,430 p=10 u=root |  TASK [baseliner : add key/value pairs for each parameter (if any)] *************
2018-06-04 19:20:13,430 p=10 u=root |  Monday 04 June 2018  19:20:13 +0000 (0:00:00.106)       0:00:13.005 *********** 
2018-06-04 19:20:13,488 p=10 u=root |  ok: [node1] => (item={'key': u'repetition', 'value': u'1'})
2018-06-04 19:20:13,494 p=10 u=root |  ok: [node2] => (item={'key': u'repetition', 'value': u'1'})
2018-06-04 19:20:13,505 p=10 u=root |  ok: [node3] => (item={'key': u'repetition', 'value': u'1'})
2018-06-04 19:20:13,513 p=10 u=root |  TASK [baseliner : ensure output folder exists] *********************************
2018-06-04 19:20:13,513 p=10 u=root |  Monday 04 June 2018  19:20:13 +0000 (0:00:00.083)       0:00:13.088 *********** 
2018-06-04 19:20:13,638 p=10 u=root |  changed: [node1 -> localhost]
2018-06-04 19:20:13,655 p=10 u=root |  changed: [node3 -> localhost]
2018-06-04 19:20:13,657 p=10 u=root |  changed: [node2 -> localhost]
2018-06-04 19:20:13,663 p=10 u=root |  TASK [baseliner : run containerized benchmark] *********************************
2018-06-04 19:20:13,663 p=10 u=root |  Monday 04 June 2018  19:20:13 +0000 (0:00:00.149)       0:00:13.238 *********** 
2018-06-04 19:20:13,788 p=10 u=root |  included: /etc/ansible/roles/baseliner/tasks/run-container.yml for node1, node2, node3
2018-06-04 19:20:13,809 p=10 u=root |  TASK [baseliner : check if image already exists] *******************************
2018-06-04 19:20:13,810 p=10 u=root |  Monday 04 June 2018  19:20:13 +0000 (0:00:00.146)       0:00:13.385 *********** 
2018-06-04 19:20:14,673 p=10 u=root |  changed: [node3]
2018-06-04 19:20:14,675 p=10 u=root |  changed: [node1]
2018-06-04 19:20:14,676 p=10 u=root |  changed: [node2]
2018-06-04 19:20:14,687 p=10 u=root |  TASK [baseliner : pull image] **************************************************
2018-06-04 19:20:14,687 p=10 u=root |  Monday 04 June 2018  19:20:14 +0000 (0:00:00.877)       0:00:14.262 *********** 
2018-06-04 19:20:16,248 p=10 u=root |  changed: [node1]
2018-06-04 19:20:16,249 p=10 u=root |  changed: [node3]
2018-06-04 19:20:16,362 p=10 u=root |  changed: [node2]
2018-06-04 19:20:16,372 p=10 u=root |  TASK [baseliner : define name of container] ************************************
2018-06-04 19:20:16,373 p=10 u=root |  Monday 04 June 2018  19:20:16 +0000 (0:00:01.685)       0:00:15.948 *********** 
2018-06-04 19:20:16,448 p=10 u=root |  ok: [node1]
2018-06-04 19:20:16,462 p=10 u=root |  ok: [node2]
2018-06-04 19:20:16,474 p=10 u=root |  ok: [node3]
2018-06-04 19:20:16,481 p=10 u=root |  TASK [baseliner : initialize docker_flags variable] ****************************
2018-06-04 19:20:16,482 p=10 u=root |  Monday 04 June 2018  19:20:16 +0000 (0:00:00.109)       0:00:16.057 *********** 
2018-06-04 19:20:16,556 p=10 u=root |  ok: [node1]
2018-06-04 19:20:16,575 p=10 u=root |  ok: [node2]
2018-06-04 19:20:16,586 p=10 u=root |  ok: [node3]
2018-06-04 19:20:16,598 p=10 u=root |  TASK [baseliner : add environment to docker_flags] *****************************
2018-06-04 19:20:16,599 p=10 u=root |  Monday 04 June 2018  19:20:16 +0000 (0:00:00.117)       0:00:16.174 *********** 
2018-06-04 19:20:16,660 p=10 u=root |  TASK [baseliner : add implicit environment from bench_params variable] *********
2018-06-04 19:20:16,660 p=10 u=root |  Monday 04 June 2018  19:20:16 +0000 (0:00:00.061)       0:00:16.236 *********** 
2018-06-04 19:20:16,749 p=10 u=root |  ok: [node1] => (item={'key': u'repetition', 'value': u'1'})
2018-06-04 19:20:16,790 p=10 u=root |  ok: [node2] => (item={'key': u'repetition', 'value': u'1'})
2018-06-04 19:20:16,800 p=10 u=root |  ok: [node3] => (item={'key': u'repetition', 'value': u'1'})
2018-06-04 19:20:16,811 p=10 u=root |  TASK [baseliner : check if we have host-specific ips] **************************
2018-06-04 19:20:16,811 p=10 u=root |  Monday 04 June 2018  19:20:16 +0000 (0:00:00.151)       0:00:16.387 *********** 
2018-06-04 19:20:16,866 p=10 u=root |  ok: [node1]
2018-06-04 19:20:16,885 p=10 u=root |  ok: [node2]
2018-06-04 19:20:16,894 p=10 u=root |  ok: [node3]
2018-06-04 19:20:16,903 p=10 u=root |  TASK [baseliner : add host-specific ips to docker_flags] ***********************
2018-06-04 19:20:16,903 p=10 u=root |  Monday 04 June 2018  19:20:16 +0000 (0:00:00.091)       0:00:16.478 *********** 
2018-06-04 19:20:16,938 p=10 u=root |  TASK [baseliner : check if we have host-specific environment] ******************
2018-06-04 19:20:16,938 p=10 u=root |  Monday 04 June 2018  19:20:16 +0000 (0:00:00.035)       0:00:16.513 *********** 
2018-06-04 19:20:16,989 p=10 u=root |  ok: [node1]
2018-06-04 19:20:16,997 p=10 u=root |  ok: [node2]
2018-06-04 19:20:17,006 p=10 u=root |  ok: [node3]
2018-06-04 19:20:17,012 p=10 u=root |  TASK [baseliner : add host-specific environment to docker_flags] ***************
2018-06-04 19:20:17,012 p=10 u=root |  Monday 04 June 2018  19:20:17 +0000 (0:00:00.074)       0:00:16.587 *********** 
2018-06-04 19:20:17,070 p=10 u=root |  ok: [node1] => (item={'key': u'CEPH_PUBLIC_NETWORK', 'value': u'128.110.152.0/22'})
2018-06-04 19:20:17,081 p=10 u=root |  ok: [node2] => (item={'key': u'CEPH_PUBLIC_NETWORK', 'value': u'128.110.152.0/22'})
2018-06-04 19:20:17,094 p=10 u=root |  ok: [node3] => (item={'key': u'CEPH_PUBLIC_NETWORK', 'value': u'128.110.152.0/22'})
2018-06-04 19:20:17,113 p=10 u=root |  ok: [node1] => (item={'key': u'MON_IP', 'value': u'128.110.153.197'})
2018-06-04 19:20:17,129 p=10 u=root |  ok: [node2] => (item={'key': u'MON_IP', 'value': u'128.110.153.197'})
2018-06-04 19:20:17,135 p=10 u=root |  ok: [node3] => (item={'key': u'CLIENT', 'value': True})
2018-06-04 19:20:17,152 p=10 u=root |  ok: [node1] => (item={'key': u'MONITOR', 'value': True})
2018-06-04 19:20:17,168 p=10 u=root |  ok: [node2] => (item={'key': u'OSD', 'value': True})
2018-06-04 19:20:17,173 p=10 u=root |  ok: [node3] => (item={'key': u'MON_IP', 'value': u'128.110.153.197'})
2018-06-04 19:20:17,237 p=10 u=root |  ok: [node2] => (item={'key': u'OSD_DEVICES', 'value': u'/dev/sdb'})
2018-06-04 19:20:17,243 p=10 u=root |  TASK [baseliner : add devices to docker_flags] *********************************
2018-06-04 19:20:17,244 p=10 u=root |  Monday 04 June 2018  19:20:17 +0000 (0:00:00.231)       0:00:16.819 *********** 
2018-06-04 19:20:17,285 p=10 u=root |  TASK [baseliner : add network mode to docker_flags] ****************************
2018-06-04 19:20:17,285 p=10 u=root |  Monday 04 June 2018  19:20:17 +0000 (0:00:00.041)       0:00:16.860 *********** 
2018-06-04 19:20:17,364 p=10 u=root |  ok: [node1]
2018-06-04 19:20:17,379 p=10 u=root |  ok: [node2]
2018-06-04 19:20:17,385 p=10 u=root |  ok: [node3]
2018-06-04 19:20:17,396 p=10 u=root |  TASK [baseliner : add ipc mode to docker_flags] ********************************
2018-06-04 19:20:17,396 p=10 u=root |  Monday 04 June 2018  19:20:17 +0000 (0:00:00.110)       0:00:16.971 *********** 
2018-06-04 19:20:17,431 p=10 u=root |  TASK [baseliner : add ports to docker_flags] ***********************************
2018-06-04 19:20:17,431 p=10 u=root |  Monday 04 June 2018  19:20:17 +0000 (0:00:00.035)       0:00:17.006 *********** 
2018-06-04 19:20:17,471 p=10 u=root |  TASK [baseliner : add volumes to docker_flags] *********************************
2018-06-04 19:20:17,471 p=10 u=root |  Monday 04 June 2018  19:20:17 +0000 (0:00:00.040)       0:00:17.047 *********** 
2018-06-04 19:20:17,526 p=10 u=root |  ok: [node1] => (item=/tmp/etc:/etc/ceph)
2018-06-04 19:20:17,534 p=10 u=root |  ok: [node2] => (item=/tmp/etc:/etc/ceph)
2018-06-04 19:20:17,545 p=10 u=root |  ok: [node3] => (item=/tmp/etc:/etc/ceph)
2018-06-04 19:20:17,552 p=10 u=root |  TASK [baseliner : add default volumes to docker_flags] *************************
2018-06-04 19:20:17,552 p=10 u=root |  Monday 04 June 2018  19:20:17 +0000 (0:00:00.080)       0:00:17.127 *********** 
2018-06-04 19:20:17,604 p=10 u=root |  ok: [node1]
2018-06-04 19:20:17,615 p=10 u=root |  ok: [node2]
2018-06-04 19:20:17,625 p=10 u=root |  ok: [node3]
2018-06-04 19:20:17,631 p=10 u=root |  TASK [baseliner : set entrypoint] **********************************************
2018-06-04 19:20:17,631 p=10 u=root |  Monday 04 June 2018  19:20:17 +0000 (0:00:00.078)       0:00:17.206 *********** 
2018-06-04 19:20:17,667 p=10 u=root |  TASK [baseliner : set limits] **************************************************
2018-06-04 19:20:17,667 p=10 u=root |  Monday 04 June 2018  19:20:17 +0000 (0:00:00.035)       0:00:17.242 *********** 
2018-06-04 19:20:17,700 p=10 u=root |  TASK [baseliner : add capabilities] ********************************************
2018-06-04 19:20:17,700 p=10 u=root |  Monday 04 June 2018  19:20:17 +0000 (0:00:00.033)       0:00:17.275 *********** 
2018-06-04 19:20:17,735 p=10 u=root |  TASK [baseliner : remove capabilities] *****************************************
2018-06-04 19:20:17,735 p=10 u=root |  Monday 04 June 2018  19:20:17 +0000 (0:00:00.034)       0:00:17.310 *********** 
2018-06-04 19:20:17,770 p=10 u=root |  TASK [baseliner : set privileged mode] *****************************************
2018-06-04 19:20:17,770 p=10 u=root |  Monday 04 June 2018  19:20:17 +0000 (0:00:00.035)       0:00:17.346 *********** 
2018-06-04 19:20:17,815 p=10 u=root |  TASK [baseliner : set memory constraint] ***************************************
2018-06-04 19:20:17,816 p=10 u=root |  Monday 04 June 2018  19:20:17 +0000 (0:00:00.045)       0:00:17.391 *********** 
2018-06-04 19:20:17,859 p=10 u=root |  TASK [baseliner : set memory-swap constraint] **********************************
2018-06-04 19:20:17,860 p=10 u=root |  Monday 04 June 2018  19:20:17 +0000 (0:00:00.043)       0:00:17.435 *********** 
2018-06-04 19:20:17,908 p=10 u=root |  TASK [baseliner : set parent cgroup] *******************************************
2018-06-04 19:20:17,908 p=10 u=root |  Monday 04 June 2018  19:20:17 +0000 (0:00:00.048)       0:00:17.484 *********** 
2018-06-04 19:20:17,952 p=10 u=root |  TASK [baseliner : debug] *******************************************************
2018-06-04 19:20:17,952 p=10 u=root |  Monday 04 June 2018  19:20:17 +0000 (0:00:00.043)       0:00:17.528 *********** 
2018-06-04 19:20:18,016 p=10 u=root |  ok: [node1] => {
    "msg": "docker run --rm  -e repetition=\"1\" -e CEPH_PUBLIC_NETWORK=\"128.110.152.0/22\" -e MON_IP=\"128.110.153.197\" -e MONITOR=\"True\" --net=host -v /tmp/etc:/etc/ceph -v /tmp/baseliner_output_node1:/results -v /tmp/baseliner_output_node1:/output mariettesouppe/rados_bench:master-ec8d33f-luminous-ubuntu-16.04-x86_64 "
}
2018-06-04 19:20:18,030 p=10 u=root |  ok: [node2] => {
    "msg": "docker run --rm  -e repetition=\"1\" -e CEPH_PUBLIC_NETWORK=\"128.110.152.0/22\" -e MON_IP=\"128.110.153.197\" -e OSD=\"True\" -e OSD_DEVICES=\"/dev/sdb\" --net=host -v /tmp/etc:/etc/ceph -v /tmp/baseliner_output_node2:/results -v /tmp/baseliner_output_node2:/output mariettesouppe/rados_bench:master-ec8d33f-luminous-ubuntu-16.04-x86_64 "
}
2018-06-04 19:20:18,046 p=10 u=root |  ok: [node3] => {
    "msg": "docker run --rm  -e repetition=\"1\" -e CEPH_PUBLIC_NETWORK=\"128.110.152.0/22\" -e CLIENT=\"True\" -e MON_IP=\"128.110.153.197\" --net=host -v /tmp/etc:/etc/ceph -v /tmp/baseliner_output_node3:/results -v /tmp/baseliner_output_node3:/output mariettesouppe/rados_bench:master-ec8d33f-luminous-ubuntu-16.04-x86_64 "
}
2018-06-04 19:20:18,055 p=10 u=root |  TASK [baseliner : run container] ***********************************************
2018-06-04 19:20:18,056 p=10 u=root |  Monday 04 June 2018  19:20:18 +0000 (0:00:00.103)       0:00:17.631 *********** 
2018-06-04 19:20:20,086 p=10 u=root |  changed: [node1]
2018-06-04 19:20:20,108 p=10 u=root |  changed: [node2]
2018-06-04 19:20:20,114 p=10 u=root |  changed: [node3]
2018-06-04 19:20:20,131 p=10 u=root |  TASK [baseliner : wait for containers in parallel mode and stop/kill them if they timeout] ***
2018-06-04 19:20:20,131 p=10 u=root |  Monday 04 June 2018  19:20:20 +0000 (0:00:02.075)       0:00:19.706 *********** 
2018-06-04 19:20:20,158 p=10 u=root |  TASK [baseliner : wait for containers in single-node mode and stop/kill them if they timeout] ***
2018-06-04 19:20:20,158 p=10 u=root |  Monday 04 June 2018  19:20:20 +0000 (0:00:00.027)       0:00:19.733 *********** 
2018-06-04 21:00:39,643 p=10 u=root |  changed: [node1 -> localhost]
2018-06-04 21:00:39,653 p=10 u=root |  TASK [baseliner : get wait result] *********************************************
2018-06-04 21:00:39,653 p=10 u=root |  Monday 04 June 2018  21:00:39 +0000 (1:40:19.495)       1:40:39.229 *********** 
2018-06-04 21:00:39,673 p=10 u=root |  ok: [node1 -> localhost]
2018-06-04 21:00:39,681 p=10 u=root |  TASK [baseliner : debug] *******************************************************
2018-06-04 21:00:39,682 p=10 u=root |  Monday 04 June 2018  21:00:39 +0000 (0:00:00.028)       1:40:39.257 *********** 
2018-06-04 21:00:39,699 p=10 u=root |  ok: [node1 -> localhost] => {
    "msg": "WAIT_FOR_RESULT:  WARNING: timeout waiting for 3 hosts"
}
2018-06-04 21:00:39,706 p=10 u=root |  TASK [baseliner : get result of container execution] ***************************
2018-06-04 21:00:39,706 p=10 u=root |  Monday 04 June 2018  21:00:39 +0000 (0:00:00.024)       1:40:39.281 *********** 
2018-06-04 21:00:40,638 p=10 u=root |  fatal: [node1]: FAILED! => {"ansible_job_id": "883816749731.14570", "changed": true, "cmd": "docker run --rm --name baseliner_node1  -e repetition=\"1\" -e CEPH_PUBLIC_NETWORK=\"128.110.152.0/22\" -e MON_IP=\"128.110.153.197\" -e MONITOR=\"True\" --net=host -v /tmp/etc:/etc/ceph -v /tmp/baseliner_output_node1:/results -v /tmp/baseliner_output_node1:/output mariettesouppe/rados_bench:master-ec8d33f-luminous-ubuntu-16.04-x86_64 ", "delta": "1:40:08.692321", "end": "2018-06-04 15:00:27.851387", "failed": true, "finished": 1, "msg": "non-zero return code", "rc": 1, "start": "2018-06-04 13:20:19.159066", "stderr": "+ '[' -z True ']'\n+ '[' -z 128.110.152.0/22 ']'\n+ '[' -z 128.110.153.197 ']'\n+ sed -i -e s@monip@128.110.153.197@ /etc/ceph/ceph.conf\n+ sed -i -e s@cephnet@128.110.152.0/22@ /etc/ceph/ceph.conf\n+ '[' -f /etc/ceph/extra.conf ']'\n+ '[' -n True ']'\n+ '[' -n '' ']'\n+ exec /entrypoint.sh mon\n2018-06-04 19:20:20.004496 7f96a03d3f00  0 set uid:gid to 64045:64045 (ceph:ceph)\n2018-06-04 19:20:20.004514 7f96a03d3f00  0 ceph version 12.2.5 (cad919881333ac92274171586c827e01f554a70a) luminous (stable), process (unknown), pid 122\n2018-06-04 19:20:20.004605 7f96a03d3f00  0 pidfile_write: ignore empty --pid-file\n2018-06-04 19:20:20.012115 7f96a03d3f00  0 load: jerasure load: lrc load: isa \n2018-06-04 19:20:20.012222 7f96a03d3f00  0  set rocksdb option compression = kNoCompression\n2018-06-04 19:20:20.012235 7f96a03d3f00  0  set rocksdb option write_buffer_size = 33554432\n2018-06-04 19:20:20.012255 7f96a03d3f00  0  set rocksdb option compression = kNoCompression\n2018-06-04 19:20:20.012258 7f96a03d3f00  0  set rocksdb option write_buffer_size = 33554432\n2018-06-04 19:20:20.012372 7f96a03d3f00  4 rocksdb: RocksDB version: 5.4.0\n\n2018-06-04 19:20:20.012389 7f96a03d3f00  4 rocksdb: Git sha rocksdb_build_git_sha:@0@\n2018-06-04 19:20:20.012391 7f96a03d3f00  4 rocksdb: Compile date Apr 23 2018\n2018-06-04 19:20:20.012392 7f96a03d3f00  4 rocksdb: DB SUMMARY\n\n2018-06-04 19:20:20.012438 7f96a03d3f00  4 rocksdb: CURRENT file:  CURRENT\n\n2018-06-04 19:20:20.012441 7f96a03d3f00  4 rocksdb: IDENTITY file:  IDENTITY\n\n2018-06-04 19:20:20.012451 7f96a03d3f00  4 rocksdb: MANIFEST file:  MANIFEST-000001 size: 13 Bytes\n\n2018-06-04 19:20:20.012454 7f96a03d3f00  4 rocksdb: SST files in /var/lib/ceph/mon/ceph-node/store.db dir, Total Num: 0, files: \n\n2018-06-04 19:20:20.012456 7f96a03d3f00  4 rocksdb: Write Ahead Log file in /var/lib/ceph/mon/ceph-node/store.db: 000003.log size: 454 ; \n\n2018-06-04 19:20:20.012457 7f96a03d3f00  4 rocksdb:                         Options.error_if_exists: 0\n2018-06-04 19:20:20.012458 7f96a03d3f00  4 rocksdb:                       Options.create_if_missing: 0\n2018-06-04 19:20:20.012459 7f96a03d3f00  4 rocksdb:                         Options.paranoid_checks: 1\n2018-06-04 19:20:20.012460 7f96a03d3f00  4 rocksdb:                                     Options.env: 0x55aba9fbb020\n2018-06-04 19:20:20.012461 7f96a03d3f00  4 rocksdb:                                Options.info_log: 0x55abac0cec80\n2018-06-04 19:20:20.012462 7f96a03d3f00  4 rocksdb:                          Options.max_open_files: -1\n2018-06-04 19:20:20.012463 7f96a03d3f00  4 rocksdb:                Options.max_file_opening_threads: 16\n2018-06-04 19:20:20.012464 7f96a03d3f00  4 rocksdb:                               Options.use_fsync: 0\n2018-06-04 19:20:20.012465 7f96a03d3f00  4 rocksdb:                       Options.max_log_file_size: 0\n2018-06-04 19:20:20.012466 7f96a03d3f00  4 rocksdb:                  Options.max_manifest_file_size: 18446744073709551615\n2018-06-04 19:20:20.012469 7f96a03d3f00  4 rocksdb:                   Options.log_file_time_to_roll: 0\n2018-06-04 19:20:20.012470 7f96a03d3f00  4 rocksdb:                       Options.keep_log_file_num: 1000\n2018-06-04 19:20:20.012471 7f96a03d3f00  4 rocksdb:                    Options.recycle_log_file_num: 0\n2018-06-04 19:20:20.012479 7f96a03d3f00  4 rocksdb:                         Options.allow_fallocate: 1\n2018-06-04 19:20:20.012480 7f96a03d3f00  4 rocksdb:                        Options.allow_mmap_reads: 0\n2018-06-04 19:20:20.012481 7f96a03d3f00  4 rocksdb:                       Options.allow_mmap_writes: 0\n2018-06-04 19:20:20.012482 7f96a03d3f00  4 rocksdb:                        Options.use_direct_reads: 0\n2018-06-04 19:20:20.012483 7f96a03d3f00  4 rocksdb:                        Options.use_direct_io_for_flush_and_compaction: 0\n2018-06-04 19:20:20.012484 7f96a03d3f00  4 rocksdb:          Options.create_missing_column_families: 0\n2018-06-04 19:20:20.012485 7f96a03d3f00  4 rocksdb:                              Options.db_log_dir: \n2018-06-04 19:20:20.012486 7f96a03d3f00  4 rocksdb:                                 Options.wal_dir: /var/lib/ceph/mon/ceph-node/store.db\n2018-06-04 19:20:20.012486 7f96a03d3f00  4 rocksdb:                Options.table_cache_numshardbits: 6\n2018-06-04 19:20:20.012487 7f96a03d3f00  4 rocksdb:                      Options.max_subcompactions: 1\n2018-06-04 19:20:20.012488 7f96a03d3f00  4 rocksdb:                  Options.max_background_flushes: 1\n2018-06-04 19:20:20.012489 7f96a03d3f00  4 rocksdb:                         Options.WAL_ttl_seconds: 0\n2018-06-04 19:20:20.012491 7f96a03d3f00  4 rocksdb:                       Options.WAL_size_limit_MB: 0\n2018-06-04 19:20:20.012491 7f96a03d3f00  4 rocksdb:             Options.manifest_preallocation_size: 4194304\n2018-06-04 19:20:20.012492 7f96a03d3f00  4 rocksdb:                     Options.is_fd_close_on_exec: 1\n2018-06-04 19:20:20.012493 7f96a03d3f00  4 rocksdb:                   Options.advise_random_on_open: 1\n2018-06-04 19:20:20.012494 7f96a03d3f00  4 rocksdb:                    Options.db_write_buffer_size: 0\n2018-06-04 19:20:20.012495 7f96a03d3f00  4 rocksdb:         Options.access_hint_on_compaction_start: 1\n2018-06-04 19:20:20.012496 7f96a03d3f00  4 rocksdb:  Options.new_table_reader_for_compaction_inputs: 0\n2018-06-04 19:20:20.012496 7f96a03d3f00  4 rocksdb:               Options.compaction_readahead_size: 0\n2018-06-04 19:20:20.012497 7f96a03d3f00  4 rocksdb:           Options.random_access_max_buffer_size: 1048576\n2018-06-04 19:20:20.012498 7f96a03d3f00  4 rocksdb:           Options.writable_file_max_buffer_size: 1048576\n2018-06-04 19:20:20.012499 7f96a03d3f00  4 rocksdb:                      Options.use_adaptive_mutex: 0\n2018-06-04 19:20:20.012499 7f96a03d3f00  4 rocksdb:                            Options.rate_limiter: (nil)\n2018-06-04 19:20:20.012500 7f96a03d3f00  4 rocksdb:     Options.sst_file_manager.rate_bytes_per_sec: 0\n2018-06-04 19:20:20.012502 7f96a03d3f00  4 rocksdb:                          Options.bytes_per_sync: 0\n2018-06-04 19:20:20.012502 7f96a03d3f00  4 rocksdb:                      Options.wal_bytes_per_sync: 0\n2018-06-04 19:20:20.012503 7f96a03d3f00  4 rocksdb:                       Options.wal_recovery_mode: 2\n2018-06-04 19:20:20.012504 7f96a03d3f00  4 rocksdb:                  Options.enable_thread_tracking: 0\n2018-06-04 19:20:20.012505 7f96a03d3f00  4 rocksdb:         Options.allow_concurrent_memtable_write: 1\n2018-06-04 19:20:20.012505 7f96a03d3f00  4 rocksdb:      Options.enable_write_thread_adaptive_yield: 1\n2018-06-04 19:20:20.012506 7f96a03d3f00  4 rocksdb:             Options.write_thread_max_yield_usec: 100\n2018-06-04 19:20:20.012507 7f96a03d3f00  4 rocksdb:            Options.write_thread_slow_yield_usec: 3\n2018-06-04 19:20:20.012508 7f96a03d3f00  4 rocksdb:                               Options.row_cache: None\n2018-06-04 19:20:20.012509 7f96a03d3f00  4 rocksdb:                              Options.wal_filter: None\n2018-06-04 19:20:20.012510 7f96a03d3f00  4 rocksdb:             Options.avoid_flush_during_recovery: 0\n2018-06-04 19:20:20.012511 7f96a03d3f00  4 rocksdb:             Options.base_background_compactions: 1\n2018-06-04 19:20:20.012511 7f96a03d3f00  4 rocksdb:             Options.max_background_compactions: 1\n2018-06-04 19:20:20.012512 7f96a03d3f00  4 rocksdb:             Options.avoid_flush_during_shutdown: 0\n2018-06-04 19:20:20.012513 7f96a03d3f00  4 rocksdb:             Options.delayed_write_rate : 16777216\n2018-06-04 19:20:20.012514 7f96a03d3f00  4 rocksdb:             Options.max_total_wal_size: 0\n2018-06-04 19:20:20.012515 7f96a03d3f00  4 rocksdb:             Options.delete_obsolete_files_period_micros: 21600000000\n2018-06-04 19:20:20.012516 7f96a03d3f00  4 rocksdb:                   Options.stats_dump_period_sec: 600\n2018-06-04 19:20:20.012517 7f96a03d3f00  4 rocksdb: Compression algorithms supported:\n2018-06-04 19:20:20.012518 7f96a03d3f00  4 rocksdb: \tSnappy supported: 0\n2018-06-04 19:20:20.012519 7f96a03d3f00  4 rocksdb: \tZlib supported: 0\n2018-06-04 19:20:20.012519 7f96a03d3f00  4 rocksdb: \tBzip supported: 0\n2018-06-04 19:20:20.012520 7f96a03d3f00  4 rocksdb: \tLZ4 supported: 0\n2018-06-04 19:20:20.012521 7f96a03d3f00  4 rocksdb: \tZSTD supported: 0\n2018-06-04 19:20:20.012522 7f96a03d3f00  4 rocksdb: Fast CRC32 supported: 1\n2018-06-04 19:20:20.012636 7f96a03d3f00  4 rocksdb: [/build/ceph-12.2.5/src/rocksdb/db/version_set.cc:2609] Recovering from manifest file: MANIFEST-000001\n\n2018-06-04 19:20:20.012692 7f96a03d3f00  4 rocksdb: [/build/ceph-12.2.5/src/rocksdb/db/column_family.cc:407] --------------- Options for column family [default]:\n\n2018-06-04 19:20:20.012695 7f96a03d3f00  4 rocksdb:               Options.comparator: leveldb.BytewiseComparator\n2018-06-04 19:20:20.012696 7f96a03d3f00  4 rocksdb:           Options.merge_operator: \n2018-06-04 19:20:20.012697 7f96a03d3f00  4 rocksdb:        Options.compaction_filter: None\n2018-06-04 19:20:20.012698 7f96a03d3f00  4 rocksdb:        Options.compaction_filter_factory: None\n2018-06-04 19:20:20.012699 7f96a03d3f00  4 rocksdb:         Options.memtable_factory: SkipListFactory\n2018-06-04 19:20:20.012700 7f96a03d3f00  4 rocksdb:            Options.table_factory: BlockBasedTable\n2018-06-04 19:20:20.012720 7f96a03d3f00  4 rocksdb:            table_factory options:   flush_block_policy_factory: FlushBlockBySizePolicyFactory (0x55ababdaa0f8)\n  cache_index_and_filter_blocks: 1\n  cache_index_and_filter_blocks_with_high_priority: 1\n  pin_l0_filter_and_index_blocks_in_cache: 1\n  index_type: 0\n  hash_index_allow_collision: 1\n  checksum: 1\n  no_block_cache: 0\n  block_cache: 0x55abac0b65c0\n  block_cache_name: LRUCache\n  block_cache_options:\n    capacity : 134217728\n    num_shard_bits : 4\n    strict_capacity_limit : 0\n    high_pri_pool_ratio: 0.000\n  block_cache_compressed: (nil)\n  persistent_cache: (nil)\n  block_size: 4096\n  block_size_deviation: 10\n  block_restart_interval: 16\n  index_block_restart_interval: 1\n  filter_policy: rocksdb.BuiltinBloomFilter\n  whole_key_filtering: 1\n  format_version: 2\n\n2018-06-04 19:20:20.012725 7f96a03d3f00  4 rocksdb:        Options.write_buffer_size: 33554432\n2018-06-04 19:20:20.012726 7f96a03d3f00  4 rocksdb:  Options.max_write_buffer_number: 2\n2018-06-04 19:20:20.012727 7f96a03d3f00  4 rocksdb:          Options.compression: NoCompression\n2018-06-04 19:20:20.012728 7f96a03d3f00  4 rocksdb:                  Options.bottommost_compression: Disabled\n2018-06-04 19:20:20.012729 7f96a03d3f00  4 rocksdb:       Options.prefix_extractor: nullptr\n2018-06-04 19:20:20.012730 7f96a03d3f00  4 rocksdb:   Options.memtable_insert_with_hint_prefix_extractor: nullptr\n2018-06-04 19:20:20.012731 7f96a03d3f00  4 rocksdb:             Options.num_levels: 7\n2018-06-04 19:20:20.012731 7f96a03d3f00  4 rocksdb:        Options.min_write_buffer_number_to_merge: 1\n2018-06-04 19:20:20.012732 7f96a03d3f00  4 rocksdb:     Options.max_write_buffer_number_to_maintain: 0\n2018-06-04 19:20:20.012733 7f96a03d3f00  4 rocksdb:            Options.compression_opts.window_bits: -14\n2018-06-04 19:20:20.012734 7f96a03d3f00  4 rocksdb:                  Options.compression_opts.level: -1\n2018-06-04 19:20:20.012735 7f96a03d3f00  4 rocksdb:               Options.compression_opts.strategy: 0\n2018-06-04 19:20:20.012735 7f96a03d3f00  4 rocksdb:         Options.compression_opts.max_dict_bytes: 0\n2018-06-04 19:20:20.012736 7f96a03d3f00  4 rocksdb:      Options.level0_file_num_compaction_trigger: 4\n2018-06-04 19:20:20.012737 7f96a03d3f00  4 rocksdb:          Options.level0_slowdown_writes_trigger: 20\n2018-06-04 19:20:20.012737 7f96a03d3f00  4 rocksdb:              Options.level0_stop_writes_trigger: 36\n2018-06-04 19:20:20.012738 7f96a03d3f00  4 rocksdb:                   Options.target_file_size_base: 67108864\n2018-06-04 19:20:20.012738 7f96a03d3f00  4 rocksdb:             Options.target_file_size_multiplier: 1\n2018-06-04 19:20:20.012739 7f96a03d3f00  4 rocksdb:                Options.max_bytes_for_level_base: 268435456\n2018-06-04 19:20:20.012746 7f96a03d3f00  4 rocksdb: Options.level_compaction_dynamic_level_bytes: 0\n2018-06-04 19:20:20.012747 7f96a03d3f00  4 rocksdb:          Options.max_bytes_for_level_multiplier: 10.000000\n2018-06-04 19:20:20.012750 7f96a03d3f00  4 rocksdb: Options.max_bytes_for_level_multiplier_addtl[0]: 1\n2018-06-04 19:20:20.012751 7f96a03d3f00  4 rocksdb: Options.max_bytes_for_level_multiplier_addtl[1]: 1\n2018-06-04 19:20:20.012751 7f96a03d3f00  4 rocksdb: Options.max_bytes_for_level_multiplier_addtl[2]: 1\n2018-06-04 19:20:20.012752 7f96a03d3f00  4 rocksdb: Options.max_bytes_for_level_multiplier_addtl[3]: 1\n2018-06-04 19:20:20.012753 7f96a03d3f00  4 rocksdb: Options.max_bytes_for_level_multiplier_addtl[4]: 1\n2018-06-04 19:20:20.012753 7f96a03d3f00  4 rocksdb: Options.max_bytes_for_level_multiplier_addtl[5]: 1\n2018-06-04 19:20:20.012754 7f96a03d3f00  4 rocksdb: Options.max_bytes_for_level_multiplier_addtl[6]: 1\n2018-06-04 19:20:20.012755 7f96a03d3f00  4 rocksdb:       Options.max_sequential_skip_in_iterations: 8\n2018-06-04 19:20:20.012755 7f96a03d3f00  4 rocksdb:                    Options.max_compaction_bytes: 1677721600\n2018-06-04 19:20:20.012756 7f96a03d3f00  4 rocksdb:                        Options.arena_block_size: 4194304\n2018-06-04 19:20:20.012757 7f96a03d3f00  4 rocksdb:   Options.soft_pending_compaction_bytes_limit: 68719476736\n2018-06-04 19:20:20.012758 7f96a03d3f00  4 rocksdb:   Options.hard_pending_compaction_bytes_limit: 274877906944\n2018-06-04 19:20:20.012758 7f96a03d3f00  4 rocksdb:       Options.rate_limit_delay_max_milliseconds: 100\n2018-06-04 19:20:20.012759 7f96a03d3f00  4 rocksdb:                Options.disable_auto_compactions: 0\n2018-06-04 19:20:20.012761 7f96a03d3f00  4 rocksdb:                         Options.compaction_style: kCompactionStyleLevel\n2018-06-04 19:20:20.012762 7f96a03d3f00  4 rocksdb:                           Options.compaction_pri: kByCompensatedSize\n2018-06-04 19:20:20.012762 7f96a03d3f00  4 rocksdb:  Options.compaction_options_universal.size_ratio: 1\n2018-06-04 19:20:20.012763 7f96a03d3f00  4 rocksdb: Options.compaction_options_universal.min_merge_width: 2\n2018-06-04 19:20:20.012764 7f96a03d3f00  4 rocksdb: Options.compaction_options_universal.max_merge_width: 4294967295\n2018-06-04 19:20:20.012765 7f96a03d3f00  4 rocksdb: Options.compaction_options_universal.max_size_amplification_percent: 200\n2018-06-04 19:20:20.012766 7f96a03d3f00  4 rocksdb: Options.compaction_options_universal.compression_size_percent: -1\n2018-06-04 19:20:20.012766 7f96a03d3f00  4 rocksdb: Options.compaction_options_fifo.max_table_files_size: 1073741824\n2018-06-04 19:20:20.012767 7f96a03d3f00  4 rocksdb:                   Options.table_properties_collectors: \n2018-06-04 19:20:20.012768 7f96a03d3f00  4 rocksdb:                   Options.inplace_update_support: 0\n2018-06-04 19:20:20.012769 7f96a03d3f00  4 rocksdb:                 Options.inplace_update_num_locks: 10000\n2018-06-04 19:20:20.012770 7f96a03d3f00  4 rocksdb:               Options.memtable_prefix_bloom_size_ratio: 0.000000\n2018-06-04 19:20:20.012771 7f96a03d3f00  4 rocksdb:   Options.memtable_huge_page_size: 0\n2018-06-04 19:20:20.012772 7f96a03d3f00  4 rocksdb:                           Options.bloom_locality: 0\n2018-06-04 19:20:20.012773 7f96a03d3f00  4 rocksdb:                    Options.max_successive_merges: 0\n2018-06-04 19:20:20.012774 7f96a03d3f00  4 rocksdb:                Options.optimize_filters_for_hits: 0\n2018-06-04 19:20:20.012774 7f96a03d3f00  4 rocksdb:                Options.paranoid_file_checks: 0\n2018-06-04 19:20:20.012775 7f96a03d3f00  4 rocksdb:                Options.force_consistency_checks: 0\n2018-06-04 19:20:20.012776 7f96a03d3f00  4 rocksdb:                Options.report_bg_io_stats: 0\n2018-06-04 19:20:20.014299 7f96a03d3f00  4 rocksdb: [/build/ceph-12.2.5/src/rocksdb/db/version_set.cc:2859] Recovered from manifest file:/var/lib/ceph/mon/ceph-node/store.db/MANIFEST-000001 succeeded,manifest_file_number is 1, next_file_number is 3, last_sequence is 0, log_number is 0,prev_log_number is 0,max_column_family is 0\n\n2018-06-04 19:20:20.014318 7f96a03d3f00  4 rocksdb: [/build/ceph-12.2.5/src/rocksdb/db/version_set.cc:2867] Column family [default] (ID 0), log number is 0\n\n2018-06-04 19:20:20.014444 7f96a03d3f00  4 rocksdb: EVENT_LOG_v1 {\"time_micros\": 1528140020014427, \"job\": 1, \"event\": \"recovery_started\", \"log_files\": [3]}\n2018-06-04 19:20:20.014454 7f96a03d3f00  4 rocksdb: [/build/ceph-12.2.5/src/rocksdb/db/db_impl_open.cc:482] Recovering log #3 mode 2\n2018-06-04 19:20:20.019673 7f96a03d3f00  4 rocksdb: EVENT_LOG_v1 {\"time_micros\": 1528140020019662, \"cf_name\": \"default\", \"job\": 1, \"event\": \"table_file_creation\", \"file_number\": 4, \"file_size\": 1420, \"table_properties\": {\"data_size\": 463, \"index_size\": 28, \"filter_size\": 20, \"raw_key_size\": 95, \"raw_average_key_size\": 23, \"raw_value_size\": 359, \"raw_average_value_size\": 89, \"num_data_blocks\": 1, \"num_entries\": 4, \"filter_policy_name\": \"rocksdb.BuiltinBloomFilter\", \"kDeletedKeys\": \"0\", \"kMergeOperands\": \"0\"}}\n2018-06-04 19:20:20.019704 7f96a03d3f00  4 rocksdb: [/build/ceph-12.2.5/src/rocksdb/db/version_set.cc:2395] Creating manifest 5\n\n2018-06-04 19:20:20.028994 7f96a03d3f00  4 rocksdb: EVENT_LOG_v1 {\"time_micros\": 1528140020028990, \"job\": 1, \"event\": \"recovery_finished\"}\n2018-06-04 19:20:20.039038 7f96a03d3f00  4 rocksdb: [/build/ceph-12.2.5/src/rocksdb/db/db_impl_open.cc:1063] DB pointer 0x55abac1ca000\n2018-06-04 19:20:20.040153 7f96a03d3f00  0 starting mon.node rank 0 at public addr 128.110.153.197:6789/0 at bind addr 128.110.153.197:6789/0 mon_data /var/lib/ceph/mon/ceph-node fsid b2bb6c0b-6cc6-44f1-bf92-1ce81d78fbdc\n2018-06-04 19:20:20.040315 7f96a03d3f00  0 starting mon.node rank 0 at 128.110.153.197:6789/0 mon_data /var/lib/ceph/mon/ceph-node fsid b2bb6c0b-6cc6-44f1-bf92-1ce81d78fbdc\n2018-06-04 19:20:20.041162 7f96a03d3f00  1 mon.node@-1(probing) e0 preinit fsid b2bb6c0b-6cc6-44f1-bf92-1ce81d78fbdc\n2018-06-04 19:20:20.041327 7f96a03d3f00  1 mon.node@-1(probing).mds e0 Unable to load 'last_metadata'\n2018-06-04 19:20:20.042578 7f96a03d3f00  0 mon.node@-1(probing) e0  my rank is now 0 (was -1)\n2018-06-04 19:20:20.042607 7f96a03d3f00  1 mon.node@0(probing) e0 win_standalone_election\n2018-06-04 19:20:20.042621 7f96a03d3f00  1 mon.node@0(probing).elector(0) init, first boot, initializing epoch at 1 \n2018-06-04 19:20:20.051554 7f96a03d3f00  0 log_channel(cluster) log [INF] : mon.node is new leader, mons node in quorum (ranks 0)\n2018-06-04 19:20:20.056409 7f96a03d3f00  1 mon.node@0(leader).osd e0 create_pending setting backfillfull_ratio = 0.9\n2018-06-04 19:20:20.056448 7f96a03d3f00  1 mon.node@0(leader).osd e0 create_pending setting full_ratio = 0.95\n2018-06-04 19:20:20.056452 7f96a03d3f00  1 mon.node@0(leader).osd e0 create_pending setting nearfull_ratio = 0.85\n2018-06-04 19:20:20.056730 7f96a03d3f00  1 mon.node@0(leader).osd e0 encode_pending skipping prime_pg_temp; mapping job did not start\n2018-06-04 19:20:20.057897 7f96a03d3f00  1 mon.node@0(leader) e0 _apply_compatset_features enabling new quorum features: compat={},rocompat={},incompat={4=support erasure code pools,5=new-style osdmap encoding,6=support isa/lrc erasure code,7=support shec erasure code}\n2018-06-04 19:20:20.066285 7f96929dd700  1 mon.node@0(leader).paxosservice(pgmap 0..0) refresh upgraded, format 1 -> 0\n2018-06-04 19:20:20.066301 7f96929dd700  1 mon.node@0(leader).pg v0 on_upgrade discarding in-core PGMap\n2018-06-04 19:20:20.070775 7f96929dd700  1 mon.node@0(leader).paxosservice(auth 0..0) refresh upgraded, format 2 -> 0\n2018-06-04 19:20:20.071057 7f96929dd700  1 mon.node@0(probing) e1 win_standalone_election\n2018-06-04 19:20:20.071072 7f96929dd700  1 mon.node@0(probing).elector(2) init, last seen epoch 2\n2018-06-04 19:20:20.076749 7f96929dd700  0 log_channel(cluster) log [INF] : mon.node is new leader, mons node in quorum (ranks 0)\n2018-06-04 19:20:20.081299 7f96929dd700  0 log_channel(cluster) log [DBG] : monmap e1: 1 mons at {node=128.110.153.197:6789/0}\n2018-06-04 19:20:20.086423 7f96929dd700  1 mon.node@0(leader).osd e0 create_pending setting backfillfull_ratio = 0.9\n2018-06-04 19:20:20.086441 7f96929dd700  1 mon.node@0(leader).osd e0 create_pending setting full_ratio = 0.95\n2018-06-04 19:20:20.086446 7f96929dd700  1 mon.node@0(leader).osd e0 create_pending setting nearfull_ratio = 0.85\n2018-06-04 19:20:20.086670 7f96929dd700  1 mon.node@0(leader).osd e0 encode_pending skipping prime_pg_temp; mapping job did not start\n2018-06-04 19:20:20.087442 7f96929dd700  1 mon.node@0(leader) e1 _apply_compatset_features enabling new quorum features: compat={},rocompat={},incompat={8=support monmap features,9=luminous ondisk layout}\n2018-06-04 19:20:20.096456 7f96929dd700  1 mon.node@0(leader).paxosservice(auth 0..0) refresh upgraded, format 2 -> 0\n2018-06-04 19:20:20.096576 7f96929dd700  0 log_channel(cluster) log [INF] : pgmap 0 pgs: ; 0 bytes data, 0 kB used, 0 kB / 0 kB avail\n2018-06-04 19:20:20.106242 7f96929dd700  0 mon.node@0(leader).mds e1 print_map\ne1\nenable_multiple, ever_enabled_multiple: 0,0\ncompat: compat={},rocompat={},incompat={1=base v0.20,2=client writeable ranges,3=default file layouts on dirs,4=dir inode in separate object,5=mds uses versioned encoding,6=dirfrag is stored in omap,8=no anchor table,9=file layout v2}\nlegacy client fscid: -1\n \nNo filesystems configured\n\n2018-06-04 19:20:20.106401 7f96929dd700  1 mon.node@0(leader).osd e1 e1: 0 total, 0 up, 0 in\n2018-06-04 19:20:20.110683 7f96929dd700  0 mon.node@0(leader).osd e1 crush map has features 288514050185494528, adjusting msgr requires\n2018-06-04 19:20:20.110692 7f96929dd700  0 mon.node@0(leader).osd e1 crush map has features 288514050185494528, adjusting msgr requires\n2018-06-04 19:20:20.110698 7f96929dd700  0 mon.node@0(leader).osd e1 crush map has features 1009089990564790272, adjusting msgr requires\n2018-06-04 19:20:20.110701 7f96929dd700  0 mon.node@0(leader).osd e1 crush map has features 288514050185494528, adjusting msgr requires\n2018-06-04 19:20:20.110899 7f96929dd700  1 mon.node@0(leader).log v1 unable to write to '/var/log/ceph/ceph.log' for channel 'cluster': (2) No such file or directory\n2018-06-04 19:20:20.110959 7f96929dd700  1 mon.node@0(leader).paxosservice(auth 1..1) refresh upgraded, format 0 -> 2\n2018-06-04 19:20:20.111660 7f96929dd700  0 log_channel(cluster) log [DBG] : fsmap \n2018-06-04 19:20:20.111764 7f96929dd700  1 mon.node@0(leader).pg v1 check_osd_map will clear pg_map data\n2018-06-04 19:20:20.111768 7f96929dd700  1 mon.node@0(leader).pg v1 encode_pending clearing pgmap data at v2\n2018-06-04 19:20:20.115757 7f96929dd700  0 log_channel(cluster) log [DBG] : osdmap e1: 0 total, 0 up, 0 in\n2018-06-04 19:20:20.115978 7f96929dd700  0 log_channel(cluster) log [DBG] : mgrmap e1: no daemons active\n2018-06-04 19:20:20.129417 7f96929dd700  1 mon.node@0(leader).log v2 unable to write to '/var/log/ceph/ceph.log' for channel 'cluster': (2) No such file or directory\n2018-06-04 19:20:20.361812 7f96971e6700  0 mon.node@0(leader) e1 handle_command mon_command({\"prefix\": \"status\"} v 0) v1\n2018-06-04 19:20:20.361853 7f96971e6700  0 log_channel(audit) log [DBG] : from='client.? 128.110.153.138:0/1904449781' entity='client.admin' cmd=[{\"prefix\": \"status\"}]: dispatch\n2018-06-04 19:20:20.410253 7f96971e6700  0 mon.node@0(leader) e1 handle_command mon_command({\"prefix\": \"auth get\", \"entity\": \"client.bootstrap-osd\"} v 0) v1\n2018-06-04 19:20:20.410300 7f96971e6700  0 log_channel(audit) log [INF] : from='client.? 128.110.153.77:0/4263726369' entity='client.admin' cmd=[{\"prefix\": \"auth get\", \"entity\": \"client.bootstrap-osd\"}]: dispatch\n2018-06-04 19:20:20.649783 7f96971e6700  0 mon.node@0(leader) e1 handle_command mon_command({\"pool2\": \"rbd\", \"prefix\": \"osd pool rm\", \"sure\": \"--yes-i-really-really-mean-it\", \"pool\": \"rbd\"} v 0) v1\n2018-06-04 19:20:20.649858 7f96971e6700  0 log_channel(audit) log [INF] : from='client.? 128.110.153.138:0/1117866057' entity='client.admin' cmd=[{\"pool2\": \"rbd\", \"prefix\": \"osd pool rm\", \"sure\": \"--yes-i-really-really-mean-it\", \"pool\": \"rbd\"}]: dispatch\n2018-06-04 19:20:20.916772 7f96971e6700  0 mon.node@0(leader) e1 handle_command mon_command({\"prefix\": \"osd pool create\", \"pg_num\": 150, \"pool\": \"scbench\", \"pgp_num\": 150} v 0) v1\n2018-06-04 19:20:20.916844 7f96971e6700  0 log_channel(audit) log [INF] : from='client.? 128.110.153.138:0/326234720' entity='client.admin' cmd=[{\"prefix\": \"osd pool create\", \"pg_num\": 150, \"pool\": \"scbench\", \"pgp_num\": 150}]: dispatch\n2018-06-04 19:20:21.100932 7f96971e6700  1 mon.node@0(leader).osd e1 encode_pending skipping prime_pg_temp; mapping job did not start\n2018-06-04 19:20:21.110505 7f96929dd700  1 mon.node@0(leader).osd e2 e2: 0 total, 0 up, 0 in\n2018-06-04 19:20:21.114976 7f96929dd700  0 mon.node@0(leader).osd e2 crush map has features 288514051259236352, adjusting msgr requires\n2018-06-04 19:20:21.115006 7f96929dd700  0 mon.node@0(leader).osd e2 crush map has features 288514051259236352, adjusting msgr requires\n2018-06-04 19:20:21.115014 7f96929dd700  0 mon.node@0(leader).osd e2 crush map has features 1009089991638532096, adjusting msgr requires\n2018-06-04 19:20:21.115017 7f96929dd700  0 mon.node@0(leader).osd e2 crush map has features 288514051259236352, adjusting msgr requires\n2018-06-04 19:20:21.115384 7f96929dd700  0 log_channel(audit) log [INF] : from='client.? 128.110.153.138:0/326234720' entity='client.admin' cmd='[{\"prefix\": \"osd pool create\", \"pg_num\": 150, \"pool\": \"scbench\", \"pgp_num\": 150}]': finished\n2018-06-04 19:20:21.115751 7f96929dd700  0 log_channel(cluster) log [DBG] : osdmap e2: 0 total, 0 up, 0 in\n2018-06-04 19:20:21.139732 7f96929dd700  1 mon.node@0(leader).log v3 unable to write to '/var/log/ceph/ceph.audit.log' for channel 'audit': (2) No such file or directory\n2018-06-04 19:20:21.139764 7f96929dd700  1 mon.node@0(leader).log v3 unable to write to '/var/log/ceph/ceph.log' for channel 'cluster': (2) No such file or directory\n2018-06-04 19:20:21.428815 7f96971e6700  0 mon.node@0(leader) e1 handle_command mon_command({\"var\": \"size\", \"prefix\": \"osd pool set\", \"pool\": \"scbench\", \"val\": \"1\"} v 0) v1\n2018-06-04 19:20:21.428901 7f96971e6700  0 log_channel(audit) log [INF] : from='client.? 128.110.153.138:0/3911707674' entity='client.admin' cmd=[{\"var\": \"size\", \"prefix\": \"osd pool set\", \"pool\": \"scbench\", \"val\": \"1\"}]: dispatch\n2018-06-04 19:20:22.149153 7f96929dd700  1 mon.node@0(leader).log v4 unable to write to '/var/log/ceph/ceph.audit.log' for channel 'audit': (2) No such file or directory\n2018-06-04 19:20:22.159162 7f96929dd700  1 mon.node@0(leader).osd e3 e3: 0 total, 0 up, 0 in\n2018-06-04 19:20:22.164492 7f96929dd700  0 log_channel(audit) log [INF] : from='client.? 128.110.153.138:0/3911707674' entity='client.admin' cmd='[{\"var\": \"size\", \"prefix\": \"osd pool set\", \"pool\": \"scbench\", \"val\": \"1\"}]': finished\n2018-06-04 19:20:22.164649 7f96929dd700  0 log_channel(cluster) log [DBG] : osdmap e3: 0 total, 0 up, 0 in\n2018-06-04 19:20:23.168593 7f96929dd700  1 mon.node@0(leader).log v5 unable to write to '/var/log/ceph/ceph.audit.log' for channel 'audit': (2) No such file or directory\n2018-06-04 19:21:20.042250 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB\n2018-06-04 19:22:20.042626 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB\n2018-06-04 19:23:20.043021 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB\n2018-06-04 19:24:20.043697 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB\n2018-06-04 19:25:20.044133 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB\n2018-06-04 19:26:20.044517 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB\n2018-06-04 19:27:20.044909 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB\n2018-06-04 19:28:20.045294 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB\n2018-06-04 19:29:20.045704 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB\n2018-06-04 19:30:20.046090 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB\n2018-06-04 19:31:20.046475 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB\n2018-06-04 19:32:20.046879 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB\n2018-06-04 19:33:20.047264 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB\n2018-06-04 19:34:20.047676 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB\n2018-06-04 19:35:20.048091 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB\n2018-06-04 19:36:20.048478 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB\n2018-06-04 19:37:20.048859 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB\n2018-06-04 19:38:20.049290 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB\n2018-06-04 19:39:20.049710 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB\n2018-06-04 19:40:20.050169 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB\n2018-06-04 19:41:20.050547 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB\n2018-06-04 19:42:20.050939 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB\n2018-06-04 19:43:20.051386 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB\n2018-06-04 19:44:20.051814 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB\n2018-06-04 19:45:20.052233 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB\n2018-06-04 19:46:20.052620 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB\n2018-06-04 19:47:20.053001 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB\n2018-06-04 19:48:20.053384 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB\n2018-06-04 19:49:20.053780 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB\n2018-06-04 19:50:20.054200 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB\n2018-06-04 19:51:20.054583 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB\n2018-06-04 19:52:20.054925 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB\n2018-06-04 19:53:20.055349 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB\n2018-06-04 19:54:20.055736 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB\n2018-06-04 19:55:20.056115 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB\n2018-06-04 19:56:20.056540 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB\n2018-06-04 19:57:20.056956 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB\n2018-06-04 19:58:20.057369 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB\n2018-06-04 19:59:20.057814 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB\n2018-06-04 20:00:20.058194 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB\n2018-06-04 20:01:20.058598 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB\n2018-06-04 20:02:20.059027 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB\n2018-06-04 20:03:20.059414 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB\n2018-06-04 20:04:20.059801 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB\n2018-06-04 20:05:20.060186 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB\n2018-06-04 20:06:20.060626 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB\n2018-06-04 20:07:20.061069 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB\n2018-06-04 20:08:20.061531 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB\n2018-06-04 20:09:20.061934 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB\n2018-06-04 20:10:20.062349 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB\n2018-06-04 20:11:20.062723 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB\n2018-06-04 20:12:20.063125 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB\n2018-06-04 20:13:20.063551 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB\n2018-06-04 20:14:20.063956 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB\n2018-06-04 20:15:20.064399 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB\n2018-06-04 20:16:20.064810 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB\n2018-06-04 20:17:20.065197 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB\n2018-06-04 20:18:20.065678 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB\n2018-06-04 20:19:20.066078 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB\n2018-06-04 20:20:20.066456 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB\n2018-06-04 20:21:20.066842 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB\n2018-06-04 20:22:20.067330 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB\n2018-06-04 20:23:20.067747 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB\n2018-06-04 20:24:20.068128 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB\n2018-06-04 20:25:20.068522 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB\n2018-06-04 20:26:20.069036 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB\n2018-06-04 20:27:20.069427 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB\n2018-06-04 20:28:20.069888 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB\n2018-06-04 20:29:20.070281 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB\n2018-06-04 20:30:20.070689 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB\n2018-06-04 20:31:20.071128 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB\n2018-06-04 20:32:20.071608 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB\n2018-06-04 20:33:20.072155 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB\n2018-06-04 20:34:20.072584 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB\n2018-06-04 20:35:20.072981 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB\n2018-06-04 20:36:20.073583 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB\n2018-06-04 20:37:20.074147 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB\n2018-06-04 20:38:20.074694 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB\n2018-06-04 20:39:20.075176 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB\n2018-06-04 20:40:20.075571 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB\n2018-06-04 20:41:20.075901 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB\n2018-06-04 20:42:20.076323 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB\n2018-06-04 20:43:20.076711 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB\n2018-06-04 20:44:20.077257 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB\n2018-06-04 20:45:20.077763 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB\n2018-06-04 20:46:20.078147 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB\n2018-06-04 20:47:20.078555 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB\n2018-06-04 20:48:20.078946 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB\n2018-06-04 20:49:20.079337 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB\n2018-06-04 20:50:20.079712 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB\n2018-06-04 20:51:20.080093 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB\n2018-06-04 20:52:20.080477 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB\n2018-06-04 20:53:20.080868 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB\n2018-06-04 20:54:20.081260 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB\n2018-06-04 20:55:20.081700 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12381 MB\n2018-06-04 20:56:20.082089 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12381 MB\n2018-06-04 20:57:20.082544 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12381 MB\n2018-06-04 20:58:20.082969 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12381 MB\n2018-06-04 20:59:20.083566 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12381 MB\n2018-06-04 21:00:20.083950 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12381 MB\n2018-06-04 21:00:27.677151 7f96949e1700 -1 received  signal: Terminated from  PID: 1 task name: /bin/bash /entrypoint.sh mon  UID: 0\n2018-06-04 21:00:27.677185 7f96949e1700 -1 mon.node@0(leader) e1 *** Got Signal Terminated ***\n2018-06-04 21:00:27.677210 7f96949e1700  1 mon.node@0(leader) e1 shutdown", "stderr_lines": ["+ '[' -z True ']'", "+ '[' -z 128.110.152.0/22 ']'", "+ '[' -z 128.110.153.197 ']'", "+ sed -i -e s@monip@128.110.153.197@ /etc/ceph/ceph.conf", "+ sed -i -e s@cephnet@128.110.152.0/22@ /etc/ceph/ceph.conf", "+ '[' -f /etc/ceph/extra.conf ']'", "+ '[' -n True ']'", "+ '[' -n '' ']'", "+ exec /entrypoint.sh mon", "2018-06-04 19:20:20.004496 7f96a03d3f00  0 set uid:gid to 64045:64045 (ceph:ceph)", "2018-06-04 19:20:20.004514 7f96a03d3f00  0 ceph version 12.2.5 (cad919881333ac92274171586c827e01f554a70a) luminous (stable), process (unknown), pid 122", "2018-06-04 19:20:20.004605 7f96a03d3f00  0 pidfile_write: ignore empty --pid-file", "2018-06-04 19:20:20.012115 7f96a03d3f00  0 load: jerasure load: lrc load: isa ", "2018-06-04 19:20:20.012222 7f96a03d3f00  0  set rocksdb option compression = kNoCompression", "2018-06-04 19:20:20.012235 7f96a03d3f00  0  set rocksdb option write_buffer_size = 33554432", "2018-06-04 19:20:20.012255 7f96a03d3f00  0  set rocksdb option compression = kNoCompression", "2018-06-04 19:20:20.012258 7f96a03d3f00  0  set rocksdb option write_buffer_size = 33554432", "2018-06-04 19:20:20.012372 7f96a03d3f00  4 rocksdb: RocksDB version: 5.4.0", "", "2018-06-04 19:20:20.012389 7f96a03d3f00  4 rocksdb: Git sha rocksdb_build_git_sha:@0@", "2018-06-04 19:20:20.012391 7f96a03d3f00  4 rocksdb: Compile date Apr 23 2018", "2018-06-04 19:20:20.012392 7f96a03d3f00  4 rocksdb: DB SUMMARY", "", "2018-06-04 19:20:20.012438 7f96a03d3f00  4 rocksdb: CURRENT file:  CURRENT", "", "2018-06-04 19:20:20.012441 7f96a03d3f00  4 rocksdb: IDENTITY file:  IDENTITY", "", "2018-06-04 19:20:20.012451 7f96a03d3f00  4 rocksdb: MANIFEST file:  MANIFEST-000001 size: 13 Bytes", "", "2018-06-04 19:20:20.012454 7f96a03d3f00  4 rocksdb: SST files in /var/lib/ceph/mon/ceph-node/store.db dir, Total Num: 0, files: ", "", "2018-06-04 19:20:20.012456 7f96a03d3f00  4 rocksdb: Write Ahead Log file in /var/lib/ceph/mon/ceph-node/store.db: 000003.log size: 454 ; ", "", "2018-06-04 19:20:20.012457 7f96a03d3f00  4 rocksdb:                         Options.error_if_exists: 0", "2018-06-04 19:20:20.012458 7f96a03d3f00  4 rocksdb:                       Options.create_if_missing: 0", "2018-06-04 19:20:20.012459 7f96a03d3f00  4 rocksdb:                         Options.paranoid_checks: 1", "2018-06-04 19:20:20.012460 7f96a03d3f00  4 rocksdb:                                     Options.env: 0x55aba9fbb020", "2018-06-04 19:20:20.012461 7f96a03d3f00  4 rocksdb:                                Options.info_log: 0x55abac0cec80", "2018-06-04 19:20:20.012462 7f96a03d3f00  4 rocksdb:                          Options.max_open_files: -1", "2018-06-04 19:20:20.012463 7f96a03d3f00  4 rocksdb:                Options.max_file_opening_threads: 16", "2018-06-04 19:20:20.012464 7f96a03d3f00  4 rocksdb:                               Options.use_fsync: 0", "2018-06-04 19:20:20.012465 7f96a03d3f00  4 rocksdb:                       Options.max_log_file_size: 0", "2018-06-04 19:20:20.012466 7f96a03d3f00  4 rocksdb:                  Options.max_manifest_file_size: 18446744073709551615", "2018-06-04 19:20:20.012469 7f96a03d3f00  4 rocksdb:                   Options.log_file_time_to_roll: 0", "2018-06-04 19:20:20.012470 7f96a03d3f00  4 rocksdb:                       Options.keep_log_file_num: 1000", "2018-06-04 19:20:20.012471 7f96a03d3f00  4 rocksdb:                    Options.recycle_log_file_num: 0", "2018-06-04 19:20:20.012479 7f96a03d3f00  4 rocksdb:                         Options.allow_fallocate: 1", "2018-06-04 19:20:20.012480 7f96a03d3f00  4 rocksdb:                        Options.allow_mmap_reads: 0", "2018-06-04 19:20:20.012481 7f96a03d3f00  4 rocksdb:                       Options.allow_mmap_writes: 0", "2018-06-04 19:20:20.012482 7f96a03d3f00  4 rocksdb:                        Options.use_direct_reads: 0", "2018-06-04 19:20:20.012483 7f96a03d3f00  4 rocksdb:                        Options.use_direct_io_for_flush_and_compaction: 0", "2018-06-04 19:20:20.012484 7f96a03d3f00  4 rocksdb:          Options.create_missing_column_families: 0", "2018-06-04 19:20:20.012485 7f96a03d3f00  4 rocksdb:                              Options.db_log_dir: ", "2018-06-04 19:20:20.012486 7f96a03d3f00  4 rocksdb:                                 Options.wal_dir: /var/lib/ceph/mon/ceph-node/store.db", "2018-06-04 19:20:20.012486 7f96a03d3f00  4 rocksdb:                Options.table_cache_numshardbits: 6", "2018-06-04 19:20:20.012487 7f96a03d3f00  4 rocksdb:                      Options.max_subcompactions: 1", "2018-06-04 19:20:20.012488 7f96a03d3f00  4 rocksdb:                  Options.max_background_flushes: 1", "2018-06-04 19:20:20.012489 7f96a03d3f00  4 rocksdb:                         Options.WAL_ttl_seconds: 0", "2018-06-04 19:20:20.012491 7f96a03d3f00  4 rocksdb:                       Options.WAL_size_limit_MB: 0", "2018-06-04 19:20:20.012491 7f96a03d3f00  4 rocksdb:             Options.manifest_preallocation_size: 4194304", "2018-06-04 19:20:20.012492 7f96a03d3f00  4 rocksdb:                     Options.is_fd_close_on_exec: 1", "2018-06-04 19:20:20.012493 7f96a03d3f00  4 rocksdb:                   Options.advise_random_on_open: 1", "2018-06-04 19:20:20.012494 7f96a03d3f00  4 rocksdb:                    Options.db_write_buffer_size: 0", "2018-06-04 19:20:20.012495 7f96a03d3f00  4 rocksdb:         Options.access_hint_on_compaction_start: 1", "2018-06-04 19:20:20.012496 7f96a03d3f00  4 rocksdb:  Options.new_table_reader_for_compaction_inputs: 0", "2018-06-04 19:20:20.012496 7f96a03d3f00  4 rocksdb:               Options.compaction_readahead_size: 0", "2018-06-04 19:20:20.012497 7f96a03d3f00  4 rocksdb:           Options.random_access_max_buffer_size: 1048576", "2018-06-04 19:20:20.012498 7f96a03d3f00  4 rocksdb:           Options.writable_file_max_buffer_size: 1048576", "2018-06-04 19:20:20.012499 7f96a03d3f00  4 rocksdb:                      Options.use_adaptive_mutex: 0", "2018-06-04 19:20:20.012499 7f96a03d3f00  4 rocksdb:                            Options.rate_limiter: (nil)", "2018-06-04 19:20:20.012500 7f96a03d3f00  4 rocksdb:     Options.sst_file_manager.rate_bytes_per_sec: 0", "2018-06-04 19:20:20.012502 7f96a03d3f00  4 rocksdb:                          Options.bytes_per_sync: 0", "2018-06-04 19:20:20.012502 7f96a03d3f00  4 rocksdb:                      Options.wal_bytes_per_sync: 0", "2018-06-04 19:20:20.012503 7f96a03d3f00  4 rocksdb:                       Options.wal_recovery_mode: 2", "2018-06-04 19:20:20.012504 7f96a03d3f00  4 rocksdb:                  Options.enable_thread_tracking: 0", "2018-06-04 19:20:20.012505 7f96a03d3f00  4 rocksdb:         Options.allow_concurrent_memtable_write: 1", "2018-06-04 19:20:20.012505 7f96a03d3f00  4 rocksdb:      Options.enable_write_thread_adaptive_yield: 1", "2018-06-04 19:20:20.012506 7f96a03d3f00  4 rocksdb:             Options.write_thread_max_yield_usec: 100", "2018-06-04 19:20:20.012507 7f96a03d3f00  4 rocksdb:            Options.write_thread_slow_yield_usec: 3", "2018-06-04 19:20:20.012508 7f96a03d3f00  4 rocksdb:                               Options.row_cache: None", "2018-06-04 19:20:20.012509 7f96a03d3f00  4 rocksdb:                              Options.wal_filter: None", "2018-06-04 19:20:20.012510 7f96a03d3f00  4 rocksdb:             Options.avoid_flush_during_recovery: 0", "2018-06-04 19:20:20.012511 7f96a03d3f00  4 rocksdb:             Options.base_background_compactions: 1", "2018-06-04 19:20:20.012511 7f96a03d3f00  4 rocksdb:             Options.max_background_compactions: 1", "2018-06-04 19:20:20.012512 7f96a03d3f00  4 rocksdb:             Options.avoid_flush_during_shutdown: 0", "2018-06-04 19:20:20.012513 7f96a03d3f00  4 rocksdb:             Options.delayed_write_rate : 16777216", "2018-06-04 19:20:20.012514 7f96a03d3f00  4 rocksdb:             Options.max_total_wal_size: 0", "2018-06-04 19:20:20.012515 7f96a03d3f00  4 rocksdb:             Options.delete_obsolete_files_period_micros: 21600000000", "2018-06-04 19:20:20.012516 7f96a03d3f00  4 rocksdb:                   Options.stats_dump_period_sec: 600", "2018-06-04 19:20:20.012517 7f96a03d3f00  4 rocksdb: Compression algorithms supported:", "2018-06-04 19:20:20.012518 7f96a03d3f00  4 rocksdb: \tSnappy supported: 0", "2018-06-04 19:20:20.012519 7f96a03d3f00  4 rocksdb: \tZlib supported: 0", "2018-06-04 19:20:20.012519 7f96a03d3f00  4 rocksdb: \tBzip supported: 0", "2018-06-04 19:20:20.012520 7f96a03d3f00  4 rocksdb: \tLZ4 supported: 0", "2018-06-04 19:20:20.012521 7f96a03d3f00  4 rocksdb: \tZSTD supported: 0", "2018-06-04 19:20:20.012522 7f96a03d3f00  4 rocksdb: Fast CRC32 supported: 1", "2018-06-04 19:20:20.012636 7f96a03d3f00  4 rocksdb: [/build/ceph-12.2.5/src/rocksdb/db/version_set.cc:2609] Recovering from manifest file: MANIFEST-000001", "", "2018-06-04 19:20:20.012692 7f96a03d3f00  4 rocksdb: [/build/ceph-12.2.5/src/rocksdb/db/column_family.cc:407] --------------- Options for column family [default]:", "", "2018-06-04 19:20:20.012695 7f96a03d3f00  4 rocksdb:               Options.comparator: leveldb.BytewiseComparator", "2018-06-04 19:20:20.012696 7f96a03d3f00  4 rocksdb:           Options.merge_operator: ", "2018-06-04 19:20:20.012697 7f96a03d3f00  4 rocksdb:        Options.compaction_filter: None", "2018-06-04 19:20:20.012698 7f96a03d3f00  4 rocksdb:        Options.compaction_filter_factory: None", "2018-06-04 19:20:20.012699 7f96a03d3f00  4 rocksdb:         Options.memtable_factory: SkipListFactory", "2018-06-04 19:20:20.012700 7f96a03d3f00  4 rocksdb:            Options.table_factory: BlockBasedTable", "2018-06-04 19:20:20.012720 7f96a03d3f00  4 rocksdb:            table_factory options:   flush_block_policy_factory: FlushBlockBySizePolicyFactory (0x55ababdaa0f8)", "  cache_index_and_filter_blocks: 1", "  cache_index_and_filter_blocks_with_high_priority: 1", "  pin_l0_filter_and_index_blocks_in_cache: 1", "  index_type: 0", "  hash_index_allow_collision: 1", "  checksum: 1", "  no_block_cache: 0", "  block_cache: 0x55abac0b65c0", "  block_cache_name: LRUCache", "  block_cache_options:", "    capacity : 134217728", "    num_shard_bits : 4", "    strict_capacity_limit : 0", "    high_pri_pool_ratio: 0.000", "  block_cache_compressed: (nil)", "  persistent_cache: (nil)", "  block_size: 4096", "  block_size_deviation: 10", "  block_restart_interval: 16", "  index_block_restart_interval: 1", "  filter_policy: rocksdb.BuiltinBloomFilter", "  whole_key_filtering: 1", "  format_version: 2", "", "2018-06-04 19:20:20.012725 7f96a03d3f00  4 rocksdb:        Options.write_buffer_size: 33554432", "2018-06-04 19:20:20.012726 7f96a03d3f00  4 rocksdb:  Options.max_write_buffer_number: 2", "2018-06-04 19:20:20.012727 7f96a03d3f00  4 rocksdb:          Options.compression: NoCompression", "2018-06-04 19:20:20.012728 7f96a03d3f00  4 rocksdb:                  Options.bottommost_compression: Disabled", "2018-06-04 19:20:20.012729 7f96a03d3f00  4 rocksdb:       Options.prefix_extractor: nullptr", "2018-06-04 19:20:20.012730 7f96a03d3f00  4 rocksdb:   Options.memtable_insert_with_hint_prefix_extractor: nullptr", "2018-06-04 19:20:20.012731 7f96a03d3f00  4 rocksdb:             Options.num_levels: 7", "2018-06-04 19:20:20.012731 7f96a03d3f00  4 rocksdb:        Options.min_write_buffer_number_to_merge: 1", "2018-06-04 19:20:20.012732 7f96a03d3f00  4 rocksdb:     Options.max_write_buffer_number_to_maintain: 0", "2018-06-04 19:20:20.012733 7f96a03d3f00  4 rocksdb:            Options.compression_opts.window_bits: -14", "2018-06-04 19:20:20.012734 7f96a03d3f00  4 rocksdb:                  Options.compression_opts.level: -1", "2018-06-04 19:20:20.012735 7f96a03d3f00  4 rocksdb:               Options.compression_opts.strategy: 0", "2018-06-04 19:20:20.012735 7f96a03d3f00  4 rocksdb:         Options.compression_opts.max_dict_bytes: 0", "2018-06-04 19:20:20.012736 7f96a03d3f00  4 rocksdb:      Options.level0_file_num_compaction_trigger: 4", "2018-06-04 19:20:20.012737 7f96a03d3f00  4 rocksdb:          Options.level0_slowdown_writes_trigger: 20", "2018-06-04 19:20:20.012737 7f96a03d3f00  4 rocksdb:              Options.level0_stop_writes_trigger: 36", "2018-06-04 19:20:20.012738 7f96a03d3f00  4 rocksdb:                   Options.target_file_size_base: 67108864", "2018-06-04 19:20:20.012738 7f96a03d3f00  4 rocksdb:             Options.target_file_size_multiplier: 1", "2018-06-04 19:20:20.012739 7f96a03d3f00  4 rocksdb:                Options.max_bytes_for_level_base: 268435456", "2018-06-04 19:20:20.012746 7f96a03d3f00  4 rocksdb: Options.level_compaction_dynamic_level_bytes: 0", "2018-06-04 19:20:20.012747 7f96a03d3f00  4 rocksdb:          Options.max_bytes_for_level_multiplier: 10.000000", "2018-06-04 19:20:20.012750 7f96a03d3f00  4 rocksdb: Options.max_bytes_for_level_multiplier_addtl[0]: 1", "2018-06-04 19:20:20.012751 7f96a03d3f00  4 rocksdb: Options.max_bytes_for_level_multiplier_addtl[1]: 1", "2018-06-04 19:20:20.012751 7f96a03d3f00  4 rocksdb: Options.max_bytes_for_level_multiplier_addtl[2]: 1", "2018-06-04 19:20:20.012752 7f96a03d3f00  4 rocksdb: Options.max_bytes_for_level_multiplier_addtl[3]: 1", "2018-06-04 19:20:20.012753 7f96a03d3f00  4 rocksdb: Options.max_bytes_for_level_multiplier_addtl[4]: 1", "2018-06-04 19:20:20.012753 7f96a03d3f00  4 rocksdb: Options.max_bytes_for_level_multiplier_addtl[5]: 1", "2018-06-04 19:20:20.012754 7f96a03d3f00  4 rocksdb: Options.max_bytes_for_level_multiplier_addtl[6]: 1", "2018-06-04 19:20:20.012755 7f96a03d3f00  4 rocksdb:       Options.max_sequential_skip_in_iterations: 8", "2018-06-04 19:20:20.012755 7f96a03d3f00  4 rocksdb:                    Options.max_compaction_bytes: 1677721600", "2018-06-04 19:20:20.012756 7f96a03d3f00  4 rocksdb:                        Options.arena_block_size: 4194304", "2018-06-04 19:20:20.012757 7f96a03d3f00  4 rocksdb:   Options.soft_pending_compaction_bytes_limit: 68719476736", "2018-06-04 19:20:20.012758 7f96a03d3f00  4 rocksdb:   Options.hard_pending_compaction_bytes_limit: 274877906944", "2018-06-04 19:20:20.012758 7f96a03d3f00  4 rocksdb:       Options.rate_limit_delay_max_milliseconds: 100", "2018-06-04 19:20:20.012759 7f96a03d3f00  4 rocksdb:                Options.disable_auto_compactions: 0", "2018-06-04 19:20:20.012761 7f96a03d3f00  4 rocksdb:                         Options.compaction_style: kCompactionStyleLevel", "2018-06-04 19:20:20.012762 7f96a03d3f00  4 rocksdb:                           Options.compaction_pri: kByCompensatedSize", "2018-06-04 19:20:20.012762 7f96a03d3f00  4 rocksdb:  Options.compaction_options_universal.size_ratio: 1", "2018-06-04 19:20:20.012763 7f96a03d3f00  4 rocksdb: Options.compaction_options_universal.min_merge_width: 2", "2018-06-04 19:20:20.012764 7f96a03d3f00  4 rocksdb: Options.compaction_options_universal.max_merge_width: 4294967295", "2018-06-04 19:20:20.012765 7f96a03d3f00  4 rocksdb: Options.compaction_options_universal.max_size_amplification_percent: 200", "2018-06-04 19:20:20.012766 7f96a03d3f00  4 rocksdb: Options.compaction_options_universal.compression_size_percent: -1", "2018-06-04 19:20:20.012766 7f96a03d3f00  4 rocksdb: Options.compaction_options_fifo.max_table_files_size: 1073741824", "2018-06-04 19:20:20.012767 7f96a03d3f00  4 rocksdb:                   Options.table_properties_collectors: ", "2018-06-04 19:20:20.012768 7f96a03d3f00  4 rocksdb:                   Options.inplace_update_support: 0", "2018-06-04 19:20:20.012769 7f96a03d3f00  4 rocksdb:                 Options.inplace_update_num_locks: 10000", "2018-06-04 19:20:20.012770 7f96a03d3f00  4 rocksdb:               Options.memtable_prefix_bloom_size_ratio: 0.000000", "2018-06-04 19:20:20.012771 7f96a03d3f00  4 rocksdb:   Options.memtable_huge_page_size: 0", "2018-06-04 19:20:20.012772 7f96a03d3f00  4 rocksdb:                           Options.bloom_locality: 0", "2018-06-04 19:20:20.012773 7f96a03d3f00  4 rocksdb:                    Options.max_successive_merges: 0", "2018-06-04 19:20:20.012774 7f96a03d3f00  4 rocksdb:                Options.optimize_filters_for_hits: 0", "2018-06-04 19:20:20.012774 7f96a03d3f00  4 rocksdb:                Options.paranoid_file_checks: 0", "2018-06-04 19:20:20.012775 7f96a03d3f00  4 rocksdb:                Options.force_consistency_checks: 0", "2018-06-04 19:20:20.012776 7f96a03d3f00  4 rocksdb:                Options.report_bg_io_stats: 0", "2018-06-04 19:20:20.014299 7f96a03d3f00  4 rocksdb: [/build/ceph-12.2.5/src/rocksdb/db/version_set.cc:2859] Recovered from manifest file:/var/lib/ceph/mon/ceph-node/store.db/MANIFEST-000001 succeeded,manifest_file_number is 1, next_file_number is 3, last_sequence is 0, log_number is 0,prev_log_number is 0,max_column_family is 0", "", "2018-06-04 19:20:20.014318 7f96a03d3f00  4 rocksdb: [/build/ceph-12.2.5/src/rocksdb/db/version_set.cc:2867] Column family [default] (ID 0), log number is 0", "", "2018-06-04 19:20:20.014444 7f96a03d3f00  4 rocksdb: EVENT_LOG_v1 {\"time_micros\": 1528140020014427, \"job\": 1, \"event\": \"recovery_started\", \"log_files\": [3]}", "2018-06-04 19:20:20.014454 7f96a03d3f00  4 rocksdb: [/build/ceph-12.2.5/src/rocksdb/db/db_impl_open.cc:482] Recovering log #3 mode 2", "2018-06-04 19:20:20.019673 7f96a03d3f00  4 rocksdb: EVENT_LOG_v1 {\"time_micros\": 1528140020019662, \"cf_name\": \"default\", \"job\": 1, \"event\": \"table_file_creation\", \"file_number\": 4, \"file_size\": 1420, \"table_properties\": {\"data_size\": 463, \"index_size\": 28, \"filter_size\": 20, \"raw_key_size\": 95, \"raw_average_key_size\": 23, \"raw_value_size\": 359, \"raw_average_value_size\": 89, \"num_data_blocks\": 1, \"num_entries\": 4, \"filter_policy_name\": \"rocksdb.BuiltinBloomFilter\", \"kDeletedKeys\": \"0\", \"kMergeOperands\": \"0\"}}", "2018-06-04 19:20:20.019704 7f96a03d3f00  4 rocksdb: [/build/ceph-12.2.5/src/rocksdb/db/version_set.cc:2395] Creating manifest 5", "", "2018-06-04 19:20:20.028994 7f96a03d3f00  4 rocksdb: EVENT_LOG_v1 {\"time_micros\": 1528140020028990, \"job\": 1, \"event\": \"recovery_finished\"}", "2018-06-04 19:20:20.039038 7f96a03d3f00  4 rocksdb: [/build/ceph-12.2.5/src/rocksdb/db/db_impl_open.cc:1063] DB pointer 0x55abac1ca000", "2018-06-04 19:20:20.040153 7f96a03d3f00  0 starting mon.node rank 0 at public addr 128.110.153.197:6789/0 at bind addr 128.110.153.197:6789/0 mon_data /var/lib/ceph/mon/ceph-node fsid b2bb6c0b-6cc6-44f1-bf92-1ce81d78fbdc", "2018-06-04 19:20:20.040315 7f96a03d3f00  0 starting mon.node rank 0 at 128.110.153.197:6789/0 mon_data /var/lib/ceph/mon/ceph-node fsid b2bb6c0b-6cc6-44f1-bf92-1ce81d78fbdc", "2018-06-04 19:20:20.041162 7f96a03d3f00  1 mon.node@-1(probing) e0 preinit fsid b2bb6c0b-6cc6-44f1-bf92-1ce81d78fbdc", "2018-06-04 19:20:20.041327 7f96a03d3f00  1 mon.node@-1(probing).mds e0 Unable to load 'last_metadata'", "2018-06-04 19:20:20.042578 7f96a03d3f00  0 mon.node@-1(probing) e0  my rank is now 0 (was -1)", "2018-06-04 19:20:20.042607 7f96a03d3f00  1 mon.node@0(probing) e0 win_standalone_election", "2018-06-04 19:20:20.042621 7f96a03d3f00  1 mon.node@0(probing).elector(0) init, first boot, initializing epoch at 1 ", "2018-06-04 19:20:20.051554 7f96a03d3f00  0 log_channel(cluster) log [INF] : mon.node is new leader, mons node in quorum (ranks 0)", "2018-06-04 19:20:20.056409 7f96a03d3f00  1 mon.node@0(leader).osd e0 create_pending setting backfillfull_ratio = 0.9", "2018-06-04 19:20:20.056448 7f96a03d3f00  1 mon.node@0(leader).osd e0 create_pending setting full_ratio = 0.95", "2018-06-04 19:20:20.056452 7f96a03d3f00  1 mon.node@0(leader).osd e0 create_pending setting nearfull_ratio = 0.85", "2018-06-04 19:20:20.056730 7f96a03d3f00  1 mon.node@0(leader).osd e0 encode_pending skipping prime_pg_temp; mapping job did not start", "2018-06-04 19:20:20.057897 7f96a03d3f00  1 mon.node@0(leader) e0 _apply_compatset_features enabling new quorum features: compat={},rocompat={},incompat={4=support erasure code pools,5=new-style osdmap encoding,6=support isa/lrc erasure code,7=support shec erasure code}", "2018-06-04 19:20:20.066285 7f96929dd700  1 mon.node@0(leader).paxosservice(pgmap 0..0) refresh upgraded, format 1 -> 0", "2018-06-04 19:20:20.066301 7f96929dd700  1 mon.node@0(leader).pg v0 on_upgrade discarding in-core PGMap", "2018-06-04 19:20:20.070775 7f96929dd700  1 mon.node@0(leader).paxosservice(auth 0..0) refresh upgraded, format 2 -> 0", "2018-06-04 19:20:20.071057 7f96929dd700  1 mon.node@0(probing) e1 win_standalone_election", "2018-06-04 19:20:20.071072 7f96929dd700  1 mon.node@0(probing).elector(2) init, last seen epoch 2", "2018-06-04 19:20:20.076749 7f96929dd700  0 log_channel(cluster) log [INF] : mon.node is new leader, mons node in quorum (ranks 0)", "2018-06-04 19:20:20.081299 7f96929dd700  0 log_channel(cluster) log [DBG] : monmap e1: 1 mons at {node=128.110.153.197:6789/0}", "2018-06-04 19:20:20.086423 7f96929dd700  1 mon.node@0(leader).osd e0 create_pending setting backfillfull_ratio = 0.9", "2018-06-04 19:20:20.086441 7f96929dd700  1 mon.node@0(leader).osd e0 create_pending setting full_ratio = 0.95", "2018-06-04 19:20:20.086446 7f96929dd700  1 mon.node@0(leader).osd e0 create_pending setting nearfull_ratio = 0.85", "2018-06-04 19:20:20.086670 7f96929dd700  1 mon.node@0(leader).osd e0 encode_pending skipping prime_pg_temp; mapping job did not start", "2018-06-04 19:20:20.087442 7f96929dd700  1 mon.node@0(leader) e1 _apply_compatset_features enabling new quorum features: compat={},rocompat={},incompat={8=support monmap features,9=luminous ondisk layout}", "2018-06-04 19:20:20.096456 7f96929dd700  1 mon.node@0(leader).paxosservice(auth 0..0) refresh upgraded, format 2 -> 0", "2018-06-04 19:20:20.096576 7f96929dd700  0 log_channel(cluster) log [INF] : pgmap 0 pgs: ; 0 bytes data, 0 kB used, 0 kB / 0 kB avail", "2018-06-04 19:20:20.106242 7f96929dd700  0 mon.node@0(leader).mds e1 print_map", "e1", "enable_multiple, ever_enabled_multiple: 0,0", "compat: compat={},rocompat={},incompat={1=base v0.20,2=client writeable ranges,3=default file layouts on dirs,4=dir inode in separate object,5=mds uses versioned encoding,6=dirfrag is stored in omap,8=no anchor table,9=file layout v2}", "legacy client fscid: -1", " ", "No filesystems configured", "", "2018-06-04 19:20:20.106401 7f96929dd700  1 mon.node@0(leader).osd e1 e1: 0 total, 0 up, 0 in", "2018-06-04 19:20:20.110683 7f96929dd700  0 mon.node@0(leader).osd e1 crush map has features 288514050185494528, adjusting msgr requires", "2018-06-04 19:20:20.110692 7f96929dd700  0 mon.node@0(leader).osd e1 crush map has features 288514050185494528, adjusting msgr requires", "2018-06-04 19:20:20.110698 7f96929dd700  0 mon.node@0(leader).osd e1 crush map has features 1009089990564790272, adjusting msgr requires", "2018-06-04 19:20:20.110701 7f96929dd700  0 mon.node@0(leader).osd e1 crush map has features 288514050185494528, adjusting msgr requires", "2018-06-04 19:20:20.110899 7f96929dd700  1 mon.node@0(leader).log v1 unable to write to '/var/log/ceph/ceph.log' for channel 'cluster': (2) No such file or directory", "2018-06-04 19:20:20.110959 7f96929dd700  1 mon.node@0(leader).paxosservice(auth 1..1) refresh upgraded, format 0 -> 2", "2018-06-04 19:20:20.111660 7f96929dd700  0 log_channel(cluster) log [DBG] : fsmap ", "2018-06-04 19:20:20.111764 7f96929dd700  1 mon.node@0(leader).pg v1 check_osd_map will clear pg_map data", "2018-06-04 19:20:20.111768 7f96929dd700  1 mon.node@0(leader).pg v1 encode_pending clearing pgmap data at v2", "2018-06-04 19:20:20.115757 7f96929dd700  0 log_channel(cluster) log [DBG] : osdmap e1: 0 total, 0 up, 0 in", "2018-06-04 19:20:20.115978 7f96929dd700  0 log_channel(cluster) log [DBG] : mgrmap e1: no daemons active", "2018-06-04 19:20:20.129417 7f96929dd700  1 mon.node@0(leader).log v2 unable to write to '/var/log/ceph/ceph.log' for channel 'cluster': (2) No such file or directory", "2018-06-04 19:20:20.361812 7f96971e6700  0 mon.node@0(leader) e1 handle_command mon_command({\"prefix\": \"status\"} v 0) v1", "2018-06-04 19:20:20.361853 7f96971e6700  0 log_channel(audit) log [DBG] : from='client.? 128.110.153.138:0/1904449781' entity='client.admin' cmd=[{\"prefix\": \"status\"}]: dispatch", "2018-06-04 19:20:20.410253 7f96971e6700  0 mon.node@0(leader) e1 handle_command mon_command({\"prefix\": \"auth get\", \"entity\": \"client.bootstrap-osd\"} v 0) v1", "2018-06-04 19:20:20.410300 7f96971e6700  0 log_channel(audit) log [INF] : from='client.? 128.110.153.77:0/4263726369' entity='client.admin' cmd=[{\"prefix\": \"auth get\", \"entity\": \"client.bootstrap-osd\"}]: dispatch", "2018-06-04 19:20:20.649783 7f96971e6700  0 mon.node@0(leader) e1 handle_command mon_command({\"pool2\": \"rbd\", \"prefix\": \"osd pool rm\", \"sure\": \"--yes-i-really-really-mean-it\", \"pool\": \"rbd\"} v 0) v1", "2018-06-04 19:20:20.649858 7f96971e6700  0 log_channel(audit) log [INF] : from='client.? 128.110.153.138:0/1117866057' entity='client.admin' cmd=[{\"pool2\": \"rbd\", \"prefix\": \"osd pool rm\", \"sure\": \"--yes-i-really-really-mean-it\", \"pool\": \"rbd\"}]: dispatch", "2018-06-04 19:20:20.916772 7f96971e6700  0 mon.node@0(leader) e1 handle_command mon_command({\"prefix\": \"osd pool create\", \"pg_num\": 150, \"pool\": \"scbench\", \"pgp_num\": 150} v 0) v1", "2018-06-04 19:20:20.916844 7f96971e6700  0 log_channel(audit) log [INF] : from='client.? 128.110.153.138:0/326234720' entity='client.admin' cmd=[{\"prefix\": \"osd pool create\", \"pg_num\": 150, \"pool\": \"scbench\", \"pgp_num\": 150}]: dispatch", "2018-06-04 19:20:21.100932 7f96971e6700  1 mon.node@0(leader).osd e1 encode_pending skipping prime_pg_temp; mapping job did not start", "2018-06-04 19:20:21.110505 7f96929dd700  1 mon.node@0(leader).osd e2 e2: 0 total, 0 up, 0 in", "2018-06-04 19:20:21.114976 7f96929dd700  0 mon.node@0(leader).osd e2 crush map has features 288514051259236352, adjusting msgr requires", "2018-06-04 19:20:21.115006 7f96929dd700  0 mon.node@0(leader).osd e2 crush map has features 288514051259236352, adjusting msgr requires", "2018-06-04 19:20:21.115014 7f96929dd700  0 mon.node@0(leader).osd e2 crush map has features 1009089991638532096, adjusting msgr requires", "2018-06-04 19:20:21.115017 7f96929dd700  0 mon.node@0(leader).osd e2 crush map has features 288514051259236352, adjusting msgr requires", "2018-06-04 19:20:21.115384 7f96929dd700  0 log_channel(audit) log [INF] : from='client.? 128.110.153.138:0/326234720' entity='client.admin' cmd='[{\"prefix\": \"osd pool create\", \"pg_num\": 150, \"pool\": \"scbench\", \"pgp_num\": 150}]': finished", "2018-06-04 19:20:21.115751 7f96929dd700  0 log_channel(cluster) log [DBG] : osdmap e2: 0 total, 0 up, 0 in", "2018-06-04 19:20:21.139732 7f96929dd700  1 mon.node@0(leader).log v3 unable to write to '/var/log/ceph/ceph.audit.log' for channel 'audit': (2) No such file or directory", "2018-06-04 19:20:21.139764 7f96929dd700  1 mon.node@0(leader).log v3 unable to write to '/var/log/ceph/ceph.log' for channel 'cluster': (2) No such file or directory", "2018-06-04 19:20:21.428815 7f96971e6700  0 mon.node@0(leader) e1 handle_command mon_command({\"var\": \"size\", \"prefix\": \"osd pool set\", \"pool\": \"scbench\", \"val\": \"1\"} v 0) v1", "2018-06-04 19:20:21.428901 7f96971e6700  0 log_channel(audit) log [INF] : from='client.? 128.110.153.138:0/3911707674' entity='client.admin' cmd=[{\"var\": \"size\", \"prefix\": \"osd pool set\", \"pool\": \"scbench\", \"val\": \"1\"}]: dispatch", "2018-06-04 19:20:22.149153 7f96929dd700  1 mon.node@0(leader).log v4 unable to write to '/var/log/ceph/ceph.audit.log' for channel 'audit': (2) No such file or directory", "2018-06-04 19:20:22.159162 7f96929dd700  1 mon.node@0(leader).osd e3 e3: 0 total, 0 up, 0 in", "2018-06-04 19:20:22.164492 7f96929dd700  0 log_channel(audit) log [INF] : from='client.? 128.110.153.138:0/3911707674' entity='client.admin' cmd='[{\"var\": \"size\", \"prefix\": \"osd pool set\", \"pool\": \"scbench\", \"val\": \"1\"}]': finished", "2018-06-04 19:20:22.164649 7f96929dd700  0 log_channel(cluster) log [DBG] : osdmap e3: 0 total, 0 up, 0 in", "2018-06-04 19:20:23.168593 7f96929dd700  1 mon.node@0(leader).log v5 unable to write to '/var/log/ceph/ceph.audit.log' for channel 'audit': (2) No such file or directory", "2018-06-04 19:21:20.042250 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB", "2018-06-04 19:22:20.042626 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB", "2018-06-04 19:23:20.043021 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB", "2018-06-04 19:24:20.043697 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB", "2018-06-04 19:25:20.044133 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB", "2018-06-04 19:26:20.044517 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB", "2018-06-04 19:27:20.044909 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB", "2018-06-04 19:28:20.045294 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB", "2018-06-04 19:29:20.045704 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB", "2018-06-04 19:30:20.046090 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB", "2018-06-04 19:31:20.046475 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB", "2018-06-04 19:32:20.046879 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB", "2018-06-04 19:33:20.047264 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB", "2018-06-04 19:34:20.047676 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB", "2018-06-04 19:35:20.048091 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB", "2018-06-04 19:36:20.048478 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB", "2018-06-04 19:37:20.048859 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB", "2018-06-04 19:38:20.049290 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB", "2018-06-04 19:39:20.049710 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB", "2018-06-04 19:40:20.050169 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB", "2018-06-04 19:41:20.050547 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB", "2018-06-04 19:42:20.050939 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB", "2018-06-04 19:43:20.051386 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB", "2018-06-04 19:44:20.051814 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB", "2018-06-04 19:45:20.052233 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB", "2018-06-04 19:46:20.052620 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB", "2018-06-04 19:47:20.053001 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB", "2018-06-04 19:48:20.053384 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB", "2018-06-04 19:49:20.053780 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB", "2018-06-04 19:50:20.054200 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB", "2018-06-04 19:51:20.054583 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB", "2018-06-04 19:52:20.054925 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB", "2018-06-04 19:53:20.055349 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB", "2018-06-04 19:54:20.055736 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB", "2018-06-04 19:55:20.056115 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB", "2018-06-04 19:56:20.056540 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB", "2018-06-04 19:57:20.056956 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB", "2018-06-04 19:58:20.057369 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB", "2018-06-04 19:59:20.057814 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB", "2018-06-04 20:00:20.058194 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB", "2018-06-04 20:01:20.058598 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB", "2018-06-04 20:02:20.059027 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB", "2018-06-04 20:03:20.059414 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB", "2018-06-04 20:04:20.059801 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB", "2018-06-04 20:05:20.060186 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB", "2018-06-04 20:06:20.060626 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB", "2018-06-04 20:07:20.061069 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB", "2018-06-04 20:08:20.061531 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB", "2018-06-04 20:09:20.061934 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB", "2018-06-04 20:10:20.062349 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB", "2018-06-04 20:11:20.062723 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB", "2018-06-04 20:12:20.063125 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB", "2018-06-04 20:13:20.063551 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB", "2018-06-04 20:14:20.063956 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB", "2018-06-04 20:15:20.064399 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB", "2018-06-04 20:16:20.064810 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB", "2018-06-04 20:17:20.065197 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB", "2018-06-04 20:18:20.065678 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB", "2018-06-04 20:19:20.066078 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB", "2018-06-04 20:20:20.066456 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB", "2018-06-04 20:21:20.066842 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB", "2018-06-04 20:22:20.067330 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB", "2018-06-04 20:23:20.067747 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB", "2018-06-04 20:24:20.068128 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB", "2018-06-04 20:25:20.068522 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB", "2018-06-04 20:26:20.069036 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB", "2018-06-04 20:27:20.069427 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB", "2018-06-04 20:28:20.069888 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB", "2018-06-04 20:29:20.070281 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB", "2018-06-04 20:30:20.070689 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB", "2018-06-04 20:31:20.071128 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB", "2018-06-04 20:32:20.071608 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB", "2018-06-04 20:33:20.072155 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB", "2018-06-04 20:34:20.072584 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB", "2018-06-04 20:35:20.072981 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB", "2018-06-04 20:36:20.073583 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB", "2018-06-04 20:37:20.074147 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB", "2018-06-04 20:38:20.074694 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB", "2018-06-04 20:39:20.075176 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB", "2018-06-04 20:40:20.075571 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB", "2018-06-04 20:41:20.075901 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB", "2018-06-04 20:42:20.076323 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB", "2018-06-04 20:43:20.076711 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB", "2018-06-04 20:44:20.077257 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB", "2018-06-04 20:45:20.077763 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB", "2018-06-04 20:46:20.078147 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB", "2018-06-04 20:47:20.078555 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB", "2018-06-04 20:48:20.078946 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB", "2018-06-04 20:49:20.079337 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB", "2018-06-04 20:50:20.079712 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB", "2018-06-04 20:51:20.080093 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB", "2018-06-04 20:52:20.080477 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB", "2018-06-04 20:53:20.080868 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB", "2018-06-04 20:54:20.081260 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12382 MB", "2018-06-04 20:55:20.081700 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12381 MB", "2018-06-04 20:56:20.082089 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12381 MB", "2018-06-04 20:57:20.082544 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12381 MB", "2018-06-04 20:58:20.082969 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12381 MB", "2018-06-04 20:59:20.083566 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12381 MB", "2018-06-04 21:00:20.083950 7f96999eb700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12381 MB", "2018-06-04 21:00:27.677151 7f96949e1700 -1 received  signal: Terminated from  PID: 1 task name: /bin/bash /entrypoint.sh mon  UID: 0", "2018-06-04 21:00:27.677185 7f96949e1700 -1 mon.node@0(leader) e1 *** Got Signal Terminated ***", "2018-06-04 21:00:27.677210 7f96949e1700  1 mon.node@0(leader) e1 shutdown"], "stdout": "creating /var/lib/ceph/bootstrap-osd/ceph.keyring\ncreating /var/lib/ceph/bootstrap-mds/ceph.keyring\ncreating /var/lib/ceph/bootstrap-rgw/ceph.keyring\ncreating /var/lib/ceph/bootstrap-rbd/ceph.keyring\nmonmaptool: monmap file /etc/ceph/monmap-ceph\nmonmaptool: set fsid to b2bb6c0b-6cc6-44f1-bf92-1ce81d78fbdc\nmonmaptool: writing epoch 0 to /etc/ceph/monmap-ceph (1 monitors)\nimporting contents of /var/lib/ceph/bootstrap-osd/ceph.keyring into /etc/ceph/ceph.mon.keyring\nimporting contents of /var/lib/ceph/bootstrap-mds/ceph.keyring into /etc/ceph/ceph.mon.keyring\nimporting contents of /var/lib/ceph/bootstrap-rgw/ceph.keyring into /etc/ceph/ceph.mon.keyring\nimporting contents of /var/lib/ceph/bootstrap-rbd/ceph.keyring into /etc/ceph/ceph.mon.keyring\nimporting contents of /etc/ceph/ceph.client.admin.keyring into /etc/ceph/ceph.mon.keyring\n2018-06-04 19:20:19  /entrypoint.sh: SUCCESS\nexec: PID 122: spawning /usr/bin/ceph-mon --cluster ceph --setuser ceph --setgroup ceph -d -i node --mon-data /var/lib/ceph/mon/ceph-node --public-addr 128.110.153.197:6789\nSending SIGTERM to PID 122", "stdout_lines": ["creating /var/lib/ceph/bootstrap-osd/ceph.keyring", "creating /var/lib/ceph/bootstrap-mds/ceph.keyring", "creating /var/lib/ceph/bootstrap-rgw/ceph.keyring", "creating /var/lib/ceph/bootstrap-rbd/ceph.keyring", "monmaptool: monmap file /etc/ceph/monmap-ceph", "monmaptool: set fsid to b2bb6c0b-6cc6-44f1-bf92-1ce81d78fbdc", "monmaptool: writing epoch 0 to /etc/ceph/monmap-ceph (1 monitors)", "importing contents of /var/lib/ceph/bootstrap-osd/ceph.keyring into /etc/ceph/ceph.mon.keyring", "importing contents of /var/lib/ceph/bootstrap-mds/ceph.keyring into /etc/ceph/ceph.mon.keyring", "importing contents of /var/lib/ceph/bootstrap-rgw/ceph.keyring into /etc/ceph/ceph.mon.keyring", "importing contents of /var/lib/ceph/bootstrap-rbd/ceph.keyring into /etc/ceph/ceph.mon.keyring", "importing contents of /etc/ceph/ceph.client.admin.keyring into /etc/ceph/ceph.mon.keyring", "2018-06-04 19:20:19  /entrypoint.sh: SUCCESS", "exec: PID 122: spawning /usr/bin/ceph-mon --cluster ceph --setuser ceph --setgroup ceph -d -i node --mon-data /var/lib/ceph/mon/ceph-node --public-addr 128.110.153.197:6789", "Sending SIGTERM to PID 122"]}
2018-06-04 21:00:40,638 p=10 u=root |  ...ignoring
2018-06-04 21:00:40,639 p=10 u=root |  fatal: [node3]: FAILED! => {"ansible_job_id": "940961924454.19276", "changed": true, "cmd": "docker run --rm --name baseliner_node3  -e repetition=\"1\" -e CEPH_PUBLIC_NETWORK=\"128.110.152.0/22\" -e CLIENT=\"True\" -e MON_IP=\"128.110.153.197\" --net=host -v /tmp/etc:/etc/ceph -v /tmp/baseliner_output_node3:/results -v /tmp/baseliner_output_node3:/output mariettesouppe/rados_bench:master-ec8d33f-luminous-ubuntu-16.04-x86_64 ", "delta": "1:40:18.646457", "end": "2018-06-04 15:00:37.835629", "failed": true, "finished": 1, "msg": "non-zero return code", "rc": 137, "start": "2018-06-04 13:20:19.189172", "stderr": "+ '[' -z '' ']'\n+ '[' -z '' ']'\n+ '[' -z True ']'\n+ '[' -z 128.110.152.0/22 ']'\n+ '[' -z 128.110.153.197 ']'\n+ sed -i -e s@monip@128.110.153.197@ /etc/ceph/ceph.conf\n+ sed -i -e s@cephnet@128.110.152.0/22@ /etc/ceph/ceph.conf\n+ '[' -f /etc/ceph/extra.conf ']'\n+ '[' -n '' ']'\n+ '[' -n '' ']'\n+ '[' -n True ']'\n+ '[' -z '' ']'\n+ echo 'No CLIENT_SCRIPT defined, running built-in radosbench script.'\n+ CLIENT_SCRIPT=run_radosbench\n+ ceph_health_ok\n+ exec run_radosbench\n+ ceph osd pool rm rbd rbd --yes-i-really-really-mean-it\npool 'rbd' does not exist\n+ ceph osd pool create scbench 150 150\npool 'scbench' created\n+ ceph osd pool set scbench size 1\nset pool 1 size to 1\n+ rados bench -p scbench 30 write --no-cleanup", "stderr_lines": ["+ '[' -z '' ']'", "+ '[' -z '' ']'", "+ '[' -z True ']'", "+ '[' -z 128.110.152.0/22 ']'", "+ '[' -z 128.110.153.197 ']'", "+ sed -i -e s@monip@128.110.153.197@ /etc/ceph/ceph.conf", "+ sed -i -e s@cephnet@128.110.152.0/22@ /etc/ceph/ceph.conf", "+ '[' -f /etc/ceph/extra.conf ']'", "+ '[' -n '' ']'", "+ '[' -n '' ']'", "+ '[' -n True ']'", "+ '[' -z '' ']'", "+ echo 'No CLIENT_SCRIPT defined, running built-in radosbench script.'", "+ CLIENT_SCRIPT=run_radosbench", "+ ceph_health_ok", "+ exec run_radosbench", "+ ceph osd pool rm rbd rbd --yes-i-really-really-mean-it", "pool 'rbd' does not exist", "+ ceph osd pool create scbench 150 150", "pool 'scbench' created", "+ ceph osd pool set scbench size 1", "set pool 1 size to 1", "+ rados bench -p scbench 30 write --no-cleanup"], "stdout": "No CLIENT_SCRIPT defined, running built-in radosbench script.", "stdout_lines": ["No CLIENT_SCRIPT defined, running built-in radosbench script."]}
2018-06-04 21:00:40,639 p=10 u=root |  ...ignoring
2018-06-04 21:00:40,642 p=10 u=root |  fatal: [node2]: FAILED! => {"ansible_job_id": "814716083344.18477", "changed": true, "cmd": "docker run --rm --name baseliner_node2  -e repetition=\"1\" -e CEPH_PUBLIC_NETWORK=\"128.110.152.0/22\" -e MON_IP=\"128.110.153.197\" -e OSD=\"True\" -e OSD_DEVICES=\"/dev/sdb\" --net=host -v /tmp/etc:/etc/ceph -v /tmp/baseliner_output_node2:/results -v /tmp/baseliner_output_node2:/output mariettesouppe/rados_bench:master-ec8d33f-luminous-ubuntu-16.04-x86_64 ", "delta": "0:00:01.384079", "end": "2018-06-04 13:20:20.568824", "failed": true, "finished": 1, "msg": "non-zero return code", "rc": 2, "start": "2018-06-04 13:20:19.184745", "stderr": "+ '[' -z '' ']'\n+ '[' -z True ']'\n+ '[' -z 128.110.152.0/22 ']'\n+ '[' -z 128.110.153.197 ']'\n+ sed -i -e s@monip@128.110.153.197@ /etc/ceph/ceph.conf\n+ sed -i -e s@cephnet@128.110.152.0/22@ /etc/ceph/ceph.conf\n+ '[' -f /etc/ceph/extra.conf ']'\n+ '[' -n '' ']'\n+ '[' -n True ']'\n+ '[' -n '' ']'\n+ exec run_osds\n+ '[' -z /dev/sdb ']'\n+ ceph auth get client.bootstrap-osd -o /var/lib/ceph/bootstrap-osd/ceph.keyring\nError ENOENT: failed to find client.bootstrap-osd in keyring", "stderr_lines": ["+ '[' -z '' ']'", "+ '[' -z True ']'", "+ '[' -z 128.110.152.0/22 ']'", "+ '[' -z 128.110.153.197 ']'", "+ sed -i -e s@monip@128.110.153.197@ /etc/ceph/ceph.conf", "+ sed -i -e s@cephnet@128.110.152.0/22@ /etc/ceph/ceph.conf", "+ '[' -f /etc/ceph/extra.conf ']'", "+ '[' -n '' ']'", "+ '[' -n True ']'", "+ '[' -n '' ']'", "+ exec run_osds", "+ '[' -z /dev/sdb ']'", "+ ceph auth get client.bootstrap-osd -o /var/lib/ceph/bootstrap-osd/ceph.keyring", "Error ENOENT: failed to find client.bootstrap-osd in keyring"], "stdout": "", "stdout_lines": []}
2018-06-04 21:00:40,642 p=10 u=root |  ...ignoring
2018-06-04 21:00:40,655 p=10 u=root |  TASK [baseliner : set fact to store result of bench execution] *****************
2018-06-04 21:00:40,655 p=10 u=root |  Monday 04 June 2018  21:00:40 +0000 (0:00:00.948)       1:40:40.230 *********** 
2018-06-04 21:00:40,673 p=10 u=root |  ok: [node1 -> localhost]
2018-06-04 21:00:40,681 p=10 u=root |  TASK [baseliner : record elapsed time in output folder] ************************
2018-06-04 21:00:40,682 p=10 u=root |  Monday 04 June 2018  21:00:40 +0000 (0:00:00.026)       1:40:40.257 *********** 
2018-06-04 21:00:40,898 p=10 u=root |  changed: [node1 -> localhost]
2018-06-04 21:00:40,922 p=10 u=root |  changed: [node2 -> localhost]
2018-06-04 21:00:40,923 p=10 u=root |  changed: [node3 -> localhost]
2018-06-04 21:00:40,932 p=10 u=root |  TASK [baseliner : capture stdout] **********************************************
2018-06-04 21:00:40,933 p=10 u=root |  Monday 04 June 2018  21:00:40 +0000 (0:00:00.250)       1:40:40.508 *********** 
2018-06-04 21:00:41,162 p=10 u=root |  changed: [node1 -> localhost]
2018-06-04 21:00:41,164 p=10 u=root |  changed: [node2 -> localhost]
2018-06-04 21:00:41,169 p=10 u=root |  changed: [node3 -> localhost]
2018-06-04 21:00:41,178 p=10 u=root |  TASK [baseliner : capture stderr] **********************************************
2018-06-04 21:00:41,178 p=10 u=root |  Monday 04 June 2018  21:00:41 +0000 (0:00:00.245)       1:40:40.754 *********** 
2018-06-04 21:00:41,397 p=10 u=root |  changed: [node1 -> localhost]
2018-06-04 21:00:41,398 p=10 u=root |  changed: [node2 -> localhost]
2018-06-04 21:00:41,416 p=10 u=root |  changed: [node3 -> localhost]
2018-06-04 21:00:41,423 p=10 u=root |  TASK [baseliner : debug] *******************************************************
2018-06-04 21:00:41,423 p=10 u=root |  Monday 04 June 2018  21:00:41 +0000 (0:00:00.244)       1:40:40.998 *********** 
2018-06-04 21:00:41,458 p=10 u=root |  TASK [baseliner : run compose benchmark] ***************************************
2018-06-04 21:00:41,458 p=10 u=root |  Monday 04 June 2018  21:00:41 +0000 (0:00:00.035)       1:40:41.034 *********** 
2018-06-04 21:00:41,493 p=10 u=root |  TASK [baseliner : run script benchmark] ****************************************
2018-06-04 21:00:41,493 p=10 u=root |  Monday 04 June 2018  21:00:41 +0000 (0:00:00.034)       1:40:41.068 *********** 
2018-06-04 21:00:41,529 p=10 u=root |  TASK [baseliner : download results] ********************************************
2018-06-04 21:00:41,529 p=10 u=root |  Monday 04 June 2018  21:00:41 +0000 (0:00:00.035)       1:40:41.104 *********** 
2018-06-04 21:00:41,580 p=10 u=root |  included: /etc/ansible/roles/baseliner/tasks/download-output.yml for node1, node2, node3
2018-06-04 21:00:41,670 p=10 u=root |  TASK [baseliner : get any bench-specific files] ********************************
2018-06-04 21:00:41,670 p=10 u=root |  Monday 04 June 2018  21:00:41 +0000 (0:00:00.141)       1:40:41.245 *********** 
2018-06-04 21:00:43,040 p=10 u=root |  changed: [node1] => (item=/tmp/baseliner_output_node1)
2018-06-04 21:00:43,044 p=10 u=root |  changed: [node2] => (item=/tmp/baseliner_output_node2)
2018-06-04 21:00:43,083 p=10 u=root |  changed: [node3] => (item=/tmp/baseliner_output_node3)
2018-06-04 21:00:43,093 p=10 u=root |  TASK [baseliner : delete remote folder after transfer] *************************
2018-06-04 21:00:43,093 p=10 u=root |  Monday 04 June 2018  21:00:43 +0000 (0:00:01.423)       1:40:42.669 *********** 
2018-06-04 21:00:43,927 p=10 u=root |  changed: [node1] => (item=/tmp/baseliner_output_node1)
2018-06-04 21:00:43,930 p=10 u=root |  changed: [node3] => (item=/tmp/baseliner_output_node3)
2018-06-04 21:00:43,936 p=10 u=root |  changed: [node2] => (item=/tmp/baseliner_output_node2)
2018-06-04 21:00:43,944 p=10 u=root |  TASK [baseliner : check if we should fail fast] ********************************
2018-06-04 21:00:43,944 p=10 u=root |  Monday 04 June 2018  21:00:43 +0000 (0:00:00.851)       1:40:43.520 *********** 
2018-06-04 21:00:43,996 p=10 u=root |  included: /etc/ansible/roles/baseliner/tasks/fail-fast.yml for node1, node2, node3
2018-06-04 21:00:44,095 p=10 u=root |  TASK [baseliner : failfast for single-node mode] *******************************
2018-06-04 21:00:44,095 p=10 u=root |  Monday 04 June 2018  21:00:44 +0000 (0:00:00.150)       1:40:43.670 *********** 
2018-06-04 21:00:44,149 p=10 u=root |  fatal: [node1]: FAILED! => {"changed": false, "failed": true, "msg": "benchmark failed"}
2018-06-04 21:00:44,149 p=10 u=root |  NO MORE HOSTS LEFT *************************************************************
2018-06-04 21:00:44,150 p=10 u=root |  PLAY RECAP *********************************************************************
2018-06-04 21:00:44,150 p=10 u=root |  node1                      : ok=50   changed=17   unreachable=0    failed=1   
2018-06-04 21:00:44,150 p=10 u=root |  node2                      : ok=43   changed=14   unreachable=0    failed=0   
2018-06-04 21:00:44,150 p=10 u=root |  node3                      : ok=43   changed=14   unreachable=0    failed=0   
2018-06-04 21:00:44,150 p=10 u=root |  Monday 04 June 2018  21:00:44 +0000 (0:00:00.055)       1:40:43.726 *********** 
2018-06-04 21:00:44,151 p=10 u=root |  =============================================================================== 
2018-06-04 21:00:44,151 p=10 u=root |  baseliner : wait for containers in single-node mode and stop/kill them if they timeout  6019.50s
2018-06-04 21:00:44,151 p=10 u=root |  Gathering Facts --------------------------------------------------------- 3.98s
2018-06-04 21:00:44,151 p=10 u=root |  baseliner : run container ----------------------------------------------- 2.08s
2018-06-04 21:00:44,151 p=10 u=root |  baseliner : pull image -------------------------------------------------- 1.69s
2018-06-04 21:00:44,151 p=10 u=root |  baseliner : install facter ---------------------------------------------- 1.54s
2018-06-04 21:00:44,151 p=10 u=root |  baseliner : upload files ------------------------------------------------ 1.52s
2018-06-04 21:00:44,151 p=10 u=root |  baseliner : get any bench-specific files -------------------------------- 1.42s
2018-06-04 21:00:44,151 p=10 u=root |  baseliner : stop any running container ---------------------------------- 0.98s
2018-06-04 21:00:44,151 p=10 u=root |  baseliner : get result of container execution --------------------------- 0.95s
2018-06-04 21:00:44,151 p=10 u=root |  baseliner : remove containers to avoid name clashes --------------------- 0.89s
2018-06-04 21:00:44,152 p=10 u=root |  baseliner : check if image already exists ------------------------------- 0.88s
2018-06-04 21:00:44,152 p=10 u=root |  baseliner : delete remote folder after transfer ------------------------- 0.85s
2018-06-04 21:00:44,152 p=10 u=root |  baseliner : remove remote results folder -------------------------------- 0.84s
2018-06-04 21:00:44,152 p=10 u=root |  baseliner : create remote results folder -------------------------------- 0.82s
2018-06-04 21:00:44,152 p=10 u=root |  baseliner : store facts about remotes ----------------------------------- 0.78s
2018-06-04 21:00:44,152 p=10 u=root |  baseliner : record elapsed time in output folder ------------------------ 0.25s
2018-06-04 21:00:44,152 p=10 u=root |  baseliner : capture stdout ---------------------------------------------- 0.25s
2018-06-04 21:00:44,152 p=10 u=root |  baseliner : capture stderr ---------------------------------------------- 0.24s
2018-06-04 21:00:44,152 p=10 u=root |  baseliner : ensure results folder exists -------------------------------- 0.23s
2018-06-04 21:00:44,152 p=10 u=root |  baseliner : add host-specific environment to docker_flags --------------- 0.23s
2018-06-04 21:00:44,152 p=10 u=root |  Playbook run took 0 days, 1 hours, 40 minutes, 43 seconds
