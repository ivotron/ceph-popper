
2018-05-30 23:37:43,329 p=10 u=root |  PLAY [all] *********************************************************************
2018-05-30 23:37:43,336 p=10 u=root |  TASK [Gathering Facts] *********************************************************
2018-05-30 23:37:43,336 p=10 u=root |  Wednesday 30 May 2018  23:37:43 +0000 (0:00:00.043)       0:00:00.043 ********* 
2018-05-30 23:37:46,239 p=10 u=root |  ok: [node3]
2018-05-30 23:37:46,246 p=10 u=root |  ok: [node1]
2018-05-30 23:37:46,250 p=10 u=root |  ok: [node2]
2018-05-30 23:37:46,259 p=10 u=root |  TASK [baseliner : include_tasks] ***********************************************
2018-05-30 23:37:46,260 p=10 u=root |  Wednesday 30 May 2018  23:37:46 +0000 (0:00:02.923)       0:00:02.966 ********* 
2018-05-30 23:37:46,325 p=10 u=root |  included: /etc/ansible/roles/baseliner/tasks/check_preconditions.yml for node1, node2, node3
2018-05-30 23:37:46,334 p=10 u=root |  TASK [baseliner : ensure expected variables are defined] ***********************
2018-05-30 23:37:46,334 p=10 u=root |  Wednesday 30 May 2018  23:37:46 +0000 (0:00:00.074)       0:00:03.041 ********* 
2018-05-30 23:37:46,389 p=10 u=root |  ok: [node1] => {
    "changed": false, 
    "failed": false, 
    "msg": "All assertions passed"
}
2018-05-30 23:37:46,398 p=10 u=root |  ok: [node2] => {
    "changed": false, 
    "failed": false, 
    "msg": "All assertions passed"
}
2018-05-30 23:37:46,409 p=10 u=root |  ok: [node3] => {
    "changed": false, 
    "failed": false, 
    "msg": "All assertions passed"
}
2018-05-30 23:37:46,416 p=10 u=root |  TASK [baseliner : set remote results path if not defined] **********************
2018-05-30 23:37:46,417 p=10 u=root |  Wednesday 30 May 2018  23:37:46 +0000 (0:00:00.082)       0:00:03.123 ********* 
2018-05-30 23:37:46,475 p=10 u=root |  ok: [node1]
2018-05-30 23:37:46,478 p=10 u=root |  ok: [node2]
2018-05-30 23:37:46,490 p=10 u=root |  ok: [node3]
2018-05-30 23:37:46,499 p=10 u=root |  TASK [baseliner : ensure results folder exists] ********************************
2018-05-30 23:37:46,499 p=10 u=root |  Wednesday 30 May 2018  23:37:46 +0000 (0:00:00.082)       0:00:03.206 ********* 
2018-05-30 23:37:46,732 p=10 u=root |  ok: [node1 -> localhost]
2018-05-30 23:37:46,743 p=10 u=root |  TASK [baseliner : check that pre-tasks file exists] ****************************
2018-05-30 23:37:46,743 p=10 u=root |  Wednesday 30 May 2018  23:37:46 +0000 (0:00:00.243)       0:00:03.450 ********* 
2018-05-30 23:37:46,762 p=10 u=root |  TASK [baseliner : check that post-tasks file exists] ***************************
2018-05-30 23:37:46,762 p=10 u=root |  Wednesday 30 May 2018  23:37:46 +0000 (0:00:00.018)       0:00:03.468 ********* 
2018-05-30 23:37:46,800 p=10 u=root |  TASK [baseliner : ensure docker-engine is installed] ***************************
2018-05-30 23:37:46,800 p=10 u=root |  Wednesday 30 May 2018  23:37:46 +0000 (0:00:00.038)       0:00:03.507 ********* 
2018-05-30 23:37:46,833 p=10 u=root |  TASK [baseliner : install statically linked docker] ****************************
2018-05-30 23:37:46,833 p=10 u=root |  Wednesday 30 May 2018  23:37:46 +0000 (0:00:00.032)       0:00:03.540 ********* 
2018-05-30 23:37:46,871 p=10 u=root |  TASK [baseliner : ensure the docker daemon is running] *************************
2018-05-30 23:37:46,871 p=10 u=root |  Wednesday 30 May 2018  23:37:46 +0000 (0:00:00.038)       0:00:03.578 ********* 
2018-05-30 23:37:46,907 p=10 u=root |  TASK [baseliner : stop any running container] **********************************
2018-05-30 23:37:46,907 p=10 u=root |  Wednesday 30 May 2018  23:37:46 +0000 (0:00:00.035)       0:00:03.614 ********* 
2018-05-30 23:37:47,881 p=10 u=root |  fatal: [node1]: FAILED! => {"changed": true, "cmd": "docker stop `docker ps -q`", "delta": "0:00:00.034943", "end": "2018-05-30 17:37:47.842223", "failed": true, "msg": "non-zero return code", "rc": 1, "start": "2018-05-30 17:37:47.807280", "stderr": "\"docker stop\" requires at least 1 argument(s).\nSee 'docker stop --help'.\n\nUsage:  docker stop [OPTIONS] CONTAINER [CONTAINER...]\n\nStop one or more running containers", "stderr_lines": ["\"docker stop\" requires at least 1 argument(s).", "See 'docker stop --help'.", "", "Usage:  docker stop [OPTIONS] CONTAINER [CONTAINER...]", "", "Stop one or more running containers"], "stdout": "", "stdout_lines": []}
2018-05-30 23:37:47,881 p=10 u=root |  ...ignoring
2018-05-30 23:37:47,886 p=10 u=root |  fatal: [node3]: FAILED! => {"changed": true, "cmd": "docker stop `docker ps -q`", "delta": "0:00:00.039070", "end": "2018-05-30 17:37:47.847939", "failed": true, "msg": "non-zero return code", "rc": 1, "start": "2018-05-30 17:37:47.808869", "stderr": "\"docker stop\" requires at least 1 argument(s).\nSee 'docker stop --help'.\n\nUsage:  docker stop [OPTIONS] CONTAINER [CONTAINER...]\n\nStop one or more running containers", "stderr_lines": ["\"docker stop\" requires at least 1 argument(s).", "See 'docker stop --help'.", "", "Usage:  docker stop [OPTIONS] CONTAINER [CONTAINER...]", "", "Stop one or more running containers"], "stdout": "", "stdout_lines": []}
2018-05-30 23:37:47,886 p=10 u=root |  ...ignoring
2018-05-30 23:37:47,887 p=10 u=root |  fatal: [node2]: FAILED! => {"changed": true, "cmd": "docker stop `docker ps -q`", "delta": "0:00:00.040498", "end": "2018-05-30 17:37:47.846520", "failed": true, "msg": "non-zero return code", "rc": 1, "start": "2018-05-30 17:37:47.806022", "stderr": "\"docker stop\" requires at least 1 argument(s).\nSee 'docker stop --help'.\n\nUsage:  docker stop [OPTIONS] CONTAINER [CONTAINER...]\n\nStop one or more running containers", "stderr_lines": ["\"docker stop\" requires at least 1 argument(s).", "See 'docker stop --help'.", "", "Usage:  docker stop [OPTIONS] CONTAINER [CONTAINER...]", "", "Stop one or more running containers"], "stdout": "", "stdout_lines": []}
2018-05-30 23:37:47,887 p=10 u=root |  ...ignoring
2018-05-30 23:37:47,895 p=10 u=root |  TASK [baseliner : remove containers to avoid name clashes] *********************
2018-05-30 23:37:47,895 p=10 u=root |  Wednesday 30 May 2018  23:37:47 +0000 (0:00:00.988)       0:00:04.602 ********* 
2018-05-30 23:37:48,727 p=10 u=root |  fatal: [node2]: FAILED! => {"changed": true, "cmd": "docker rm `docker ps -qa`", "delta": "0:00:00.037056", "end": "2018-05-30 17:37:48.690891", "failed": true, "msg": "non-zero return code", "rc": 1, "start": "2018-05-30 17:37:48.653835", "stderr": "\"docker rm\" requires at least 1 argument(s).\nSee 'docker rm --help'.\n\nUsage:  docker rm [OPTIONS] CONTAINER [CONTAINER...]\n\nRemove one or more containers", "stderr_lines": ["\"docker rm\" requires at least 1 argument(s).", "See 'docker rm --help'.", "", "Usage:  docker rm [OPTIONS] CONTAINER [CONTAINER...]", "", "Remove one or more containers"], "stdout": "", "stdout_lines": []}
2018-05-30 23:37:48,727 p=10 u=root |  ...ignoring
2018-05-30 23:37:48,734 p=10 u=root |  fatal: [node3]: FAILED! => {"changed": true, "cmd": "docker rm `docker ps -qa`", "delta": "0:00:00.044397", "end": "2018-05-30 17:37:48.695181", "failed": true, "msg": "non-zero return code", "rc": 1, "start": "2018-05-30 17:37:48.650784", "stderr": "\"docker rm\" requires at least 1 argument(s).\nSee 'docker rm --help'.\n\nUsage:  docker rm [OPTIONS] CONTAINER [CONTAINER...]\n\nRemove one or more containers", "stderr_lines": ["\"docker rm\" requires at least 1 argument(s).", "See 'docker rm --help'.", "", "Usage:  docker rm [OPTIONS] CONTAINER [CONTAINER...]", "", "Remove one or more containers"], "stdout": "", "stdout_lines": []}
2018-05-30 23:37:48,734 p=10 u=root |  ...ignoring
2018-05-30 23:37:48,736 p=10 u=root |  fatal: [node1]: FAILED! => {"changed": true, "cmd": "docker rm `docker ps -qa`", "delta": "0:00:00.039114", "end": "2018-05-30 17:37:48.694588", "failed": true, "msg": "non-zero return code", "rc": 1, "start": "2018-05-30 17:37:48.655474", "stderr": "\"docker rm\" requires at least 1 argument(s).\nSee 'docker rm --help'.\n\nUsage:  docker rm [OPTIONS] CONTAINER [CONTAINER...]\n\nRemove one or more containers", "stderr_lines": ["\"docker rm\" requires at least 1 argument(s).", "See 'docker rm --help'.", "", "Usage:  docker rm [OPTIONS] CONTAINER [CONTAINER...]", "", "Remove one or more containers"], "stdout": "", "stdout_lines": []}
2018-05-30 23:37:48,737 p=10 u=root |  ...ignoring
2018-05-30 23:37:48,745 p=10 u=root |  TASK [baseliner : store facts] *************************************************
2018-05-30 23:37:48,745 p=10 u=root |  Wednesday 30 May 2018  23:37:48 +0000 (0:00:00.849)       0:00:05.451 ********* 
2018-05-30 23:37:48,797 p=10 u=root |  included: /etc/ansible/roles/baseliner/tasks/store-facts.yml for node1, node2, node3
2018-05-30 23:37:48,812 p=10 u=root |  TASK [baseliner : install facter] **********************************************
2018-05-30 23:37:48,812 p=10 u=root |  Wednesday 30 May 2018  23:37:48 +0000 (0:00:00.067)       0:00:05.519 ********* 
2018-05-30 23:37:50,227 p=10 u=root |  ok: [node2]
2018-05-30 23:37:50,231 p=10 u=root |  ok: [node1]
2018-05-30 23:37:50,245 p=10 u=root |  ok: [node3]
2018-05-30 23:37:50,258 p=10 u=root |  TASK [baseliner : create facts folder if it doesn't exist] *********************
2018-05-30 23:37:50,259 p=10 u=root |  Wednesday 30 May 2018  23:37:50 +0000 (0:00:01.446)       0:00:06.965 ********* 
2018-05-30 23:37:50,365 p=10 u=root |  changed: [node1 -> localhost]
2018-05-30 23:37:50,376 p=10 u=root |  TASK [baseliner : store facts about remotes] ***********************************
2018-05-30 23:37:50,376 p=10 u=root |  Wednesday 30 May 2018  23:37:50 +0000 (0:00:00.117)       0:00:07.083 ********* 
2018-05-30 23:37:50,751 p=10 u=root |  changed: [node1 -> localhost] => (item=node1)
2018-05-30 23:37:50,940 p=10 u=root |  changed: [node1 -> localhost] => (item=node2)
2018-05-30 23:37:51,131 p=10 u=root |  changed: [node1 -> localhost] => (item=node3)
2018-05-30 23:37:51,139 p=10 u=root |  TASK [baseliner : start monitoring] ********************************************
2018-05-30 23:37:51,140 p=10 u=root |  Wednesday 30 May 2018  23:37:51 +0000 (0:00:00.763)       0:00:07.846 ********* 
2018-05-30 23:37:51,177 p=10 u=root |  TASK [baseliner : include_tasks] ***********************************************
2018-05-30 23:37:51,177 p=10 u=root |  Wednesday 30 May 2018  23:37:51 +0000 (0:00:00.037)       0:00:07.884 ********* 
2018-05-30 23:37:51,211 p=10 u=root |  TASK [baseliner : get number of repetitions] ***********************************
2018-05-30 23:37:51,211 p=10 u=root |  Wednesday 30 May 2018  23:37:51 +0000 (0:00:00.034)       0:00:07.918 ********* 
2018-05-30 23:37:51,265 p=10 u=root |  ok: [node1] => (item=1)
2018-05-30 23:37:51,275 p=10 u=root |  ok: [node2] => (item=1)
2018-05-30 23:37:51,284 p=10 u=root |  ok: [node3] => (item=1)
2018-05-30 23:37:51,290 p=10 u=root |  TASK [baseliner : execute each benchmark] **************************************
2018-05-30 23:37:51,290 p=10 u=root |  Wednesday 30 May 2018  23:37:51 +0000 (0:00:00.078)       0:00:07.997 ********* 
2018-05-30 23:37:51,345 p=10 u=root |  included: /etc/ansible/roles/baseliner/tasks/run-bench.yml for node1, node2, node3
2018-05-30 23:37:51,367 p=10 u=root |  TASK [baseliner : upload files] ************************************************
2018-05-30 23:37:51,368 p=10 u=root |  Wednesday 30 May 2018  23:37:51 +0000 (0:00:00.077)       0:00:08.074 ********* 
2018-05-30 23:37:52,794 p=10 u=root |  changed: [node3] => (item={u'dest': u'/tmp/etc/ceph.conf', u'src': u'/experiment/scripts/docker-bench/ceph/etcceph/ceph.conf'})
2018-05-30 23:37:52,795 p=10 u=root |  changed: [node2] => (item={u'dest': u'/tmp/etc/ceph.conf', u'src': u'/experiment/scripts/docker-bench/ceph/etcceph/ceph.conf'})
2018-05-30 23:37:52,798 p=10 u=root |  changed: [node1] => (item={u'dest': u'/tmp/etc/ceph.conf', u'src': u'/experiment/scripts/docker-bench/ceph/etcceph/ceph.conf'})
2018-05-30 23:37:52,805 p=10 u=root |  TASK [baseliner : initialize parameters and add number of repetitions] *********
2018-05-30 23:37:52,805 p=10 u=root |  Wednesday 30 May 2018  23:37:52 +0000 (0:00:01.437)       0:00:09.512 ********* 
2018-05-30 23:37:52,858 p=10 u=root |  ok: [node1]
2018-05-30 23:37:52,865 p=10 u=root |  ok: [node2]
2018-05-30 23:37:52,875 p=10 u=root |  ok: [node3]
2018-05-30 23:37:52,881 p=10 u=root |  TASK [baseliner : unnest parameters when parameters for benchmark were passed] ***
2018-05-30 23:37:52,882 p=10 u=root |  Wednesday 30 May 2018  23:37:52 +0000 (0:00:00.076)       0:00:09.588 ********* 
2018-05-30 23:37:52,916 p=10 u=root |  TASK [baseliner : parametrized execution] **************************************
2018-05-30 23:37:52,916 p=10 u=root |  Wednesday 30 May 2018  23:37:52 +0000 (0:00:00.034)       0:00:09.623 ********* 
2018-05-30 23:37:52,991 p=10 u=root |  included: /etc/ansible/roles/baseliner/tasks/run-bench-parametrized.yml for node1, node2, node3
2018-05-30 23:37:53,002 p=10 u=root |  TASK [baseliner : remove remote results folder] ********************************
2018-05-30 23:37:53,002 p=10 u=root |  Wednesday 30 May 2018  23:37:53 +0000 (0:00:00.085)       0:00:09.708 ********* 
2018-05-30 23:37:53,831 p=10 u=root |  ok: [node1]
2018-05-30 23:37:53,844 p=10 u=root |  ok: [node2]
2018-05-30 23:37:53,856 p=10 u=root |  ok: [node3]
2018-05-30 23:37:53,867 p=10 u=root |  TASK [baseliner : create remote results folder] ********************************
2018-05-30 23:37:53,868 p=10 u=root |  Wednesday 30 May 2018  23:37:53 +0000 (0:00:00.865)       0:00:10.574 ********* 
2018-05-30 23:37:54,663 p=10 u=root |  changed: [node1]
2018-05-30 23:37:54,664 p=10 u=root |  changed: [node2]
2018-05-30 23:37:54,671 p=10 u=root |  changed: [node3]
2018-05-30 23:37:54,678 p=10 u=root |  TASK [baseliner : initialize parameter dictionary] *****************************
2018-05-30 23:37:54,679 p=10 u=root |  Wednesday 30 May 2018  23:37:54 +0000 (0:00:00.810)       0:00:11.385 ********* 
2018-05-30 23:37:54,734 p=10 u=root |  ok: [node1]
2018-05-30 23:37:54,740 p=10 u=root |  ok: [node2]
2018-05-30 23:37:54,748 p=10 u=root |  ok: [node3]
2018-05-30 23:37:54,754 p=10 u=root |  TASK [baseliner : populate parameter dictionary] *******************************
2018-05-30 23:37:54,755 p=10 u=root |  Wednesday 30 May 2018  23:37:54 +0000 (0:00:00.075)       0:00:11.461 ********* 
2018-05-30 23:37:54,810 p=10 u=root |  ok: [node1] => (item=[u'repetition', u'1'])
2018-05-30 23:37:54,820 p=10 u=root |  ok: [node2] => (item=[u'repetition', u'1'])
2018-05-30 23:37:54,828 p=10 u=root |  ok: [node3] => (item=[u'repetition', u'1'])
2018-05-30 23:37:54,835 p=10 u=root |  TASK [baseliner : merge default variables to the benchmark-specific options] ***
2018-05-30 23:37:54,835 p=10 u=root |  Wednesday 30 May 2018  23:37:54 +0000 (0:00:00.080)       0:00:11.541 ********* 
2018-05-30 23:37:54,888 p=10 u=root |  ok: [node1]
2018-05-30 23:37:54,896 p=10 u=root |  ok: [node2]
2018-05-30 23:37:54,906 p=10 u=root |  ok: [node3]
2018-05-30 23:37:54,912 p=10 u=root |  TASK [baseliner : initialize string for path] **********************************
2018-05-30 23:37:54,912 p=10 u=root |  Wednesday 30 May 2018  23:37:54 +0000 (0:00:00.077)       0:00:11.619 ********* 
2018-05-30 23:37:54,964 p=10 u=root |  ok: [node1]
2018-05-30 23:37:54,972 p=10 u=root |  ok: [node2]
2018-05-30 23:37:54,984 p=10 u=root |  ok: [node3]
2018-05-30 23:37:54,990 p=10 u=root |  TASK [baseliner : add key/value pairs for each parameter (if any)] *************
2018-05-30 23:37:54,991 p=10 u=root |  Wednesday 30 May 2018  23:37:54 +0000 (0:00:00.078)       0:00:11.697 ********* 
2018-05-30 23:37:55,045 p=10 u=root |  ok: [node1] => (item={'key': u'repetition', 'value': u'1'})
2018-05-30 23:37:55,060 p=10 u=root |  ok: [node2] => (item={'key': u'repetition', 'value': u'1'})
2018-05-30 23:37:55,070 p=10 u=root |  ok: [node3] => (item={'key': u'repetition', 'value': u'1'})
2018-05-30 23:37:55,078 p=10 u=root |  TASK [baseliner : ensure output folder exists] *********************************
2018-05-30 23:37:55,079 p=10 u=root |  Wednesday 30 May 2018  23:37:55 +0000 (0:00:00.087)       0:00:11.785 ********* 
2018-05-30 23:37:55,201 p=10 u=root |  changed: [node1 -> localhost]
2018-05-30 23:37:55,218 p=10 u=root |  changed: [node2 -> localhost]
2018-05-30 23:37:55,220 p=10 u=root |  changed: [node3 -> localhost]
2018-05-30 23:37:55,227 p=10 u=root |  TASK [baseliner : run containerized benchmark] *********************************
2018-05-30 23:37:55,227 p=10 u=root |  Wednesday 30 May 2018  23:37:55 +0000 (0:00:00.148)       0:00:11.934 ********* 
2018-05-30 23:37:55,351 p=10 u=root |  included: /etc/ansible/roles/baseliner/tasks/run-container.yml for node1, node2, node3
2018-05-30 23:37:55,372 p=10 u=root |  TASK [baseliner : check if image already exists] *******************************
2018-05-30 23:37:55,372 p=10 u=root |  Wednesday 30 May 2018  23:37:55 +0000 (0:00:00.144)       0:00:12.079 ********* 
2018-05-30 23:37:56,243 p=10 u=root |  changed: [node1]
2018-05-30 23:37:56,254 p=10 u=root |  changed: [node2]
2018-05-30 23:37:56,259 p=10 u=root |  changed: [node3]
2018-05-30 23:37:56,268 p=10 u=root |  TASK [baseliner : pull image] **************************************************
2018-05-30 23:37:56,268 p=10 u=root |  Wednesday 30 May 2018  23:37:56 +0000 (0:00:00.895)       0:00:12.974 ********* 
2018-05-30 23:37:57,918 p=10 u=root |  changed: [node2]
2018-05-30 23:37:57,942 p=10 u=root |  changed: [node1]
2018-05-30 23:37:57,942 p=10 u=root |  changed: [node3]
2018-05-30 23:37:57,950 p=10 u=root |  TASK [baseliner : define name of container] ************************************
2018-05-30 23:37:57,950 p=10 u=root |  Wednesday 30 May 2018  23:37:57 +0000 (0:00:01.681)       0:00:14.656 ********* 
2018-05-30 23:37:58,004 p=10 u=root |  ok: [node1]
2018-05-30 23:37:58,013 p=10 u=root |  ok: [node2]
2018-05-30 23:37:58,022 p=10 u=root |  ok: [node3]
2018-05-30 23:37:58,030 p=10 u=root |  TASK [baseliner : initialize docker_flags variable] ****************************
2018-05-30 23:37:58,030 p=10 u=root |  Wednesday 30 May 2018  23:37:58 +0000 (0:00:00.079)       0:00:14.736 ********* 
2018-05-30 23:37:58,079 p=10 u=root |  ok: [node1]
2018-05-30 23:37:58,087 p=10 u=root |  ok: [node2]
2018-05-30 23:37:58,099 p=10 u=root |  ok: [node3]
2018-05-30 23:37:58,108 p=10 u=root |  TASK [baseliner : add environment to docker_flags] *****************************
2018-05-30 23:37:58,109 p=10 u=root |  Wednesday 30 May 2018  23:37:58 +0000 (0:00:00.078)       0:00:14.815 ********* 
2018-05-30 23:37:58,145 p=10 u=root |  TASK [baseliner : add implicit environment from bench_params variable] *********
2018-05-30 23:37:58,145 p=10 u=root |  Wednesday 30 May 2018  23:37:58 +0000 (0:00:00.036)       0:00:14.851 ********* 
2018-05-30 23:37:58,202 p=10 u=root |  ok: [node1] => (item={'key': u'repetition', 'value': u'1'})
2018-05-30 23:37:58,208 p=10 u=root |  ok: [node2] => (item={'key': u'repetition', 'value': u'1'})
2018-05-30 23:37:58,219 p=10 u=root |  ok: [node3] => (item={'key': u'repetition', 'value': u'1'})
2018-05-30 23:37:58,226 p=10 u=root |  TASK [baseliner : check if we have host-specific ips] **************************
2018-05-30 23:37:58,226 p=10 u=root |  Wednesday 30 May 2018  23:37:58 +0000 (0:00:00.080)       0:00:14.932 ********* 
2018-05-30 23:37:58,277 p=10 u=root |  ok: [node1]
2018-05-30 23:37:58,285 p=10 u=root |  ok: [node2]
2018-05-30 23:37:58,295 p=10 u=root |  ok: [node3]
2018-05-30 23:37:58,302 p=10 u=root |  TASK [baseliner : add host-specific ips to docker_flags] ***********************
2018-05-30 23:37:58,302 p=10 u=root |  Wednesday 30 May 2018  23:37:58 +0000 (0:00:00.075)       0:00:15.008 ********* 
2018-05-30 23:37:58,334 p=10 u=root |  TASK [baseliner : check if we have host-specific environment] ******************
2018-05-30 23:37:58,335 p=10 u=root |  Wednesday 30 May 2018  23:37:58 +0000 (0:00:00.032)       0:00:15.041 ********* 
2018-05-30 23:37:58,387 p=10 u=root |  ok: [node1]
2018-05-30 23:37:58,396 p=10 u=root |  ok: [node2]
2018-05-30 23:37:58,406 p=10 u=root |  ok: [node3]
2018-05-30 23:37:58,413 p=10 u=root |  TASK [baseliner : add host-specific environment to docker_flags] ***************
2018-05-30 23:37:58,413 p=10 u=root |  Wednesday 30 May 2018  23:37:58 +0000 (0:00:00.078)       0:00:15.119 ********* 
2018-05-30 23:37:58,468 p=10 u=root |  ok: [node1] => (item={'key': u'CEPH_PUBLIC_NETWORK', 'value': u'128.110.152.0/22'})
2018-05-30 23:37:58,479 p=10 u=root |  ok: [node2] => (item={'key': u'CEPH_PUBLIC_NETWORK', 'value': u'128.110.152.0/22'})
2018-05-30 23:37:58,486 p=10 u=root |  ok: [node3] => (item={'key': u'CEPH_PUBLIC_NETWORK', 'value': u'128.110.152.0/22'})
2018-05-30 23:37:58,506 p=10 u=root |  ok: [node1] => (item={'key': u'MON_IP', 'value': u'128.110.153.197'})
2018-05-30 23:37:58,520 p=10 u=root |  ok: [node2] => (item={'key': u'MON_IP', 'value': u'128.110.153.114'})
2018-05-30 23:37:58,526 p=10 u=root |  ok: [node3] => (item={'key': u'CLIENT', 'value': True})
2018-05-30 23:37:58,545 p=10 u=root |  ok: [node1] => (item={'key': u'MONITOR', 'value': True})
2018-05-30 23:37:58,562 p=10 u=root |  ok: [node2] => (item={'key': u'OSD', 'value': True})
2018-05-30 23:37:58,562 p=10 u=root |  ok: [node3] => (item={'key': u'MON_IP', 'value': u'128.110.153.173'})
2018-05-30 23:37:58,596 p=10 u=root |  ok: [node2] => (item={'key': u'OSD_DEVICES', 'value': u'/dev/sdb'})
2018-05-30 23:37:58,603 p=10 u=root |  TASK [baseliner : add devices to docker_flags] *********************************
2018-05-30 23:37:58,603 p=10 u=root |  Wednesday 30 May 2018  23:37:58 +0000 (0:00:00.189)       0:00:15.309 ********* 
2018-05-30 23:37:58,638 p=10 u=root |  TASK [baseliner : add network mode to docker_flags] ****************************
2018-05-30 23:37:58,638 p=10 u=root |  Wednesday 30 May 2018  23:37:58 +0000 (0:00:00.035)       0:00:15.345 ********* 
2018-05-30 23:37:58,691 p=10 u=root |  ok: [node1]
2018-05-30 23:37:58,699 p=10 u=root |  ok: [node2]
2018-05-30 23:37:58,710 p=10 u=root |  ok: [node3]
2018-05-30 23:37:58,717 p=10 u=root |  TASK [baseliner : add ipc mode to docker_flags] ********************************
2018-05-30 23:37:58,717 p=10 u=root |  Wednesday 30 May 2018  23:37:58 +0000 (0:00:00.078)       0:00:15.424 ********* 
2018-05-30 23:37:58,752 p=10 u=root |  TASK [baseliner : add ports to docker_flags] ***********************************
2018-05-30 23:37:58,753 p=10 u=root |  Wednesday 30 May 2018  23:37:58 +0000 (0:00:00.035)       0:00:15.459 ********* 
2018-05-30 23:37:58,788 p=10 u=root |  TASK [baseliner : add volumes to docker_flags] *********************************
2018-05-30 23:37:58,789 p=10 u=root |  Wednesday 30 May 2018  23:37:58 +0000 (0:00:00.035)       0:00:15.495 ********* 
2018-05-30 23:37:58,844 p=10 u=root |  ok: [node1] => (item=/tmp/etc:/etc/ceph)
2018-05-30 23:37:58,852 p=10 u=root |  ok: [node2] => (item=/tmp/etc:/etc/ceph)
2018-05-30 23:37:58,863 p=10 u=root |  ok: [node3] => (item=/tmp/etc:/etc/ceph)
2018-05-30 23:37:58,870 p=10 u=root |  TASK [baseliner : add default volumes to docker_flags] *************************
2018-05-30 23:37:58,870 p=10 u=root |  Wednesday 30 May 2018  23:37:58 +0000 (0:00:00.081)       0:00:15.577 ********* 
2018-05-30 23:37:58,923 p=10 u=root |  ok: [node1]
2018-05-30 23:37:58,930 p=10 u=root |  ok: [node2]
2018-05-30 23:37:58,941 p=10 u=root |  ok: [node3]
2018-05-30 23:37:58,948 p=10 u=root |  TASK [baseliner : set entrypoint] **********************************************
2018-05-30 23:37:58,948 p=10 u=root |  Wednesday 30 May 2018  23:37:58 +0000 (0:00:00.077)       0:00:15.654 ********* 
2018-05-30 23:37:58,984 p=10 u=root |  TASK [baseliner : set limits] **************************************************
2018-05-30 23:37:58,985 p=10 u=root |  Wednesday 30 May 2018  23:37:58 +0000 (0:00:00.036)       0:00:15.691 ********* 
2018-05-30 23:37:59,019 p=10 u=root |  TASK [baseliner : add capabilities] ********************************************
2018-05-30 23:37:59,020 p=10 u=root |  Wednesday 30 May 2018  23:37:59 +0000 (0:00:00.035)       0:00:15.726 ********* 
2018-05-30 23:37:59,054 p=10 u=root |  TASK [baseliner : remove capabilities] *****************************************
2018-05-30 23:37:59,054 p=10 u=root |  Wednesday 30 May 2018  23:37:59 +0000 (0:00:00.034)       0:00:15.761 ********* 
2018-05-30 23:37:59,089 p=10 u=root |  TASK [baseliner : set privileged mode] *****************************************
2018-05-30 23:37:59,090 p=10 u=root |  Wednesday 30 May 2018  23:37:59 +0000 (0:00:00.035)       0:00:15.796 ********* 
2018-05-30 23:37:59,122 p=10 u=root |  TASK [baseliner : set memory constraint] ***************************************
2018-05-30 23:37:59,123 p=10 u=root |  Wednesday 30 May 2018  23:37:59 +0000 (0:00:00.033)       0:00:15.829 ********* 
2018-05-30 23:37:59,157 p=10 u=root |  TASK [baseliner : set memory-swap constraint] **********************************
2018-05-30 23:37:59,157 p=10 u=root |  Wednesday 30 May 2018  23:37:59 +0000 (0:00:00.034)       0:00:15.863 ********* 
2018-05-30 23:37:59,189 p=10 u=root |  TASK [baseliner : set parent cgroup] *******************************************
2018-05-30 23:37:59,189 p=10 u=root |  Wednesday 30 May 2018  23:37:59 +0000 (0:00:00.032)       0:00:15.896 ********* 
2018-05-30 23:37:59,223 p=10 u=root |  TASK [baseliner : debug] *******************************************************
2018-05-30 23:37:59,223 p=10 u=root |  Wednesday 30 May 2018  23:37:59 +0000 (0:00:00.034)       0:00:15.930 ********* 
2018-05-30 23:37:59,275 p=10 u=root |  ok: [node1] => {
    "msg": "docker run --rm  -e repetition=\"1\" -e CEPH_PUBLIC_NETWORK=\"128.110.152.0/22\" -e MON_IP=\"128.110.153.197\" -e MONITOR=\"True\" --net=host -v /tmp/etc:/etc/ceph -v /tmp/baseliner_output_node1:/results -v /tmp/baseliner_output_node1:/output mariettesouppe/rados_bench:master-ec8d33f-luminous-ubuntu-16.04-x86_64 "
}
2018-05-30 23:37:59,284 p=10 u=root |  ok: [node2] => {
    "msg": "docker run --rm  -e repetition=\"1\" -e CEPH_PUBLIC_NETWORK=\"128.110.152.0/22\" -e MON_IP=\"128.110.153.114\" -e OSD=\"True\" -e OSD_DEVICES=\"/dev/sdb\" --net=host -v /tmp/etc:/etc/ceph -v /tmp/baseliner_output_node2:/results -v /tmp/baseliner_output_node2:/output mariettesouppe/rados_bench:master-ec8d33f-luminous-ubuntu-16.04-x86_64 "
}
2018-05-30 23:37:59,294 p=10 u=root |  ok: [node3] => {
    "msg": "docker run --rm  -e repetition=\"1\" -e CEPH_PUBLIC_NETWORK=\"128.110.152.0/22\" -e CLIENT=\"True\" -e MON_IP=\"128.110.153.173\" --net=host -v /tmp/etc:/etc/ceph -v /tmp/baseliner_output_node3:/results -v /tmp/baseliner_output_node3:/output mariettesouppe/rados_bench:master-ec8d33f-luminous-ubuntu-16.04-x86_64 "
}
2018-05-30 23:37:59,301 p=10 u=root |  TASK [baseliner : run container] ***********************************************
2018-05-30 23:37:59,301 p=10 u=root |  Wednesday 30 May 2018  23:37:59 +0000 (0:00:00.077)       0:00:16.008 ********* 
2018-05-30 23:38:01,315 p=10 u=root |  changed: [node1]
2018-05-30 23:38:01,319 p=10 u=root |  changed: [node2]
2018-05-30 23:38:01,321 p=10 u=root |  changed: [node3]
2018-05-30 23:38:01,332 p=10 u=root |  TASK [baseliner : wait for containers in parallel mode and stop/kill them if they timeout] ***
2018-05-30 23:38:01,332 p=10 u=root |  Wednesday 30 May 2018  23:38:01 +0000 (0:00:02.031)       0:00:18.039 ********* 
2018-05-30 23:38:01,352 p=10 u=root |  TASK [baseliner : wait for containers in single-node mode and stop/kill them if they timeout] ***
2018-05-30 23:38:01,352 p=10 u=root |  Wednesday 30 May 2018  23:38:01 +0000 (0:00:00.019)       0:00:18.058 ********* 
2018-05-31 01:19:10,008 p=10 u=root |  changed: [node1 -> localhost]
2018-05-31 01:19:10,020 p=10 u=root |  TASK [baseliner : get wait result] *********************************************
2018-05-31 01:19:10,021 p=10 u=root |  Thursday 31 May 2018  01:19:10 +0000 (1:41:08.668)       1:41:26.727 ********** 
2018-05-31 01:19:10,044 p=10 u=root |  ok: [node1 -> localhost]
2018-05-31 01:19:10,055 p=10 u=root |  TASK [baseliner : debug] *******************************************************
2018-05-31 01:19:10,055 p=10 u=root |  Thursday 31 May 2018  01:19:10 +0000 (0:00:00.034)       1:41:26.762 ********** 
2018-05-31 01:19:10,073 p=10 u=root |  ok: [node1 -> localhost] => {
    "msg": "WAIT_FOR_RESULT:  WARNING: timeout waiting for 3 hosts"
}
2018-05-31 01:19:10,081 p=10 u=root |  TASK [baseliner : get result of container execution] ***************************
2018-05-31 01:19:10,081 p=10 u=root |  Thursday 31 May 2018  01:19:10 +0000 (0:00:00.025)       1:41:26.787 ********** 
2018-05-31 01:19:11,033 p=10 u=root |  fatal: [node3]: FAILED! => {"ansible_job_id": "573922188825.10250", "changed": true, "cmd": "docker run --rm --name baseliner_node3  -e repetition=\"1\" -e CEPH_PUBLIC_NETWORK=\"128.110.152.0/22\" -e CLIENT=\"True\" -e MON_IP=\"128.110.153.173\" --net=host -v /tmp/etc:/etc/ceph -v /tmp/baseliner_output_node3:/results -v /tmp/baseliner_output_node3:/output mariettesouppe/rados_bench:master-ec8d33f-luminous-ubuntu-16.04-x86_64 ", "delta": "1:41:07.813099", "end": "2018-05-30 19:19:08.212778", "failed": true, "finished": 1, "msg": "non-zero return code", "rc": 137, "start": "2018-05-30 17:38:00.399679", "stderr": "+ '[' -z '' ']'\n+ '[' -z '' ']'\n+ '[' -z True ']'\n+ '[' -z 128.110.152.0/22 ']'\n+ '[' -z 128.110.153.173 ']'\n+ sed -i -e s@monip@128.110.153.173@ /etc/ceph/ceph.conf\n+ sed -i -e s@cephnet@128.110.152.0/22@ /etc/ceph/ceph.conf\n+ '[' -f /etc/ceph/extra.conf ']'\n+ '[' -n '' ']'\n+ '[' -n '' ']'\n+ '[' -n True ']'\n+ '[' -z '' ']'\n+ echo 'No CLIENT_SCRIPT defined, running built-in radosbench script.'\n+ CLIENT_SCRIPT=run_radosbench\n+ ceph_health_ok\n2018-05-30 23:38:00.833814 7f74e94bd700 -1 auth: unable to find a keyring on /etc/ceph/ceph.client.admin.keyring,/etc/ceph/ceph.keyring,/etc/ceph/keyring,/etc/ceph/keyring.bin,: (2) No such file or directory\n2018-05-30 23:43:00.834814 7f74e94bd700  0 monclient(hunting): authenticate timed out after 300\n2018-05-30 23:43:00.834848 7f74e94bd700 -1 monclient(hunting): authenticate NOTE: no keyring found; disabled cephx authentication\n2018-05-30 23:43:00.834857 7f74e94bd700  0 librados: client.admin authentication error (110) Connection timed out\n[errno 110] error connecting to the cluster\n2018-05-30 23:43:10.987021 7f4f38049700 -1 auth: unable to find a keyring on /etc/ceph/ceph.client.admin.keyring,/etc/ceph/ceph.keyring,/etc/ceph/keyring,/etc/ceph/keyring.bin,: (2) No such file or directory\n2018-05-30 23:48:10.987273 7f4f38049700  0 monclient(hunting): authenticate timed out after 300\n2018-05-30 23:48:10.987306 7f4f38049700 -1 monclient(hunting): authenticate NOTE: no keyring found; disabled cephx authentication\n2018-05-30 23:48:10.987315 7f4f38049700  0 librados: client.admin authentication error (110) Connection timed out\n[errno 110] error connecting to the cluster\n2018-05-30 23:48:21.166062 7fc036670700 -1 auth: unable to find a keyring on /etc/ceph/ceph.client.admin.keyring,/etc/ceph/ceph.keyring,/etc/ceph/keyring,/etc/ceph/keyring.bin,: (2) No such file or directory\n2018-05-30 23:53:21.166458 7fc036670700  0 monclient(hunting): authenticate timed out after 300\n2018-05-30 23:53:21.166491 7fc036670700 -1 monclient(hunting): authenticate NOTE: no keyring found; disabled cephx authentication\n2018-05-30 23:53:21.166500 7fc036670700  0 librados: client.admin authentication error (110) Connection timed out\n[errno 110] error connecting to the cluster\n2018-05-30 23:53:31.348049 7f8105483700 -1 auth: unable to find a keyring on /etc/ceph/ceph.client.admin.keyring,/etc/ceph/ceph.keyring,/etc/ceph/keyring,/etc/ceph/keyring.bin,: (2) No such file or directory\n2018-05-30 23:58:31.349582 7f8105483700  0 monclient(hunting): authenticate timed out after 300\n2018-05-30 23:58:31.349616 7f8105483700 -1 monclient(hunting): authenticate NOTE: no keyring found; disabled cephx authentication\n2018-05-30 23:58:31.349625 7f8105483700  0 librados: client.admin authentication error (110) Connection timed out\n[errno 110] error connecting to the cluster\n2018-05-30 23:58:41.529637 7f9d7c199700 -1 auth: unable to find a keyring on /etc/ceph/ceph.client.admin.keyring,/etc/ceph/ceph.keyring,/etc/ceph/keyring,/etc/ceph/keyring.bin,: (2) No such file or directory\n2018-05-31 00:03:41.530046 7f9d7c199700  0 monclient(hunting): authenticate timed out after 300\n2018-05-31 00:03:41.530079 7f9d7c199700 -1 monclient(hunting): authenticate NOTE: no keyring found; disabled cephx authentication\n2018-05-31 00:03:41.530088 7f9d7c199700  0 librados: client.admin authentication error (110) Connection timed out\n[errno 110] error connecting to the cluster\n2018-05-31 00:03:51.708657 7fc2948ee700 -1 auth: unable to find a keyring on /etc/ceph/ceph.client.admin.keyring,/etc/ceph/ceph.keyring,/etc/ceph/keyring,/etc/ceph/keyring.bin,: (2) No such file or directory\n2018-05-31 00:08:51.709325 7fc2948ee700  0 monclient(hunting): authenticate timed out after 300\n2018-05-31 00:08:51.709361 7fc2948ee700 -1 monclient(hunting): authenticate NOTE: no keyring found; disabled cephx authentication\n2018-05-31 00:08:51.709371 7fc2948ee700  0 librados: client.admin authentication error (110) Connection timed out\n[errno 110] error connecting to the cluster\n2018-05-31 00:09:01.890260 7f96bd07e700 -1 auth: unable to find a keyring on /etc/ceph/ceph.client.admin.keyring,/etc/ceph/ceph.keyring,/etc/ceph/keyring,/etc/ceph/keyring.bin,: (2) No such file or directory\n2018-05-31 00:14:01.891053 7f96bd07e700  0 monclient(hunting): authenticate timed out after 300\n2018-05-31 00:14:01.891087 7f96bd07e700 -1 monclient(hunting): authenticate NOTE: no keyring found; disabled cephx authentication\n2018-05-31 00:14:01.891095 7f96bd07e700  0 librados: client.admin authentication error (110) Connection timed out\n[errno 110] error connecting to the cluster\n2018-05-31 00:14:12.075213 7f06cda13700 -1 auth: unable to find a keyring on /etc/ceph/ceph.client.admin.keyring,/etc/ceph/ceph.keyring,/etc/ceph/keyring,/etc/ceph/keyring.bin,: (2) No such file or directory\n2018-05-31 00:19:12.075929 7f06cda13700  0 monclient(hunting): authenticate timed out after 300\n2018-05-31 00:19:12.075964 7f06cda13700 -1 monclient(hunting): authenticate NOTE: no keyring found; disabled cephx authentication\n2018-05-31 00:19:12.075974 7f06cda13700  0 librados: client.admin authentication error (110) Connection timed out\n[errno 110] error connecting to the cluster\n2018-05-31 00:19:22.259635 7fc6e8ba8700 -1 auth: unable to find a keyring on /etc/ceph/ceph.client.admin.keyring,/etc/ceph/ceph.keyring,/etc/ceph/keyring,/etc/ceph/keyring.bin,: (2) No such file or directory\n2018-05-31 00:24:22.260004 7fc6e8ba8700  0 monclient(hunting): authenticate timed out after 300\n2018-05-31 00:24:22.260038 7fc6e8ba8700 -1 monclient(hunting): authenticate NOTE: no keyring found; disabled cephx authentication\n2018-05-31 00:24:22.260047 7fc6e8ba8700  0 librados: client.admin authentication error (110) Connection timed out\n[errno 110] error connecting to the cluster\n2018-05-31 00:24:32.437506 7f9a1a1bc700 -1 auth: unable to find a keyring on /etc/ceph/ceph.client.admin.keyring,/etc/ceph/ceph.keyring,/etc/ceph/keyring,/etc/ceph/keyring.bin,: (2) No such file or directory\n2018-05-31 00:29:32.438270 7f9a1a1bc700  0 monclient(hunting): authenticate timed out after 300\n2018-05-31 00:29:32.438304 7f9a1a1bc700 -1 monclient(hunting): authenticate NOTE: no keyring found; disabled cephx authentication\n2018-05-31 00:29:32.438312 7f9a1a1bc700  0 librados: client.admin authentication error (110) Connection timed out\n[errno 110] error connecting to the cluster\n2018-05-31 00:29:42.619758 7f44e1216700 -1 auth: unable to find a keyring on /etc/ceph/ceph.client.admin.keyring,/etc/ceph/ceph.keyring,/etc/ceph/keyring,/etc/ceph/keyring.bin,: (2) No such file or directory\n2018-05-31 00:34:42.620967 7f44e1216700  0 monclient(hunting): authenticate timed out after 300\n2018-05-31 00:34:42.621000 7f44e1216700 -1 monclient(hunting): authenticate NOTE: no keyring found; disabled cephx authentication\n2018-05-31 00:34:42.621008 7f44e1216700  0 librados: client.admin authentication error (110) Connection timed out\n[errno 110] error connecting to the cluster\n2018-05-31 00:34:52.798647 7fde3d583700 -1 auth: unable to find a keyring on /etc/ceph/ceph.client.admin.keyring,/etc/ceph/ceph.keyring,/etc/ceph/keyring,/etc/ceph/keyring.bin,: (2) No such file or directory\n2018-05-31 00:39:52.799392 7fde3d583700  0 monclient(hunting): authenticate timed out after 300\n2018-05-31 00:39:52.799425 7fde3d583700 -1 monclient(hunting): authenticate NOTE: no keyring found; disabled cephx authentication\n2018-05-31 00:39:52.799434 7fde3d583700  0 librados: client.admin authentication error (110) Connection timed out\n[errno 110] error connecting to the cluster\n2018-05-31 00:40:02.973675 7ff45207b700 -1 auth: unable to find a keyring on /etc/ceph/ceph.client.admin.keyring,/etc/ceph/ceph.keyring,/etc/ceph/keyring,/etc/ceph/keyring.bin,: (2) No such file or directory\n2018-05-31 00:45:02.974424 7ff45207b700  0 monclient(hunting): authenticate timed out after 300\n2018-05-31 00:45:02.974458 7ff45207b700 -1 monclient(hunting): authenticate NOTE: no keyring found; disabled cephx authentication\n2018-05-31 00:45:02.974466 7ff45207b700  0 librados: client.admin authentication error (110) Connection timed out\n[errno 110] error connecting to the cluster\n2018-05-31 00:45:13.156724 7f1b09e84700 -1 auth: unable to find a keyring on /etc/ceph/ceph.client.admin.keyring,/etc/ceph/ceph.keyring,/etc/ceph/keyring,/etc/ceph/keyring.bin,: (2) No such file or directory\n2018-05-31 00:50:13.157401 7f1b09e84700  0 monclient(hunting): authenticate timed out after 300\n2018-05-31 00:50:13.157435 7f1b09e84700 -1 monclient(hunting): authenticate NOTE: no keyring found; disabled cephx authentication\n2018-05-31 00:50:13.157444 7f1b09e84700  0 librados: client.admin authentication error (110) Connection timed out\n[errno 110] error connecting to the cluster\n2018-05-31 00:50:23.338306 7f29ee0a4700 -1 auth: unable to find a keyring on /etc/ceph/ceph.client.admin.keyring,/etc/ceph/ceph.keyring,/etc/ceph/keyring,/etc/ceph/keyring.bin,: (2) No such file or directory\n2018-05-31 00:55:23.339110 7f29ee0a4700  0 monclient(hunting): authenticate timed out after 300\n2018-05-31 00:55:23.339143 7f29ee0a4700 -1 monclient(hunting): authenticate NOTE: no keyring found; disabled cephx authentication\n2018-05-31 00:55:23.339152 7f29ee0a4700  0 librados: client.admin authentication error (110) Connection timed out\n[errno 110] error connecting to the cluster\n2018-05-31 00:55:33.518850 7f5ed02e5700 -1 auth: unable to find a keyring on /etc/ceph/ceph.client.admin.keyring,/etc/ceph/ceph.keyring,/etc/ceph/keyring,/etc/ceph/keyring.bin,: (2) No such file or directory\n2018-05-31 01:00:33.519561 7f5ed02e5700  0 monclient(hunting): authenticate timed out after 300\n2018-05-31 01:00:33.519595 7f5ed02e5700 -1 monclient(hunting): authenticate NOTE: no keyring found; disabled cephx authentication\n2018-05-31 01:00:33.519603 7f5ed02e5700  0 librados: client.admin authentication error (110) Connection timed out\n[errno 110] error connecting to the cluster\n2018-05-31 01:00:43.691468 7fd9cc32f700 -1 auth: unable to find a keyring on /etc/ceph/ceph.client.admin.keyring,/etc/ceph/ceph.keyring,/etc/ceph/keyring,/etc/ceph/keyring.bin,: (2) No such file or directory\n2018-05-31 01:05:43.691841 7fd9cc32f700  0 monclient(hunting): authenticate timed out after 300\n2018-05-31 01:05:43.691875 7fd9cc32f700 -1 monclient(hunting): authenticate NOTE: no keyring found; disabled cephx authentication\n2018-05-31 01:05:43.691883 7fd9cc32f700  0 librados: client.admin authentication error (110) Connection timed out\n[errno 110] error connecting to the cluster\n2018-05-31 01:05:53.870859 7f4ccccbb700 -1 auth: unable to find a keyring on /etc/ceph/ceph.client.admin.keyring,/etc/ceph/ceph.keyring,/etc/ceph/keyring,/etc/ceph/keyring.bin,: (2) No such file or directory\n2018-05-31 01:10:53.871612 7f4ccccbb700  0 monclient(hunting): authenticate timed out after 300\n2018-05-31 01:10:53.871645 7f4ccccbb700 -1 monclient(hunting): authenticate NOTE: no keyring found; disabled cephx authentication\n2018-05-31 01:10:53.871653 7f4ccccbb700  0 librados: client.admin authentication error (110) Connection timed out\n[errno 110] error connecting to the cluster\n2018-05-31 01:11:04.050610 7fd03188b700 -1 auth: unable to find a keyring on /etc/ceph/ceph.client.admin.keyring,/etc/ceph/ceph.keyring,/etc/ceph/keyring,/etc/ceph/keyring.bin,: (2) No such file or directory\n2018-05-31 01:16:04.051596 7fd03188b700  0 monclient(hunting): authenticate timed out after 300\n2018-05-31 01:16:04.051629 7fd03188b700 -1 monclient(hunting): authenticate NOTE: no keyring found; disabled cephx authentication\n2018-05-31 01:16:04.051638 7fd03188b700  0 librados: client.admin authentication error (110) Connection timed out\n[errno 110] error connecting to the cluster\n2018-05-31 01:16:14.234148 7fdbb90d6700 -1 auth: unable to find a keyring on /etc/ceph/ceph.client.admin.keyring,/etc/ceph/ceph.keyring,/etc/ceph/keyring,/etc/ceph/keyring.bin,: (2) No such file or directory", "stderr_lines": ["+ '[' -z '' ']'", "+ '[' -z '' ']'", "+ '[' -z True ']'", "+ '[' -z 128.110.152.0/22 ']'", "+ '[' -z 128.110.153.173 ']'", "+ sed -i -e s@monip@128.110.153.173@ /etc/ceph/ceph.conf", "+ sed -i -e s@cephnet@128.110.152.0/22@ /etc/ceph/ceph.conf", "+ '[' -f /etc/ceph/extra.conf ']'", "+ '[' -n '' ']'", "+ '[' -n '' ']'", "+ '[' -n True ']'", "+ '[' -z '' ']'", "+ echo 'No CLIENT_SCRIPT defined, running built-in radosbench script.'", "+ CLIENT_SCRIPT=run_radosbench", "+ ceph_health_ok", "2018-05-30 23:38:00.833814 7f74e94bd700 -1 auth: unable to find a keyring on /etc/ceph/ceph.client.admin.keyring,/etc/ceph/ceph.keyring,/etc/ceph/keyring,/etc/ceph/keyring.bin,: (2) No such file or directory", "2018-05-30 23:43:00.834814 7f74e94bd700  0 monclient(hunting): authenticate timed out after 300", "2018-05-30 23:43:00.834848 7f74e94bd700 -1 monclient(hunting): authenticate NOTE: no keyring found; disabled cephx authentication", "2018-05-30 23:43:00.834857 7f74e94bd700  0 librados: client.admin authentication error (110) Connection timed out", "[errno 110] error connecting to the cluster", "2018-05-30 23:43:10.987021 7f4f38049700 -1 auth: unable to find a keyring on /etc/ceph/ceph.client.admin.keyring,/etc/ceph/ceph.keyring,/etc/ceph/keyring,/etc/ceph/keyring.bin,: (2) No such file or directory", "2018-05-30 23:48:10.987273 7f4f38049700  0 monclient(hunting): authenticate timed out after 300", "2018-05-30 23:48:10.987306 7f4f38049700 -1 monclient(hunting): authenticate NOTE: no keyring found; disabled cephx authentication", "2018-05-30 23:48:10.987315 7f4f38049700  0 librados: client.admin authentication error (110) Connection timed out", "[errno 110] error connecting to the cluster", "2018-05-30 23:48:21.166062 7fc036670700 -1 auth: unable to find a keyring on /etc/ceph/ceph.client.admin.keyring,/etc/ceph/ceph.keyring,/etc/ceph/keyring,/etc/ceph/keyring.bin,: (2) No such file or directory", "2018-05-30 23:53:21.166458 7fc036670700  0 monclient(hunting): authenticate timed out after 300", "2018-05-30 23:53:21.166491 7fc036670700 -1 monclient(hunting): authenticate NOTE: no keyring found; disabled cephx authentication", "2018-05-30 23:53:21.166500 7fc036670700  0 librados: client.admin authentication error (110) Connection timed out", "[errno 110] error connecting to the cluster", "2018-05-30 23:53:31.348049 7f8105483700 -1 auth: unable to find a keyring on /etc/ceph/ceph.client.admin.keyring,/etc/ceph/ceph.keyring,/etc/ceph/keyring,/etc/ceph/keyring.bin,: (2) No such file or directory", "2018-05-30 23:58:31.349582 7f8105483700  0 monclient(hunting): authenticate timed out after 300", "2018-05-30 23:58:31.349616 7f8105483700 -1 monclient(hunting): authenticate NOTE: no keyring found; disabled cephx authentication", "2018-05-30 23:58:31.349625 7f8105483700  0 librados: client.admin authentication error (110) Connection timed out", "[errno 110] error connecting to the cluster", "2018-05-30 23:58:41.529637 7f9d7c199700 -1 auth: unable to find a keyring on /etc/ceph/ceph.client.admin.keyring,/etc/ceph/ceph.keyring,/etc/ceph/keyring,/etc/ceph/keyring.bin,: (2) No such file or directory", "2018-05-31 00:03:41.530046 7f9d7c199700  0 monclient(hunting): authenticate timed out after 300", "2018-05-31 00:03:41.530079 7f9d7c199700 -1 monclient(hunting): authenticate NOTE: no keyring found; disabled cephx authentication", "2018-05-31 00:03:41.530088 7f9d7c199700  0 librados: client.admin authentication error (110) Connection timed out", "[errno 110] error connecting to the cluster", "2018-05-31 00:03:51.708657 7fc2948ee700 -1 auth: unable to find a keyring on /etc/ceph/ceph.client.admin.keyring,/etc/ceph/ceph.keyring,/etc/ceph/keyring,/etc/ceph/keyring.bin,: (2) No such file or directory", "2018-05-31 00:08:51.709325 7fc2948ee700  0 monclient(hunting): authenticate timed out after 300", "2018-05-31 00:08:51.709361 7fc2948ee700 -1 monclient(hunting): authenticate NOTE: no keyring found; disabled cephx authentication", "2018-05-31 00:08:51.709371 7fc2948ee700  0 librados: client.admin authentication error (110) Connection timed out", "[errno 110] error connecting to the cluster", "2018-05-31 00:09:01.890260 7f96bd07e700 -1 auth: unable to find a keyring on /etc/ceph/ceph.client.admin.keyring,/etc/ceph/ceph.keyring,/etc/ceph/keyring,/etc/ceph/keyring.bin,: (2) No such file or directory", "2018-05-31 00:14:01.891053 7f96bd07e700  0 monclient(hunting): authenticate timed out after 300", "2018-05-31 00:14:01.891087 7f96bd07e700 -1 monclient(hunting): authenticate NOTE: no keyring found; disabled cephx authentication", "2018-05-31 00:14:01.891095 7f96bd07e700  0 librados: client.admin authentication error (110) Connection timed out", "[errno 110] error connecting to the cluster", "2018-05-31 00:14:12.075213 7f06cda13700 -1 auth: unable to find a keyring on /etc/ceph/ceph.client.admin.keyring,/etc/ceph/ceph.keyring,/etc/ceph/keyring,/etc/ceph/keyring.bin,: (2) No such file or directory", "2018-05-31 00:19:12.075929 7f06cda13700  0 monclient(hunting): authenticate timed out after 300", "2018-05-31 00:19:12.075964 7f06cda13700 -1 monclient(hunting): authenticate NOTE: no keyring found; disabled cephx authentication", "2018-05-31 00:19:12.075974 7f06cda13700  0 librados: client.admin authentication error (110) Connection timed out", "[errno 110] error connecting to the cluster", "2018-05-31 00:19:22.259635 7fc6e8ba8700 -1 auth: unable to find a keyring on /etc/ceph/ceph.client.admin.keyring,/etc/ceph/ceph.keyring,/etc/ceph/keyring,/etc/ceph/keyring.bin,: (2) No such file or directory", "2018-05-31 00:24:22.260004 7fc6e8ba8700  0 monclient(hunting): authenticate timed out after 300", "2018-05-31 00:24:22.260038 7fc6e8ba8700 -1 monclient(hunting): authenticate NOTE: no keyring found; disabled cephx authentication", "2018-05-31 00:24:22.260047 7fc6e8ba8700  0 librados: client.admin authentication error (110) Connection timed out", "[errno 110] error connecting to the cluster", "2018-05-31 00:24:32.437506 7f9a1a1bc700 -1 auth: unable to find a keyring on /etc/ceph/ceph.client.admin.keyring,/etc/ceph/ceph.keyring,/etc/ceph/keyring,/etc/ceph/keyring.bin,: (2) No such file or directory", "2018-05-31 00:29:32.438270 7f9a1a1bc700  0 monclient(hunting): authenticate timed out after 300", "2018-05-31 00:29:32.438304 7f9a1a1bc700 -1 monclient(hunting): authenticate NOTE: no keyring found; disabled cephx authentication", "2018-05-31 00:29:32.438312 7f9a1a1bc700  0 librados: client.admin authentication error (110) Connection timed out", "[errno 110] error connecting to the cluster", "2018-05-31 00:29:42.619758 7f44e1216700 -1 auth: unable to find a keyring on /etc/ceph/ceph.client.admin.keyring,/etc/ceph/ceph.keyring,/etc/ceph/keyring,/etc/ceph/keyring.bin,: (2) No such file or directory", "2018-05-31 00:34:42.620967 7f44e1216700  0 monclient(hunting): authenticate timed out after 300", "2018-05-31 00:34:42.621000 7f44e1216700 -1 monclient(hunting): authenticate NOTE: no keyring found; disabled cephx authentication", "2018-05-31 00:34:42.621008 7f44e1216700  0 librados: client.admin authentication error (110) Connection timed out", "[errno 110] error connecting to the cluster", "2018-05-31 00:34:52.798647 7fde3d583700 -1 auth: unable to find a keyring on /etc/ceph/ceph.client.admin.keyring,/etc/ceph/ceph.keyring,/etc/ceph/keyring,/etc/ceph/keyring.bin,: (2) No such file or directory", "2018-05-31 00:39:52.799392 7fde3d583700  0 monclient(hunting): authenticate timed out after 300", "2018-05-31 00:39:52.799425 7fde3d583700 -1 monclient(hunting): authenticate NOTE: no keyring found; disabled cephx authentication", "2018-05-31 00:39:52.799434 7fde3d583700  0 librados: client.admin authentication error (110) Connection timed out", "[errno 110] error connecting to the cluster", "2018-05-31 00:40:02.973675 7ff45207b700 -1 auth: unable to find a keyring on /etc/ceph/ceph.client.admin.keyring,/etc/ceph/ceph.keyring,/etc/ceph/keyring,/etc/ceph/keyring.bin,: (2) No such file or directory", "2018-05-31 00:45:02.974424 7ff45207b700  0 monclient(hunting): authenticate timed out after 300", "2018-05-31 00:45:02.974458 7ff45207b700 -1 monclient(hunting): authenticate NOTE: no keyring found; disabled cephx authentication", "2018-05-31 00:45:02.974466 7ff45207b700  0 librados: client.admin authentication error (110) Connection timed out", "[errno 110] error connecting to the cluster", "2018-05-31 00:45:13.156724 7f1b09e84700 -1 auth: unable to find a keyring on /etc/ceph/ceph.client.admin.keyring,/etc/ceph/ceph.keyring,/etc/ceph/keyring,/etc/ceph/keyring.bin,: (2) No such file or directory", "2018-05-31 00:50:13.157401 7f1b09e84700  0 monclient(hunting): authenticate timed out after 300", "2018-05-31 00:50:13.157435 7f1b09e84700 -1 monclient(hunting): authenticate NOTE: no keyring found; disabled cephx authentication", "2018-05-31 00:50:13.157444 7f1b09e84700  0 librados: client.admin authentication error (110) Connection timed out", "[errno 110] error connecting to the cluster", "2018-05-31 00:50:23.338306 7f29ee0a4700 -1 auth: unable to find a keyring on /etc/ceph/ceph.client.admin.keyring,/etc/ceph/ceph.keyring,/etc/ceph/keyring,/etc/ceph/keyring.bin,: (2) No such file or directory", "2018-05-31 00:55:23.339110 7f29ee0a4700  0 monclient(hunting): authenticate timed out after 300", "2018-05-31 00:55:23.339143 7f29ee0a4700 -1 monclient(hunting): authenticate NOTE: no keyring found; disabled cephx authentication", "2018-05-31 00:55:23.339152 7f29ee0a4700  0 librados: client.admin authentication error (110) Connection timed out", "[errno 110] error connecting to the cluster", "2018-05-31 00:55:33.518850 7f5ed02e5700 -1 auth: unable to find a keyring on /etc/ceph/ceph.client.admin.keyring,/etc/ceph/ceph.keyring,/etc/ceph/keyring,/etc/ceph/keyring.bin,: (2) No such file or directory", "2018-05-31 01:00:33.519561 7f5ed02e5700  0 monclient(hunting): authenticate timed out after 300", "2018-05-31 01:00:33.519595 7f5ed02e5700 -1 monclient(hunting): authenticate NOTE: no keyring found; disabled cephx authentication", "2018-05-31 01:00:33.519603 7f5ed02e5700  0 librados: client.admin authentication error (110) Connection timed out", "[errno 110] error connecting to the cluster", "2018-05-31 01:00:43.691468 7fd9cc32f700 -1 auth: unable to find a keyring on /etc/ceph/ceph.client.admin.keyring,/etc/ceph/ceph.keyring,/etc/ceph/keyring,/etc/ceph/keyring.bin,: (2) No such file or directory", "2018-05-31 01:05:43.691841 7fd9cc32f700  0 monclient(hunting): authenticate timed out after 300", "2018-05-31 01:05:43.691875 7fd9cc32f700 -1 monclient(hunting): authenticate NOTE: no keyring found; disabled cephx authentication", "2018-05-31 01:05:43.691883 7fd9cc32f700  0 librados: client.admin authentication error (110) Connection timed out", "[errno 110] error connecting to the cluster", "2018-05-31 01:05:53.870859 7f4ccccbb700 -1 auth: unable to find a keyring on /etc/ceph/ceph.client.admin.keyring,/etc/ceph/ceph.keyring,/etc/ceph/keyring,/etc/ceph/keyring.bin,: (2) No such file or directory", "2018-05-31 01:10:53.871612 7f4ccccbb700  0 monclient(hunting): authenticate timed out after 300", "2018-05-31 01:10:53.871645 7f4ccccbb700 -1 monclient(hunting): authenticate NOTE: no keyring found; disabled cephx authentication", "2018-05-31 01:10:53.871653 7f4ccccbb700  0 librados: client.admin authentication error (110) Connection timed out", "[errno 110] error connecting to the cluster", "2018-05-31 01:11:04.050610 7fd03188b700 -1 auth: unable to find a keyring on /etc/ceph/ceph.client.admin.keyring,/etc/ceph/ceph.keyring,/etc/ceph/keyring,/etc/ceph/keyring.bin,: (2) No such file or directory", "2018-05-31 01:16:04.051596 7fd03188b700  0 monclient(hunting): authenticate timed out after 300", "2018-05-31 01:16:04.051629 7fd03188b700 -1 monclient(hunting): authenticate NOTE: no keyring found; disabled cephx authentication", "2018-05-31 01:16:04.051638 7fd03188b700  0 librados: client.admin authentication error (110) Connection timed out", "[errno 110] error connecting to the cluster", "2018-05-31 01:16:14.234148 7fdbb90d6700 -1 auth: unable to find a keyring on /etc/ceph/ceph.client.admin.keyring,/etc/ceph/ceph.keyring,/etc/ceph/keyring,/etc/ceph/keyring.bin,: (2) No such file or directory"], "stdout": "No CLIENT_SCRIPT defined, running built-in radosbench script.", "stdout_lines": ["No CLIENT_SCRIPT defined, running built-in radosbench script."]}
2018-05-31 01:19:11,034 p=10 u=root |  ...ignoring
2018-05-31 01:19:11,035 p=10 u=root |  fatal: [node2]: FAILED! => {"ansible_job_id": "354702438475.10604", "changed": true, "cmd": "docker run --rm --name baseliner_node2  -e repetition=\"1\" -e CEPH_PUBLIC_NETWORK=\"128.110.152.0/22\" -e MON_IP=\"128.110.153.114\" -e OSD=\"True\" -e OSD_DEVICES=\"/dev/sdb\" --net=host -v /tmp/etc:/etc/ceph -v /tmp/baseliner_output_node2:/results -v /tmp/baseliner_output_node2:/output mariettesouppe/rados_bench:master-ec8d33f-luminous-ubuntu-16.04-x86_64 ", "delta": "0:05:00.650423", "end": "2018-05-30 17:43:01.053145", "failed": true, "finished": 1, "msg": "non-zero return code", "rc": 1, "start": "2018-05-30 17:38:00.402722", "stderr": "+ '[' -z '' ']'\n+ '[' -z True ']'\n+ '[' -z 128.110.152.0/22 ']'\n+ '[' -z 128.110.153.114 ']'\n+ sed -i -e s@monip@128.110.153.114@ /etc/ceph/ceph.conf\n+ sed -i -e s@cephnet@128.110.152.0/22@ /etc/ceph/ceph.conf\n+ '[' -f /etc/ceph/extra.conf ']'\n+ '[' -n '' ']'\n+ '[' -n True ']'\n+ '[' -n '' ']'\n+ exec run_osds\n+ '[' -z /dev/sdb ']'\n+ ceph auth get client.bootstrap-osd -o /var/lib/ceph/bootstrap-osd/ceph.keyring\n2018-05-30 23:38:00.855473 7fc68c43c700 -1 auth: unable to find a keyring on /etc/ceph/ceph.client.admin.keyring,/etc/ceph/ceph.keyring,/etc/ceph/keyring,/etc/ceph/keyring.bin,: (2) No such file or directory\n2018-05-30 23:43:00.855749 7fc68c43c700  0 monclient(hunting): authenticate timed out after 300\n2018-05-30 23:43:00.855782 7fc68c43c700 -1 monclient(hunting): authenticate NOTE: no keyring found; disabled cephx authentication\n2018-05-30 23:43:00.855791 7fc68c43c700  0 librados: client.admin authentication error (110) Connection timed out\n[errno 110] error connecting to the cluster", "stderr_lines": ["+ '[' -z '' ']'", "+ '[' -z True ']'", "+ '[' -z 128.110.152.0/22 ']'", "+ '[' -z 128.110.153.114 ']'", "+ sed -i -e s@monip@128.110.153.114@ /etc/ceph/ceph.conf", "+ sed -i -e s@cephnet@128.110.152.0/22@ /etc/ceph/ceph.conf", "+ '[' -f /etc/ceph/extra.conf ']'", "+ '[' -n '' ']'", "+ '[' -n True ']'", "+ '[' -n '' ']'", "+ exec run_osds", "+ '[' -z /dev/sdb ']'", "+ ceph auth get client.bootstrap-osd -o /var/lib/ceph/bootstrap-osd/ceph.keyring", "2018-05-30 23:38:00.855473 7fc68c43c700 -1 auth: unable to find a keyring on /etc/ceph/ceph.client.admin.keyring,/etc/ceph/ceph.keyring,/etc/ceph/keyring,/etc/ceph/keyring.bin,: (2) No such file or directory", "2018-05-30 23:43:00.855749 7fc68c43c700  0 monclient(hunting): authenticate timed out after 300", "2018-05-30 23:43:00.855782 7fc68c43c700 -1 monclient(hunting): authenticate NOTE: no keyring found; disabled cephx authentication", "2018-05-30 23:43:00.855791 7fc68c43c700  0 librados: client.admin authentication error (110) Connection timed out", "[errno 110] error connecting to the cluster"], "stdout": "", "stdout_lines": []}
2018-05-31 01:19:11,035 p=10 u=root |  ...ignoring
2018-05-31 01:19:11,065 p=10 u=root |  fatal: [node1]: FAILED! => {"ansible_job_id": "150446818319.9545", "changed": true, "cmd": "docker run --rm --name baseliner_node1  -e repetition=\"1\" -e CEPH_PUBLIC_NETWORK=\"128.110.152.0/22\" -e MON_IP=\"128.110.153.197\" -e MONITOR=\"True\" --net=host -v /tmp/etc:/etc/ceph -v /tmp/baseliner_output_node1:/results -v /tmp/baseliner_output_node1:/output mariettesouppe/rados_bench:master-ec8d33f-luminous-ubuntu-16.04-x86_64 ", "delta": "1:40:57.809206", "end": "2018-05-30 19:18:58.205331", "failed": true, "finished": 1, "msg": "non-zero return code", "rc": 1, "start": "2018-05-30 17:38:00.396125", "stderr": "+ '[' -z True ']'\n+ '[' -z 128.110.152.0/22 ']'\n+ '[' -z 128.110.153.197 ']'\n+ sed -i -e s@monip@128.110.153.197@ /etc/ceph/ceph.conf\n+ sed -i -e s@cephnet@128.110.152.0/22@ /etc/ceph/ceph.conf\n+ '[' -f /etc/ceph/extra.conf ']'\n+ '[' -n True ']'\n+ '[' -n '' ']'\n+ exec /entrypoint.sh mon\n2018-05-30 23:38:01.237984 7ffbb727cf00  0 set uid:gid to 64045:64045 (ceph:ceph)\n2018-05-30 23:38:01.238000 7ffbb727cf00  0 ceph version 12.2.5 (cad919881333ac92274171586c827e01f554a70a) luminous (stable), process (unknown), pid 123\n2018-05-30 23:38:01.238089 7ffbb727cf00  0 pidfile_write: ignore empty --pid-file\n2018-05-30 23:38:01.245537 7ffbb727cf00  0 load: jerasure load: lrc load: isa \n2018-05-30 23:38:01.245641 7ffbb727cf00  0  set rocksdb option compression = kNoCompression\n2018-05-30 23:38:01.245654 7ffbb727cf00  0  set rocksdb option write_buffer_size = 33554432\n2018-05-30 23:38:01.245672 7ffbb727cf00  0  set rocksdb option compression = kNoCompression\n2018-05-30 23:38:01.245678 7ffbb727cf00  0  set rocksdb option write_buffer_size = 33554432\n2018-05-30 23:38:01.245808 7ffbb727cf00  4 rocksdb: RocksDB version: 5.4.0\n\n2018-05-30 23:38:01.245819 7ffbb727cf00  4 rocksdb: Git sha rocksdb_build_git_sha:@0@\n2018-05-30 23:38:01.245821 7ffbb727cf00  4 rocksdb: Compile date Apr 23 2018\n2018-05-30 23:38:01.245822 7ffbb727cf00  4 rocksdb: DB SUMMARY\n\n2018-05-30 23:38:01.245875 7ffbb727cf00  4 rocksdb: CURRENT file:  CURRENT\n\n2018-05-30 23:38:01.245877 7ffbb727cf00  4 rocksdb: IDENTITY file:  IDENTITY\n\n2018-05-30 23:38:01.245889 7ffbb727cf00  4 rocksdb: MANIFEST file:  MANIFEST-000001 size: 13 Bytes\n\n2018-05-30 23:38:01.245893 7ffbb727cf00  4 rocksdb: SST files in /var/lib/ceph/mon/ceph-node/store.db dir, Total Num: 0, files: \n\n2018-05-30 23:38:01.245896 7ffbb727cf00  4 rocksdb: Write Ahead Log file in /var/lib/ceph/mon/ceph-node/store.db: 000003.log size: 1081 ; \n\n2018-05-30 23:38:01.245901 7ffbb727cf00  4 rocksdb:                         Options.error_if_exists: 0\n2018-05-30 23:38:01.245902 7ffbb727cf00  4 rocksdb:                       Options.create_if_missing: 0\n2018-05-30 23:38:01.245903 7ffbb727cf00  4 rocksdb:                         Options.paranoid_checks: 1\n2018-05-30 23:38:01.245904 7ffbb727cf00  4 rocksdb:                                     Options.env: 0x563740f3d020\n2018-05-30 23:38:01.245906 7ffbb727cf00  4 rocksdb:                                Options.info_log: 0x563742394c20\n2018-05-30 23:38:01.245908 7ffbb727cf00  4 rocksdb:                          Options.max_open_files: -1\n2018-05-30 23:38:01.245910 7ffbb727cf00  4 rocksdb:                Options.max_file_opening_threads: 16\n2018-05-30 23:38:01.245911 7ffbb727cf00  4 rocksdb:                               Options.use_fsync: 0\n2018-05-30 23:38:01.245912 7ffbb727cf00  4 rocksdb:                       Options.max_log_file_size: 0\n2018-05-30 23:38:01.245914 7ffbb727cf00  4 rocksdb:                  Options.max_manifest_file_size: 18446744073709551615\n2018-05-30 23:38:01.245915 7ffbb727cf00  4 rocksdb:                   Options.log_file_time_to_roll: 0\n2018-05-30 23:38:01.245916 7ffbb727cf00  4 rocksdb:                       Options.keep_log_file_num: 1000\n2018-05-30 23:38:01.245918 7ffbb727cf00  4 rocksdb:                    Options.recycle_log_file_num: 0\n2018-05-30 23:38:01.245919 7ffbb727cf00  4 rocksdb:                         Options.allow_fallocate: 1\n2018-05-30 23:38:01.245921 7ffbb727cf00  4 rocksdb:                        Options.allow_mmap_reads: 0\n2018-05-30 23:38:01.245922 7ffbb727cf00  4 rocksdb:                       Options.allow_mmap_writes: 0\n2018-05-30 23:38:01.245923 7ffbb727cf00  4 rocksdb:                        Options.use_direct_reads: 0\n2018-05-30 23:38:01.245925 7ffbb727cf00  4 rocksdb:                        Options.use_direct_io_for_flush_and_compaction: 0\n2018-05-30 23:38:01.245926 7ffbb727cf00  4 rocksdb:          Options.create_missing_column_families: 0\n2018-05-30 23:38:01.245937 7ffbb727cf00  4 rocksdb:                              Options.db_log_dir: \n2018-05-30 23:38:01.245939 7ffbb727cf00  4 rocksdb:                                 Options.wal_dir: /var/lib/ceph/mon/ceph-node/store.db\n2018-05-30 23:38:01.245941 7ffbb727cf00  4 rocksdb:                Options.table_cache_numshardbits: 6\n2018-05-30 23:38:01.245943 7ffbb727cf00  4 rocksdb:                      Options.max_subcompactions: 1\n2018-05-30 23:38:01.245944 7ffbb727cf00  4 rocksdb:                  Options.max_background_flushes: 1\n2018-05-30 23:38:01.245945 7ffbb727cf00  4 rocksdb:                         Options.WAL_ttl_seconds: 0\n2018-05-30 23:38:01.245947 7ffbb727cf00  4 rocksdb:                       Options.WAL_size_limit_MB: 0\n2018-05-30 23:38:01.245948 7ffbb727cf00  4 rocksdb:             Options.manifest_preallocation_size: 4194304\n2018-05-30 23:38:01.245950 7ffbb727cf00  4 rocksdb:                     Options.is_fd_close_on_exec: 1\n2018-05-30 23:38:01.245951 7ffbb727cf00  4 rocksdb:                   Options.advise_random_on_open: 1\n2018-05-30 23:38:01.245952 7ffbb727cf00  4 rocksdb:                    Options.db_write_buffer_size: 0\n2018-05-30 23:38:01.245954 7ffbb727cf00  4 rocksdb:         Options.access_hint_on_compaction_start: 1\n2018-05-30 23:38:01.245955 7ffbb727cf00  4 rocksdb:  Options.new_table_reader_for_compaction_inputs: 0\n2018-05-30 23:38:01.245957 7ffbb727cf00  4 rocksdb:               Options.compaction_readahead_size: 0\n2018-05-30 23:38:01.245958 7ffbb727cf00  4 rocksdb:           Options.random_access_max_buffer_size: 1048576\n2018-05-30 23:38:01.245960 7ffbb727cf00  4 rocksdb:           Options.writable_file_max_buffer_size: 1048576\n2018-05-30 23:38:01.245961 7ffbb727cf00  4 rocksdb:                      Options.use_adaptive_mutex: 0\n2018-05-30 23:38:01.245962 7ffbb727cf00  4 rocksdb:                            Options.rate_limiter: (nil)\n2018-05-30 23:38:01.245964 7ffbb727cf00  4 rocksdb:     Options.sst_file_manager.rate_bytes_per_sec: 0\n2018-05-30 23:38:01.245965 7ffbb727cf00  4 rocksdb:                          Options.bytes_per_sync: 0\n2018-05-30 23:38:01.245966 7ffbb727cf00  4 rocksdb:                      Options.wal_bytes_per_sync: 0\n2018-05-30 23:38:01.245968 7ffbb727cf00  4 rocksdb:                       Options.wal_recovery_mode: 2\n2018-05-30 23:38:01.245969 7ffbb727cf00  4 rocksdb:                  Options.enable_thread_tracking: 0\n2018-05-30 23:38:01.245971 7ffbb727cf00  4 rocksdb:         Options.allow_concurrent_memtable_write: 1\n2018-05-30 23:38:01.245972 7ffbb727cf00  4 rocksdb:      Options.enable_write_thread_adaptive_yield: 1\n2018-05-30 23:38:01.245974 7ffbb727cf00  4 rocksdb:             Options.write_thread_max_yield_usec: 100\n2018-05-30 23:38:01.245975 7ffbb727cf00  4 rocksdb:            Options.write_thread_slow_yield_usec: 3\n2018-05-30 23:38:01.245976 7ffbb727cf00  4 rocksdb:                               Options.row_cache: None\n2018-05-30 23:38:01.245978 7ffbb727cf00  4 rocksdb:                              Options.wal_filter: None\n2018-05-30 23:38:01.245979 7ffbb727cf00  4 rocksdb:             Options.avoid_flush_during_recovery: 0\n2018-05-30 23:38:01.245980 7ffbb727cf00  4 rocksdb:             Options.base_background_compactions: 1\n2018-05-30 23:38:01.245980 7ffbb727cf00  4 rocksdb:             Options.max_background_compactions: 1\n2018-05-30 23:38:01.245981 7ffbb727cf00  4 rocksdb:             Options.avoid_flush_during_shutdown: 0\n2018-05-30 23:38:01.245982 7ffbb727cf00  4 rocksdb:             Options.delayed_write_rate : 16777216\n2018-05-30 23:38:01.245983 7ffbb727cf00  4 rocksdb:             Options.max_total_wal_size: 0\n2018-05-30 23:38:01.245984 7ffbb727cf00  4 rocksdb:             Options.delete_obsolete_files_period_micros: 21600000000\n2018-05-30 23:38:01.245986 7ffbb727cf00  4 rocksdb:                   Options.stats_dump_period_sec: 600\n2018-05-30 23:38:01.245987 7ffbb727cf00  4 rocksdb: Compression algorithms supported:\n2018-05-30 23:38:01.245989 7ffbb727cf00  4 rocksdb: \tSnappy supported: 0\n2018-05-30 23:38:01.245990 7ffbb727cf00  4 rocksdb: \tZlib supported: 0\n2018-05-30 23:38:01.245992 7ffbb727cf00  4 rocksdb: \tBzip supported: 0\n2018-05-30 23:38:01.245993 7ffbb727cf00  4 rocksdb: \tLZ4 supported: 0\n2018-05-30 23:38:01.245994 7ffbb727cf00  4 rocksdb: \tZSTD supported: 0\n2018-05-30 23:38:01.245996 7ffbb727cf00  4 rocksdb: Fast CRC32 supported: 1\n2018-05-30 23:38:01.246132 7ffbb727cf00  4 rocksdb: [/build/ceph-12.2.5/src/rocksdb/db/version_set.cc:2609] Recovering from manifest file: MANIFEST-000001\n\n2018-05-30 23:38:01.246193 7ffbb727cf00  4 rocksdb: [/build/ceph-12.2.5/src/rocksdb/db/column_family.cc:407] --------------- Options for column family [default]:\n\n2018-05-30 23:38:01.246196 7ffbb727cf00  4 rocksdb:               Options.comparator: leveldb.BytewiseComparator\n2018-05-30 23:38:01.246198 7ffbb727cf00  4 rocksdb:           Options.merge_operator: \n2018-05-30 23:38:01.246199 7ffbb727cf00  4 rocksdb:        Options.compaction_filter: None\n2018-05-30 23:38:01.246200 7ffbb727cf00  4 rocksdb:        Options.compaction_filter_factory: None\n2018-05-30 23:38:01.246201 7ffbb727cf00  4 rocksdb:         Options.memtable_factory: SkipListFactory\n2018-05-30 23:38:01.246202 7ffbb727cf00  4 rocksdb:            Options.table_factory: BlockBasedTable\n2018-05-30 23:38:01.246221 7ffbb727cf00  4 rocksdb:            table_factory options:   flush_block_policy_factory: FlushBlockBySizePolicyFactory (0x5637420700f8)\n  cache_index_and_filter_blocks: 1\n  cache_index_and_filter_blocks_with_high_priority: 1\n  pin_l0_filter_and_index_blocks_in_cache: 1\n  index_type: 0\n  hash_index_allow_collision: 1\n  checksum: 1\n  no_block_cache: 0\n  block_cache: 0x56374237c5c0\n  block_cache_name: LRUCache\n  block_cache_options:\n    capacity : 134217728\n    num_shard_bits : 4\n    strict_capacity_limit : 0\n    high_pri_pool_ratio: 0.000\n  block_cache_compressed: (nil)\n  persistent_cache: (nil)\n  block_size: 4096\n  block_size_deviation: 10\n  block_restart_interval: 16\n  index_block_restart_interval: 1\n  filter_policy: rocksdb.BuiltinBloomFilter\n  whole_key_filtering: 1\n  format_version: 2\n\n2018-05-30 23:38:01.246226 7ffbb727cf00  4 rocksdb:        Options.write_buffer_size: 33554432\n2018-05-30 23:38:01.246227 7ffbb727cf00  4 rocksdb:  Options.max_write_buffer_number: 2\n2018-05-30 23:38:01.246228 7ffbb727cf00  4 rocksdb:          Options.compression: NoCompression\n2018-05-30 23:38:01.246229 7ffbb727cf00  4 rocksdb:                  Options.bottommost_compression: Disabled\n2018-05-30 23:38:01.246229 7ffbb727cf00  4 rocksdb:       Options.prefix_extractor: nullptr\n2018-05-30 23:38:01.246230 7ffbb727cf00  4 rocksdb:   Options.memtable_insert_with_hint_prefix_extractor: nullptr\n2018-05-30 23:38:01.246231 7ffbb727cf00  4 rocksdb:             Options.num_levels: 7\n2018-05-30 23:38:01.246232 7ffbb727cf00  4 rocksdb:        Options.min_write_buffer_number_to_merge: 1\n2018-05-30 23:38:01.246232 7ffbb727cf00  4 rocksdb:     Options.max_write_buffer_number_to_maintain: 0\n2018-05-30 23:38:01.246233 7ffbb727cf00  4 rocksdb:            Options.compression_opts.window_bits: -14\n2018-05-30 23:38:01.246234 7ffbb727cf00  4 rocksdb:                  Options.compression_opts.level: -1\n2018-05-30 23:38:01.246250 7ffbb727cf00  4 rocksdb:               Options.compression_opts.strategy: 0\n2018-05-30 23:38:01.246251 7ffbb727cf00  4 rocksdb:         Options.compression_opts.max_dict_bytes: 0\n2018-05-30 23:38:01.246252 7ffbb727cf00  4 rocksdb:      Options.level0_file_num_compaction_trigger: 4\n2018-05-30 23:38:01.246252 7ffbb727cf00  4 rocksdb:          Options.level0_slowdown_writes_trigger: 20\n2018-05-30 23:38:01.246253 7ffbb727cf00  4 rocksdb:              Options.level0_stop_writes_trigger: 36\n2018-05-30 23:38:01.246253 7ffbb727cf00  4 rocksdb:                   Options.target_file_size_base: 67108864\n2018-05-30 23:38:01.246254 7ffbb727cf00  4 rocksdb:             Options.target_file_size_multiplier: 1\n2018-05-30 23:38:01.246255 7ffbb727cf00  4 rocksdb:                Options.max_bytes_for_level_base: 268435456\n2018-05-30 23:38:01.246256 7ffbb727cf00  4 rocksdb: Options.level_compaction_dynamic_level_bytes: 0\n2018-05-30 23:38:01.246256 7ffbb727cf00  4 rocksdb:          Options.max_bytes_for_level_multiplier: 10.000000\n2018-05-30 23:38:01.246259 7ffbb727cf00  4 rocksdb: Options.max_bytes_for_level_multiplier_addtl[0]: 1\n2018-05-30 23:38:01.246260 7ffbb727cf00  4 rocksdb: Options.max_bytes_for_level_multiplier_addtl[1]: 1\n2018-05-30 23:38:01.246261 7ffbb727cf00  4 rocksdb: Options.max_bytes_for_level_multiplier_addtl[2]: 1\n2018-05-30 23:38:01.246261 7ffbb727cf00  4 rocksdb: Options.max_bytes_for_level_multiplier_addtl[3]: 1\n2018-05-30 23:38:01.246262 7ffbb727cf00  4 rocksdb: Options.max_bytes_for_level_multiplier_addtl[4]: 1\n2018-05-30 23:38:01.246263 7ffbb727cf00  4 rocksdb: Options.max_bytes_for_level_multiplier_addtl[5]: 1\n2018-05-30 23:38:01.246263 7ffbb727cf00  4 rocksdb: Options.max_bytes_for_level_multiplier_addtl[6]: 1\n2018-05-30 23:38:01.246264 7ffbb727cf00  4 rocksdb:       Options.max_sequential_skip_in_iterations: 8\n2018-05-30 23:38:01.246265 7ffbb727cf00  4 rocksdb:                    Options.max_compaction_bytes: 1677721600\n2018-05-30 23:38:01.246266 7ffbb727cf00  4 rocksdb:                        Options.arena_block_size: 4194304\n2018-05-30 23:38:01.246267 7ffbb727cf00  4 rocksdb:   Options.soft_pending_compaction_bytes_limit: 68719476736\n2018-05-30 23:38:01.246267 7ffbb727cf00  4 rocksdb:   Options.hard_pending_compaction_bytes_limit: 274877906944\n2018-05-30 23:38:01.246268 7ffbb727cf00  4 rocksdb:       Options.rate_limit_delay_max_milliseconds: 100\n2018-05-30 23:38:01.246269 7ffbb727cf00  4 rocksdb:                Options.disable_auto_compactions: 0\n2018-05-30 23:38:01.246270 7ffbb727cf00  4 rocksdb:                         Options.compaction_style: kCompactionStyleLevel\n2018-05-30 23:38:01.246271 7ffbb727cf00  4 rocksdb:                           Options.compaction_pri: kByCompensatedSize\n2018-05-30 23:38:01.246272 7ffbb727cf00  4 rocksdb:  Options.compaction_options_universal.size_ratio: 1\n2018-05-30 23:38:01.246273 7ffbb727cf00  4 rocksdb: Options.compaction_options_universal.min_merge_width: 2\n2018-05-30 23:38:01.246273 7ffbb727cf00  4 rocksdb: Options.compaction_options_universal.max_merge_width: 4294967295\n2018-05-30 23:38:01.246274 7ffbb727cf00  4 rocksdb: Options.compaction_options_universal.max_size_amplification_percent: 200\n2018-05-30 23:38:01.246275 7ffbb727cf00  4 rocksdb: Options.compaction_options_universal.compression_size_percent: -1\n2018-05-30 23:38:01.246276 7ffbb727cf00  4 rocksdb: Options.compaction_options_fifo.max_table_files_size: 1073741824\n2018-05-30 23:38:01.246277 7ffbb727cf00  4 rocksdb:                   Options.table_properties_collectors: \n2018-05-30 23:38:01.246277 7ffbb727cf00  4 rocksdb:                   Options.inplace_update_support: 0\n2018-05-30 23:38:01.246278 7ffbb727cf00  4 rocksdb:                 Options.inplace_update_num_locks: 10000\n2018-05-30 23:38:01.246279 7ffbb727cf00  4 rocksdb:               Options.memtable_prefix_bloom_size_ratio: 0.000000\n2018-05-30 23:38:01.246280 7ffbb727cf00  4 rocksdb:   Options.memtable_huge_page_size: 0\n2018-05-30 23:38:01.246281 7ffbb727cf00  4 rocksdb:                           Options.bloom_locality: 0\n2018-05-30 23:38:01.246282 7ffbb727cf00  4 rocksdb:                    Options.max_successive_merges: 0\n2018-05-30 23:38:01.246282 7ffbb727cf00  4 rocksdb:                Options.optimize_filters_for_hits: 0\n2018-05-30 23:38:01.246283 7ffbb727cf00  4 rocksdb:                Options.paranoid_file_checks: 0\n2018-05-30 23:38:01.246284 7ffbb727cf00  4 rocksdb:                Options.force_consistency_checks: 0\n2018-05-30 23:38:01.246285 7ffbb727cf00  4 rocksdb:                Options.report_bg_io_stats: 0\n2018-05-30 23:38:01.246948 7ffbb727cf00  4 rocksdb: [/build/ceph-12.2.5/src/rocksdb/db/version_set.cc:2859] Recovered from manifest file:/var/lib/ceph/mon/ceph-node/store.db/MANIFEST-000001 succeeded,manifest_file_number is 1, next_file_number is 3, last_sequence is 0, log_number is 0,prev_log_number is 0,max_column_family is 0\n\n2018-05-30 23:38:01.246958 7ffbb727cf00  4 rocksdb: [/build/ceph-12.2.5/src/rocksdb/db/version_set.cc:2867] Column family [default] (ID 0), log number is 0\n\n2018-05-30 23:38:01.247019 7ffbb727cf00  4 rocksdb: EVENT_LOG_v1 {\"time_micros\": 1527723481247008, \"job\": 1, \"event\": \"recovery_started\", \"log_files\": [3]}\n2018-05-30 23:38:01.247025 7ffbb727cf00  4 rocksdb: [/build/ceph-12.2.5/src/rocksdb/db/db_impl_open.cc:482] Recovering log #3 mode 2\n2018-05-30 23:38:01.252177 7ffbb727cf00  4 rocksdb: EVENT_LOG_v1 {\"time_micros\": 1527723481252159, \"cf_name\": \"default\", \"job\": 1, \"event\": \"table_file_creation\", \"file_number\": 4, \"file_size\": 2053, \"table_properties\": {\"data_size\": 1093, \"index_size\": 28, \"filter_size\": 23, \"raw_key_size\": 115, \"raw_average_key_size\": 23, \"raw_value_size\": 970, \"raw_average_value_size\": 194, \"num_data_blocks\": 1, \"num_entries\": 5, \"filter_policy_name\": \"rocksdb.BuiltinBloomFilter\", \"kDeletedKeys\": \"0\", \"kMergeOperands\": \"0\"}}\n2018-05-30 23:38:01.252225 7ffbb727cf00  4 rocksdb: [/build/ceph-12.2.5/src/rocksdb/db/version_set.cc:2395] Creating manifest 5\n\n2018-05-30 23:38:01.260437 7ffbb727cf00  4 rocksdb: EVENT_LOG_v1 {\"time_micros\": 1527723481260430, \"job\": 1, \"event\": \"recovery_finished\"}\n2018-05-30 23:38:01.269399 7ffbb727cf00  4 rocksdb: [/build/ceph-12.2.5/src/rocksdb/db/db_impl_open.cc:1063] DB pointer 0x563742490000\n2018-05-30 23:38:01.270854 7ffbb727cf00  0 starting mon.node rank 0 at public addr 128.110.153.197:6789/0 at bind addr 128.110.153.197:6789/0 mon_data /var/lib/ceph/mon/ceph-node fsid b2bb6c0b-6cc6-44f1-bf92-1ce81d78fbdc\n2018-05-30 23:38:01.271080 7ffbb727cf00  0 starting mon.node rank 0 at 128.110.153.197:6789/0 mon_data /var/lib/ceph/mon/ceph-node fsid b2bb6c0b-6cc6-44f1-bf92-1ce81d78fbdc\n2018-05-30 23:38:01.272715 7ffbb727cf00  1 mon.node@-1(probing) e0 preinit fsid b2bb6c0b-6cc6-44f1-bf92-1ce81d78fbdc\n2018-05-30 23:38:01.272917 7ffbb727cf00  1 mon.node@-1(probing).mds e0 Unable to load 'last_metadata'\n2018-05-30 23:38:01.274622 7ffbb727cf00  0 mon.node@-1(probing) e0  my rank is now 0 (was -1)\n2018-05-30 23:38:01.274660 7ffbb727cf00  1 mon.node@0(probing) e0 win_standalone_election\n2018-05-30 23:38:01.274682 7ffbb727cf00  1 mon.node@0(probing).elector(0) init, first boot, initializing epoch at 1 \n2018-05-30 23:38:01.283967 7ffbb727cf00  0 log_channel(cluster) log [INF] : mon.node is new leader, mons node in quorum (ranks 0)\n2018-05-30 23:38:01.288393 7ffbb727cf00  1 mon.node@0(leader).osd e0 create_pending setting backfillfull_ratio = 0.9\n2018-05-30 23:38:01.288426 7ffbb727cf00  1 mon.node@0(leader).osd e0 create_pending setting full_ratio = 0.95\n2018-05-30 23:38:01.288432 7ffbb727cf00  1 mon.node@0(leader).osd e0 create_pending setting nearfull_ratio = 0.85\n2018-05-30 23:38:01.288754 7ffbb727cf00  1 mon.node@0(leader).osd e0 encode_pending skipping prime_pg_temp; mapping job did not start\n2018-05-30 23:38:01.290382 7ffbb727cf00  1 mon.node@0(leader) e0 _apply_compatset_features enabling new quorum features: compat={},rocompat={},incompat={4=support erasure code pools,5=new-style osdmap encoding,6=support isa/lrc erasure code,7=support shec erasure code}\n2018-05-30 23:38:01.296805 7ffba9886700  1 mon.node@0(leader).paxosservice(pgmap 0..0) refresh upgraded, format 1 -> 0\n2018-05-30 23:38:01.296825 7ffba9886700  1 mon.node@0(leader).pg v0 on_upgrade discarding in-core PGMap\n2018-05-30 23:38:01.301403 7ffba9886700  1 mon.node@0(leader).paxosservice(auth 0..0) refresh upgraded, format 2 -> 0\n2018-05-30 23:38:01.301716 7ffba9886700  1 mon.node@0(probing) e1 win_standalone_election\n2018-05-30 23:38:01.301738 7ffba9886700  1 mon.node@0(probing).elector(2) init, last seen epoch 2\n2018-05-30 23:38:01.305742 7ffba9886700  0 log_channel(cluster) log [INF] : mon.node is new leader, mons node in quorum (ranks 0)\n2018-05-30 23:38:01.309672 7ffba9886700  0 log_channel(cluster) log [DBG] : monmap e1: 1 mons at {node=128.110.153.197:6789/0}\n2018-05-30 23:38:01.313774 7ffba9886700  1 mon.node@0(leader).osd e0 create_pending setting backfillfull_ratio = 0.9\n2018-05-30 23:38:01.313784 7ffba9886700  1 mon.node@0(leader).osd e0 create_pending setting full_ratio = 0.95\n2018-05-30 23:38:01.313787 7ffba9886700  1 mon.node@0(leader).osd e0 create_pending setting nearfull_ratio = 0.85\n2018-05-30 23:38:01.313904 7ffba9886700  1 mon.node@0(leader).osd e0 encode_pending skipping prime_pg_temp; mapping job did not start\n2018-05-30 23:38:01.314492 7ffba9886700  1 mon.node@0(leader) e1 _apply_compatset_features enabling new quorum features: compat={},rocompat={},incompat={8=support monmap features,9=luminous ondisk layout}\n2018-05-30 23:38:01.322998 7ffba9886700  1 mon.node@0(leader).paxosservice(auth 0..0) refresh upgraded, format 2 -> 0\n2018-05-30 23:38:01.323115 7ffba9886700  0 log_channel(cluster) log [INF] : pgmap 0 pgs: ; 0 bytes data, 0 kB used, 0 kB / 0 kB avail\n2018-05-30 23:38:01.332668 7ffba9886700  0 mon.node@0(leader).mds e1 print_map\ne1\nenable_multiple, ever_enabled_multiple: 0,0\ncompat: compat={},rocompat={},incompat={1=base v0.20,2=client writeable ranges,3=default file layouts on dirs,4=dir inode in separate object,5=mds uses versioned encoding,6=dirfrag is stored in omap,8=no anchor table,9=file layout v2}\nlegacy client fscid: -1\n \nNo filesystems configured\n\n2018-05-30 23:38:01.332844 7ffba9886700  1 mon.node@0(leader).osd e1 e1: 0 total, 0 up, 0 in\n2018-05-30 23:38:01.337177 7ffba9886700  0 mon.node@0(leader).osd e1 crush map has features 288514050185494528, adjusting msgr requires\n2018-05-30 23:38:01.337181 7ffba9886700  0 mon.node@0(leader).osd e1 crush map has features 288514050185494528, adjusting msgr requires\n2018-05-30 23:38:01.337184 7ffba9886700  0 mon.node@0(leader).osd e1 crush map has features 1009089990564790272, adjusting msgr requires\n2018-05-30 23:38:01.337185 7ffba9886700  0 mon.node@0(leader).osd e1 crush map has features 288514050185494528, adjusting msgr requires\n2018-05-30 23:38:01.337335 7ffba9886700  1 mon.node@0(leader).log v1 unable to write to '/var/log/ceph/ceph.log' for channel 'cluster': (2) No such file or directory\n2018-05-30 23:38:01.337402 7ffba9886700  1 mon.node@0(leader).paxosservice(auth 1..1) refresh upgraded, format 0 -> 2\n2018-05-30 23:38:01.342206 7ffba9886700  0 log_channel(cluster) log [DBG] : fsmap \n2018-05-30 23:38:01.342329 7ffba9886700  1 mon.node@0(leader).pg v1 check_osd_map will clear pg_map data\n2018-05-30 23:38:01.342334 7ffba9886700  1 mon.node@0(leader).pg v1 encode_pending clearing pgmap data at v2\n2018-05-30 23:38:01.346257 7ffba9886700  0 log_channel(cluster) log [DBG] : osdmap e1: 0 total, 0 up, 0 in\n2018-05-30 23:38:01.346508 7ffba9886700  0 log_channel(cluster) log [DBG] : mgrmap e1: no daemons active\n2018-05-30 23:38:01.358592 7ffba9886700  1 mon.node@0(leader).log v2 unable to write to '/var/log/ceph/ceph.log' for channel 'cluster': (2) No such file or directory\n2018-05-30 23:38:02.367975 7ffba9886700  1 mon.node@0(leader).log v3 unable to write to '/var/log/ceph/ceph.log' for channel 'cluster': (2) No such file or directory\n2018-05-30 23:39:01.273324 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12384 MB\n2018-05-30 23:39:56.181642 7ffbae08f700  0 mon.node@0(leader) e1 handle_command mon_command({\"prefix\": \"status\"} v 0) v1\n2018-05-30 23:39:56.181688 7ffbae08f700  0 log_channel(audit) log [DBG] : from='client.? 128.110.153.197:0/1393663213' entity='client.admin' cmd=[{\"prefix\": \"status\"}]: dispatch\n2018-05-30 23:40:01.273746 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12384 MB\n2018-05-30 23:41:01.274140 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12384 MB\n2018-05-30 23:42:01.274534 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12384 MB\n2018-05-30 23:43:01.274958 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12384 MB\n2018-05-30 23:44:01.275346 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12384 MB\n2018-05-30 23:45:01.275748 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12384 MB\n2018-05-30 23:46:01.276160 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12384 MB\n2018-05-30 23:47:01.276549 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12384 MB\n2018-05-30 23:48:01.276939 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12384 MB\n2018-05-30 23:49:01.277379 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12384 MB\n2018-05-30 23:50:01.277802 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12384 MB\n2018-05-30 23:51:01.278223 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12384 MB\n2018-05-30 23:52:01.278588 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12384 MB\n2018-05-30 23:53:01.278994 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12384 MB\n2018-05-30 23:54:01.279365 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12384 MB\n2018-05-30 23:55:01.279768 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12384 MB\n2018-05-30 23:56:01.280148 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12384 MB\n2018-05-30 23:57:01.280583 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12383 MB\n2018-05-30 23:58:01.281014 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12383 MB\n2018-05-30 23:59:01.281412 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12383 MB\n2018-05-31 00:00:01.281798 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12383 MB\n2018-05-31 00:01:01.282198 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12383 MB\n2018-05-31 00:02:01.282589 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12383 MB\n2018-05-31 00:03:01.283031 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12383 MB\n2018-05-31 00:04:01.283496 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12383 MB\n2018-05-31 00:05:01.283889 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12383 MB\n2018-05-31 00:06:01.284282 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12383 MB\n2018-05-31 00:07:01.284677 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12383 MB\n2018-05-31 00:08:01.285100 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12383 MB\n2018-05-31 00:09:01.285480 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12383 MB\n2018-05-31 00:10:01.285898 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12383 MB\n2018-05-31 00:11:01.286271 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12383 MB\n2018-05-31 00:12:01.286677 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12383 MB\n2018-05-31 00:13:01.287060 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12383 MB\n2018-05-31 00:14:01.287548 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12383 MB\n2018-05-31 00:15:01.287959 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12383 MB\n2018-05-31 00:16:01.288332 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12383 MB\n2018-05-31 00:17:01.288709 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12383 MB\n2018-05-31 00:18:01.289123 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12383 MB\n2018-05-31 00:19:01.289504 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12383 MB\n2018-05-31 00:20:01.289866 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12383 MB\n2018-05-31 00:21:01.290273 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12383 MB\n2018-05-31 00:22:01.290651 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12383 MB\n2018-05-31 00:23:01.291032 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12383 MB\n2018-05-31 00:24:01.291450 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12383 MB\n2018-05-31 00:25:01.291833 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12383 MB\n2018-05-31 00:26:01.292226 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12383 MB\n2018-05-31 00:27:01.292614 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12383 MB\n2018-05-31 00:28:01.293028 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12383 MB\n2018-05-31 00:29:01.293459 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12383 MB\n2018-05-31 00:30:01.293893 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12383 MB\n2018-05-31 00:31:01.294304 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12383 MB\n2018-05-31 00:32:01.294721 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12383 MB\n2018-05-31 00:33:01.295104 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12383 MB\n2018-05-31 00:34:01.295519 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12383 MB\n2018-05-31 00:35:01.295900 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12383 MB\n2018-05-31 00:36:01.296312 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12383 MB\n2018-05-31 00:37:01.296697 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12383 MB\n2018-05-31 00:38:01.297150 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12383 MB\n2018-05-31 00:39:01.297526 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12383 MB\n2018-05-31 00:40:01.297906 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12383 MB\n2018-05-31 00:41:01.298351 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12383 MB\n2018-05-31 00:42:01.298731 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12383 MB\n2018-05-31 00:43:01.299151 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12383 MB\n2018-05-31 00:44:01.299600 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12383 MB\n2018-05-31 00:45:01.300016 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12383 MB\n2018-05-31 00:46:01.300408 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12383 MB\n2018-05-31 00:47:01.300807 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12383 MB\n2018-05-31 00:48:01.301188 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12383 MB\n2018-05-31 00:49:01.301565 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12383 MB\n2018-05-31 00:50:01.301991 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12383 MB\n2018-05-31 00:51:01.302371 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12383 MB\n2018-05-31 00:52:01.302788 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12383 MB\n2018-05-31 00:53:01.303213 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12383 MB\n2018-05-31 00:54:01.303671 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12383 MB\n2018-05-31 00:55:01.304091 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12383 MB\n2018-05-31 00:56:01.304481 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12383 MB\n2018-05-31 00:57:01.304856 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12383 MB\n2018-05-31 00:58:01.305238 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12383 MB\n2018-05-31 00:59:01.305651 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12383 MB\n2018-05-31 01:00:01.306026 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12383 MB\n2018-05-31 01:01:01.306473 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12383 MB\n2018-05-31 01:02:01.306856 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12383 MB\n2018-05-31 01:03:01.307336 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12383 MB\n2018-05-31 01:04:01.307757 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12383 MB\n2018-05-31 01:05:01.308178 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12383 MB\n2018-05-31 01:06:01.308555 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12383 MB\n2018-05-31 01:07:01.308944 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12383 MB\n2018-05-31 01:08:01.309360 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12383 MB\n2018-05-31 01:09:01.309736 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12383 MB\n2018-05-31 01:10:01.310182 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12383 MB\n2018-05-31 01:11:01.310563 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12383 MB\n2018-05-31 01:12:01.310978 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12383 MB\n2018-05-31 01:13:01.311355 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12383 MB\n2018-05-31 01:14:01.311765 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12383 MB\n2018-05-31 01:15:01.312145 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12383 MB\n2018-05-31 01:16:01.312594 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12383 MB\n2018-05-31 01:17:01.312974 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2796 MB, avail 12383 MB\n2018-05-31 01:18:01.313403 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2796 MB, avail 12383 MB\n2018-05-31 01:18:58.052535 7ffbab88a700 -1 received  signal: Terminated from  PID: 1 task name:  UID: 0\n2018-05-31 01:18:58.052567 7ffbab88a700 -1 mon.node@0(leader) e1 *** Got Signal Terminated ***\n2018-05-31 01:18:58.052573 7ffbab88a700  1 mon.node@0(leader) e1 shutdown", "stderr_lines": ["+ '[' -z True ']'", "+ '[' -z 128.110.152.0/22 ']'", "+ '[' -z 128.110.153.197 ']'", "+ sed -i -e s@monip@128.110.153.197@ /etc/ceph/ceph.conf", "+ sed -i -e s@cephnet@128.110.152.0/22@ /etc/ceph/ceph.conf", "+ '[' -f /etc/ceph/extra.conf ']'", "+ '[' -n True ']'", "+ '[' -n '' ']'", "+ exec /entrypoint.sh mon", "2018-05-30 23:38:01.237984 7ffbb727cf00  0 set uid:gid to 64045:64045 (ceph:ceph)", "2018-05-30 23:38:01.238000 7ffbb727cf00  0 ceph version 12.2.5 (cad919881333ac92274171586c827e01f554a70a) luminous (stable), process (unknown), pid 123", "2018-05-30 23:38:01.238089 7ffbb727cf00  0 pidfile_write: ignore empty --pid-file", "2018-05-30 23:38:01.245537 7ffbb727cf00  0 load: jerasure load: lrc load: isa ", "2018-05-30 23:38:01.245641 7ffbb727cf00  0  set rocksdb option compression = kNoCompression", "2018-05-30 23:38:01.245654 7ffbb727cf00  0  set rocksdb option write_buffer_size = 33554432", "2018-05-30 23:38:01.245672 7ffbb727cf00  0  set rocksdb option compression = kNoCompression", "2018-05-30 23:38:01.245678 7ffbb727cf00  0  set rocksdb option write_buffer_size = 33554432", "2018-05-30 23:38:01.245808 7ffbb727cf00  4 rocksdb: RocksDB version: 5.4.0", "", "2018-05-30 23:38:01.245819 7ffbb727cf00  4 rocksdb: Git sha rocksdb_build_git_sha:@0@", "2018-05-30 23:38:01.245821 7ffbb727cf00  4 rocksdb: Compile date Apr 23 2018", "2018-05-30 23:38:01.245822 7ffbb727cf00  4 rocksdb: DB SUMMARY", "", "2018-05-30 23:38:01.245875 7ffbb727cf00  4 rocksdb: CURRENT file:  CURRENT", "", "2018-05-30 23:38:01.245877 7ffbb727cf00  4 rocksdb: IDENTITY file:  IDENTITY", "", "2018-05-30 23:38:01.245889 7ffbb727cf00  4 rocksdb: MANIFEST file:  MANIFEST-000001 size: 13 Bytes", "", "2018-05-30 23:38:01.245893 7ffbb727cf00  4 rocksdb: SST files in /var/lib/ceph/mon/ceph-node/store.db dir, Total Num: 0, files: ", "", "2018-05-30 23:38:01.245896 7ffbb727cf00  4 rocksdb: Write Ahead Log file in /var/lib/ceph/mon/ceph-node/store.db: 000003.log size: 1081 ; ", "", "2018-05-30 23:38:01.245901 7ffbb727cf00  4 rocksdb:                         Options.error_if_exists: 0", "2018-05-30 23:38:01.245902 7ffbb727cf00  4 rocksdb:                       Options.create_if_missing: 0", "2018-05-30 23:38:01.245903 7ffbb727cf00  4 rocksdb:                         Options.paranoid_checks: 1", "2018-05-30 23:38:01.245904 7ffbb727cf00  4 rocksdb:                                     Options.env: 0x563740f3d020", "2018-05-30 23:38:01.245906 7ffbb727cf00  4 rocksdb:                                Options.info_log: 0x563742394c20", "2018-05-30 23:38:01.245908 7ffbb727cf00  4 rocksdb:                          Options.max_open_files: -1", "2018-05-30 23:38:01.245910 7ffbb727cf00  4 rocksdb:                Options.max_file_opening_threads: 16", "2018-05-30 23:38:01.245911 7ffbb727cf00  4 rocksdb:                               Options.use_fsync: 0", "2018-05-30 23:38:01.245912 7ffbb727cf00  4 rocksdb:                       Options.max_log_file_size: 0", "2018-05-30 23:38:01.245914 7ffbb727cf00  4 rocksdb:                  Options.max_manifest_file_size: 18446744073709551615", "2018-05-30 23:38:01.245915 7ffbb727cf00  4 rocksdb:                   Options.log_file_time_to_roll: 0", "2018-05-30 23:38:01.245916 7ffbb727cf00  4 rocksdb:                       Options.keep_log_file_num: 1000", "2018-05-30 23:38:01.245918 7ffbb727cf00  4 rocksdb:                    Options.recycle_log_file_num: 0", "2018-05-30 23:38:01.245919 7ffbb727cf00  4 rocksdb:                         Options.allow_fallocate: 1", "2018-05-30 23:38:01.245921 7ffbb727cf00  4 rocksdb:                        Options.allow_mmap_reads: 0", "2018-05-30 23:38:01.245922 7ffbb727cf00  4 rocksdb:                       Options.allow_mmap_writes: 0", "2018-05-30 23:38:01.245923 7ffbb727cf00  4 rocksdb:                        Options.use_direct_reads: 0", "2018-05-30 23:38:01.245925 7ffbb727cf00  4 rocksdb:                        Options.use_direct_io_for_flush_and_compaction: 0", "2018-05-30 23:38:01.245926 7ffbb727cf00  4 rocksdb:          Options.create_missing_column_families: 0", "2018-05-30 23:38:01.245937 7ffbb727cf00  4 rocksdb:                              Options.db_log_dir: ", "2018-05-30 23:38:01.245939 7ffbb727cf00  4 rocksdb:                                 Options.wal_dir: /var/lib/ceph/mon/ceph-node/store.db", "2018-05-30 23:38:01.245941 7ffbb727cf00  4 rocksdb:                Options.table_cache_numshardbits: 6", "2018-05-30 23:38:01.245943 7ffbb727cf00  4 rocksdb:                      Options.max_subcompactions: 1", "2018-05-30 23:38:01.245944 7ffbb727cf00  4 rocksdb:                  Options.max_background_flushes: 1", "2018-05-30 23:38:01.245945 7ffbb727cf00  4 rocksdb:                         Options.WAL_ttl_seconds: 0", "2018-05-30 23:38:01.245947 7ffbb727cf00  4 rocksdb:                       Options.WAL_size_limit_MB: 0", "2018-05-30 23:38:01.245948 7ffbb727cf00  4 rocksdb:             Options.manifest_preallocation_size: 4194304", "2018-05-30 23:38:01.245950 7ffbb727cf00  4 rocksdb:                     Options.is_fd_close_on_exec: 1", "2018-05-30 23:38:01.245951 7ffbb727cf00  4 rocksdb:                   Options.advise_random_on_open: 1", "2018-05-30 23:38:01.245952 7ffbb727cf00  4 rocksdb:                    Options.db_write_buffer_size: 0", "2018-05-30 23:38:01.245954 7ffbb727cf00  4 rocksdb:         Options.access_hint_on_compaction_start: 1", "2018-05-30 23:38:01.245955 7ffbb727cf00  4 rocksdb:  Options.new_table_reader_for_compaction_inputs: 0", "2018-05-30 23:38:01.245957 7ffbb727cf00  4 rocksdb:               Options.compaction_readahead_size: 0", "2018-05-30 23:38:01.245958 7ffbb727cf00  4 rocksdb:           Options.random_access_max_buffer_size: 1048576", "2018-05-30 23:38:01.245960 7ffbb727cf00  4 rocksdb:           Options.writable_file_max_buffer_size: 1048576", "2018-05-30 23:38:01.245961 7ffbb727cf00  4 rocksdb:                      Options.use_adaptive_mutex: 0", "2018-05-30 23:38:01.245962 7ffbb727cf00  4 rocksdb:                            Options.rate_limiter: (nil)", "2018-05-30 23:38:01.245964 7ffbb727cf00  4 rocksdb:     Options.sst_file_manager.rate_bytes_per_sec: 0", "2018-05-30 23:38:01.245965 7ffbb727cf00  4 rocksdb:                          Options.bytes_per_sync: 0", "2018-05-30 23:38:01.245966 7ffbb727cf00  4 rocksdb:                      Options.wal_bytes_per_sync: 0", "2018-05-30 23:38:01.245968 7ffbb727cf00  4 rocksdb:                       Options.wal_recovery_mode: 2", "2018-05-30 23:38:01.245969 7ffbb727cf00  4 rocksdb:                  Options.enable_thread_tracking: 0", "2018-05-30 23:38:01.245971 7ffbb727cf00  4 rocksdb:         Options.allow_concurrent_memtable_write: 1", "2018-05-30 23:38:01.245972 7ffbb727cf00  4 rocksdb:      Options.enable_write_thread_adaptive_yield: 1", "2018-05-30 23:38:01.245974 7ffbb727cf00  4 rocksdb:             Options.write_thread_max_yield_usec: 100", "2018-05-30 23:38:01.245975 7ffbb727cf00  4 rocksdb:            Options.write_thread_slow_yield_usec: 3", "2018-05-30 23:38:01.245976 7ffbb727cf00  4 rocksdb:                               Options.row_cache: None", "2018-05-30 23:38:01.245978 7ffbb727cf00  4 rocksdb:                              Options.wal_filter: None", "2018-05-30 23:38:01.245979 7ffbb727cf00  4 rocksdb:             Options.avoid_flush_during_recovery: 0", "2018-05-30 23:38:01.245980 7ffbb727cf00  4 rocksdb:             Options.base_background_compactions: 1", "2018-05-30 23:38:01.245980 7ffbb727cf00  4 rocksdb:             Options.max_background_compactions: 1", "2018-05-30 23:38:01.245981 7ffbb727cf00  4 rocksdb:             Options.avoid_flush_during_shutdown: 0", "2018-05-30 23:38:01.245982 7ffbb727cf00  4 rocksdb:             Options.delayed_write_rate : 16777216", "2018-05-30 23:38:01.245983 7ffbb727cf00  4 rocksdb:             Options.max_total_wal_size: 0", "2018-05-30 23:38:01.245984 7ffbb727cf00  4 rocksdb:             Options.delete_obsolete_files_period_micros: 21600000000", "2018-05-30 23:38:01.245986 7ffbb727cf00  4 rocksdb:                   Options.stats_dump_period_sec: 600", "2018-05-30 23:38:01.245987 7ffbb727cf00  4 rocksdb: Compression algorithms supported:", "2018-05-30 23:38:01.245989 7ffbb727cf00  4 rocksdb: \tSnappy supported: 0", "2018-05-30 23:38:01.245990 7ffbb727cf00  4 rocksdb: \tZlib supported: 0", "2018-05-30 23:38:01.245992 7ffbb727cf00  4 rocksdb: \tBzip supported: 0", "2018-05-30 23:38:01.245993 7ffbb727cf00  4 rocksdb: \tLZ4 supported: 0", "2018-05-30 23:38:01.245994 7ffbb727cf00  4 rocksdb: \tZSTD supported: 0", "2018-05-30 23:38:01.245996 7ffbb727cf00  4 rocksdb: Fast CRC32 supported: 1", "2018-05-30 23:38:01.246132 7ffbb727cf00  4 rocksdb: [/build/ceph-12.2.5/src/rocksdb/db/version_set.cc:2609] Recovering from manifest file: MANIFEST-000001", "", "2018-05-30 23:38:01.246193 7ffbb727cf00  4 rocksdb: [/build/ceph-12.2.5/src/rocksdb/db/column_family.cc:407] --------------- Options for column family [default]:", "", "2018-05-30 23:38:01.246196 7ffbb727cf00  4 rocksdb:               Options.comparator: leveldb.BytewiseComparator", "2018-05-30 23:38:01.246198 7ffbb727cf00  4 rocksdb:           Options.merge_operator: ", "2018-05-30 23:38:01.246199 7ffbb727cf00  4 rocksdb:        Options.compaction_filter: None", "2018-05-30 23:38:01.246200 7ffbb727cf00  4 rocksdb:        Options.compaction_filter_factory: None", "2018-05-30 23:38:01.246201 7ffbb727cf00  4 rocksdb:         Options.memtable_factory: SkipListFactory", "2018-05-30 23:38:01.246202 7ffbb727cf00  4 rocksdb:            Options.table_factory: BlockBasedTable", "2018-05-30 23:38:01.246221 7ffbb727cf00  4 rocksdb:            table_factory options:   flush_block_policy_factory: FlushBlockBySizePolicyFactory (0x5637420700f8)", "  cache_index_and_filter_blocks: 1", "  cache_index_and_filter_blocks_with_high_priority: 1", "  pin_l0_filter_and_index_blocks_in_cache: 1", "  index_type: 0", "  hash_index_allow_collision: 1", "  checksum: 1", "  no_block_cache: 0", "  block_cache: 0x56374237c5c0", "  block_cache_name: LRUCache", "  block_cache_options:", "    capacity : 134217728", "    num_shard_bits : 4", "    strict_capacity_limit : 0", "    high_pri_pool_ratio: 0.000", "  block_cache_compressed: (nil)", "  persistent_cache: (nil)", "  block_size: 4096", "  block_size_deviation: 10", "  block_restart_interval: 16", "  index_block_restart_interval: 1", "  filter_policy: rocksdb.BuiltinBloomFilter", "  whole_key_filtering: 1", "  format_version: 2", "", "2018-05-30 23:38:01.246226 7ffbb727cf00  4 rocksdb:        Options.write_buffer_size: 33554432", "2018-05-30 23:38:01.246227 7ffbb727cf00  4 rocksdb:  Options.max_write_buffer_number: 2", "2018-05-30 23:38:01.246228 7ffbb727cf00  4 rocksdb:          Options.compression: NoCompression", "2018-05-30 23:38:01.246229 7ffbb727cf00  4 rocksdb:                  Options.bottommost_compression: Disabled", "2018-05-30 23:38:01.246229 7ffbb727cf00  4 rocksdb:       Options.prefix_extractor: nullptr", "2018-05-30 23:38:01.246230 7ffbb727cf00  4 rocksdb:   Options.memtable_insert_with_hint_prefix_extractor: nullptr", "2018-05-30 23:38:01.246231 7ffbb727cf00  4 rocksdb:             Options.num_levels: 7", "2018-05-30 23:38:01.246232 7ffbb727cf00  4 rocksdb:        Options.min_write_buffer_number_to_merge: 1", "2018-05-30 23:38:01.246232 7ffbb727cf00  4 rocksdb:     Options.max_write_buffer_number_to_maintain: 0", "2018-05-30 23:38:01.246233 7ffbb727cf00  4 rocksdb:            Options.compression_opts.window_bits: -14", "2018-05-30 23:38:01.246234 7ffbb727cf00  4 rocksdb:                  Options.compression_opts.level: -1", "2018-05-30 23:38:01.246250 7ffbb727cf00  4 rocksdb:               Options.compression_opts.strategy: 0", "2018-05-30 23:38:01.246251 7ffbb727cf00  4 rocksdb:         Options.compression_opts.max_dict_bytes: 0", "2018-05-30 23:38:01.246252 7ffbb727cf00  4 rocksdb:      Options.level0_file_num_compaction_trigger: 4", "2018-05-30 23:38:01.246252 7ffbb727cf00  4 rocksdb:          Options.level0_slowdown_writes_trigger: 20", "2018-05-30 23:38:01.246253 7ffbb727cf00  4 rocksdb:              Options.level0_stop_writes_trigger: 36", "2018-05-30 23:38:01.246253 7ffbb727cf00  4 rocksdb:                   Options.target_file_size_base: 67108864", "2018-05-30 23:38:01.246254 7ffbb727cf00  4 rocksdb:             Options.target_file_size_multiplier: 1", "2018-05-30 23:38:01.246255 7ffbb727cf00  4 rocksdb:                Options.max_bytes_for_level_base: 268435456", "2018-05-30 23:38:01.246256 7ffbb727cf00  4 rocksdb: Options.level_compaction_dynamic_level_bytes: 0", "2018-05-30 23:38:01.246256 7ffbb727cf00  4 rocksdb:          Options.max_bytes_for_level_multiplier: 10.000000", "2018-05-30 23:38:01.246259 7ffbb727cf00  4 rocksdb: Options.max_bytes_for_level_multiplier_addtl[0]: 1", "2018-05-30 23:38:01.246260 7ffbb727cf00  4 rocksdb: Options.max_bytes_for_level_multiplier_addtl[1]: 1", "2018-05-30 23:38:01.246261 7ffbb727cf00  4 rocksdb: Options.max_bytes_for_level_multiplier_addtl[2]: 1", "2018-05-30 23:38:01.246261 7ffbb727cf00  4 rocksdb: Options.max_bytes_for_level_multiplier_addtl[3]: 1", "2018-05-30 23:38:01.246262 7ffbb727cf00  4 rocksdb: Options.max_bytes_for_level_multiplier_addtl[4]: 1", "2018-05-30 23:38:01.246263 7ffbb727cf00  4 rocksdb: Options.max_bytes_for_level_multiplier_addtl[5]: 1", "2018-05-30 23:38:01.246263 7ffbb727cf00  4 rocksdb: Options.max_bytes_for_level_multiplier_addtl[6]: 1", "2018-05-30 23:38:01.246264 7ffbb727cf00  4 rocksdb:       Options.max_sequential_skip_in_iterations: 8", "2018-05-30 23:38:01.246265 7ffbb727cf00  4 rocksdb:                    Options.max_compaction_bytes: 1677721600", "2018-05-30 23:38:01.246266 7ffbb727cf00  4 rocksdb:                        Options.arena_block_size: 4194304", "2018-05-30 23:38:01.246267 7ffbb727cf00  4 rocksdb:   Options.soft_pending_compaction_bytes_limit: 68719476736", "2018-05-30 23:38:01.246267 7ffbb727cf00  4 rocksdb:   Options.hard_pending_compaction_bytes_limit: 274877906944", "2018-05-30 23:38:01.246268 7ffbb727cf00  4 rocksdb:       Options.rate_limit_delay_max_milliseconds: 100", "2018-05-30 23:38:01.246269 7ffbb727cf00  4 rocksdb:                Options.disable_auto_compactions: 0", "2018-05-30 23:38:01.246270 7ffbb727cf00  4 rocksdb:                         Options.compaction_style: kCompactionStyleLevel", "2018-05-30 23:38:01.246271 7ffbb727cf00  4 rocksdb:                           Options.compaction_pri: kByCompensatedSize", "2018-05-30 23:38:01.246272 7ffbb727cf00  4 rocksdb:  Options.compaction_options_universal.size_ratio: 1", "2018-05-30 23:38:01.246273 7ffbb727cf00  4 rocksdb: Options.compaction_options_universal.min_merge_width: 2", "2018-05-30 23:38:01.246273 7ffbb727cf00  4 rocksdb: Options.compaction_options_universal.max_merge_width: 4294967295", "2018-05-30 23:38:01.246274 7ffbb727cf00  4 rocksdb: Options.compaction_options_universal.max_size_amplification_percent: 200", "2018-05-30 23:38:01.246275 7ffbb727cf00  4 rocksdb: Options.compaction_options_universal.compression_size_percent: -1", "2018-05-30 23:38:01.246276 7ffbb727cf00  4 rocksdb: Options.compaction_options_fifo.max_table_files_size: 1073741824", "2018-05-30 23:38:01.246277 7ffbb727cf00  4 rocksdb:                   Options.table_properties_collectors: ", "2018-05-30 23:38:01.246277 7ffbb727cf00  4 rocksdb:                   Options.inplace_update_support: 0", "2018-05-30 23:38:01.246278 7ffbb727cf00  4 rocksdb:                 Options.inplace_update_num_locks: 10000", "2018-05-30 23:38:01.246279 7ffbb727cf00  4 rocksdb:               Options.memtable_prefix_bloom_size_ratio: 0.000000", "2018-05-30 23:38:01.246280 7ffbb727cf00  4 rocksdb:   Options.memtable_huge_page_size: 0", "2018-05-30 23:38:01.246281 7ffbb727cf00  4 rocksdb:                           Options.bloom_locality: 0", "2018-05-30 23:38:01.246282 7ffbb727cf00  4 rocksdb:                    Options.max_successive_merges: 0", "2018-05-30 23:38:01.246282 7ffbb727cf00  4 rocksdb:                Options.optimize_filters_for_hits: 0", "2018-05-30 23:38:01.246283 7ffbb727cf00  4 rocksdb:                Options.paranoid_file_checks: 0", "2018-05-30 23:38:01.246284 7ffbb727cf00  4 rocksdb:                Options.force_consistency_checks: 0", "2018-05-30 23:38:01.246285 7ffbb727cf00  4 rocksdb:                Options.report_bg_io_stats: 0", "2018-05-30 23:38:01.246948 7ffbb727cf00  4 rocksdb: [/build/ceph-12.2.5/src/rocksdb/db/version_set.cc:2859] Recovered from manifest file:/var/lib/ceph/mon/ceph-node/store.db/MANIFEST-000001 succeeded,manifest_file_number is 1, next_file_number is 3, last_sequence is 0, log_number is 0,prev_log_number is 0,max_column_family is 0", "", "2018-05-30 23:38:01.246958 7ffbb727cf00  4 rocksdb: [/build/ceph-12.2.5/src/rocksdb/db/version_set.cc:2867] Column family [default] (ID 0), log number is 0", "", "2018-05-30 23:38:01.247019 7ffbb727cf00  4 rocksdb: EVENT_LOG_v1 {\"time_micros\": 1527723481247008, \"job\": 1, \"event\": \"recovery_started\", \"log_files\": [3]}", "2018-05-30 23:38:01.247025 7ffbb727cf00  4 rocksdb: [/build/ceph-12.2.5/src/rocksdb/db/db_impl_open.cc:482] Recovering log #3 mode 2", "2018-05-30 23:38:01.252177 7ffbb727cf00  4 rocksdb: EVENT_LOG_v1 {\"time_micros\": 1527723481252159, \"cf_name\": \"default\", \"job\": 1, \"event\": \"table_file_creation\", \"file_number\": 4, \"file_size\": 2053, \"table_properties\": {\"data_size\": 1093, \"index_size\": 28, \"filter_size\": 23, \"raw_key_size\": 115, \"raw_average_key_size\": 23, \"raw_value_size\": 970, \"raw_average_value_size\": 194, \"num_data_blocks\": 1, \"num_entries\": 5, \"filter_policy_name\": \"rocksdb.BuiltinBloomFilter\", \"kDeletedKeys\": \"0\", \"kMergeOperands\": \"0\"}}", "2018-05-30 23:38:01.252225 7ffbb727cf00  4 rocksdb: [/build/ceph-12.2.5/src/rocksdb/db/version_set.cc:2395] Creating manifest 5", "", "2018-05-30 23:38:01.260437 7ffbb727cf00  4 rocksdb: EVENT_LOG_v1 {\"time_micros\": 1527723481260430, \"job\": 1, \"event\": \"recovery_finished\"}", "2018-05-30 23:38:01.269399 7ffbb727cf00  4 rocksdb: [/build/ceph-12.2.5/src/rocksdb/db/db_impl_open.cc:1063] DB pointer 0x563742490000", "2018-05-30 23:38:01.270854 7ffbb727cf00  0 starting mon.node rank 0 at public addr 128.110.153.197:6789/0 at bind addr 128.110.153.197:6789/0 mon_data /var/lib/ceph/mon/ceph-node fsid b2bb6c0b-6cc6-44f1-bf92-1ce81d78fbdc", "2018-05-30 23:38:01.271080 7ffbb727cf00  0 starting mon.node rank 0 at 128.110.153.197:6789/0 mon_data /var/lib/ceph/mon/ceph-node fsid b2bb6c0b-6cc6-44f1-bf92-1ce81d78fbdc", "2018-05-30 23:38:01.272715 7ffbb727cf00  1 mon.node@-1(probing) e0 preinit fsid b2bb6c0b-6cc6-44f1-bf92-1ce81d78fbdc", "2018-05-30 23:38:01.272917 7ffbb727cf00  1 mon.node@-1(probing).mds e0 Unable to load 'last_metadata'", "2018-05-30 23:38:01.274622 7ffbb727cf00  0 mon.node@-1(probing) e0  my rank is now 0 (was -1)", "2018-05-30 23:38:01.274660 7ffbb727cf00  1 mon.node@0(probing) e0 win_standalone_election", "2018-05-30 23:38:01.274682 7ffbb727cf00  1 mon.node@0(probing).elector(0) init, first boot, initializing epoch at 1 ", "2018-05-30 23:38:01.283967 7ffbb727cf00  0 log_channel(cluster) log [INF] : mon.node is new leader, mons node in quorum (ranks 0)", "2018-05-30 23:38:01.288393 7ffbb727cf00  1 mon.node@0(leader).osd e0 create_pending setting backfillfull_ratio = 0.9", "2018-05-30 23:38:01.288426 7ffbb727cf00  1 mon.node@0(leader).osd e0 create_pending setting full_ratio = 0.95", "2018-05-30 23:38:01.288432 7ffbb727cf00  1 mon.node@0(leader).osd e0 create_pending setting nearfull_ratio = 0.85", "2018-05-30 23:38:01.288754 7ffbb727cf00  1 mon.node@0(leader).osd e0 encode_pending skipping prime_pg_temp; mapping job did not start", "2018-05-30 23:38:01.290382 7ffbb727cf00  1 mon.node@0(leader) e0 _apply_compatset_features enabling new quorum features: compat={},rocompat={},incompat={4=support erasure code pools,5=new-style osdmap encoding,6=support isa/lrc erasure code,7=support shec erasure code}", "2018-05-30 23:38:01.296805 7ffba9886700  1 mon.node@0(leader).paxosservice(pgmap 0..0) refresh upgraded, format 1 -> 0", "2018-05-30 23:38:01.296825 7ffba9886700  1 mon.node@0(leader).pg v0 on_upgrade discarding in-core PGMap", "2018-05-30 23:38:01.301403 7ffba9886700  1 mon.node@0(leader).paxosservice(auth 0..0) refresh upgraded, format 2 -> 0", "2018-05-30 23:38:01.301716 7ffba9886700  1 mon.node@0(probing) e1 win_standalone_election", "2018-05-30 23:38:01.301738 7ffba9886700  1 mon.node@0(probing).elector(2) init, last seen epoch 2", "2018-05-30 23:38:01.305742 7ffba9886700  0 log_channel(cluster) log [INF] : mon.node is new leader, mons node in quorum (ranks 0)", "2018-05-30 23:38:01.309672 7ffba9886700  0 log_channel(cluster) log [DBG] : monmap e1: 1 mons at {node=128.110.153.197:6789/0}", "2018-05-30 23:38:01.313774 7ffba9886700  1 mon.node@0(leader).osd e0 create_pending setting backfillfull_ratio = 0.9", "2018-05-30 23:38:01.313784 7ffba9886700  1 mon.node@0(leader).osd e0 create_pending setting full_ratio = 0.95", "2018-05-30 23:38:01.313787 7ffba9886700  1 mon.node@0(leader).osd e0 create_pending setting nearfull_ratio = 0.85", "2018-05-30 23:38:01.313904 7ffba9886700  1 mon.node@0(leader).osd e0 encode_pending skipping prime_pg_temp; mapping job did not start", "2018-05-30 23:38:01.314492 7ffba9886700  1 mon.node@0(leader) e1 _apply_compatset_features enabling new quorum features: compat={},rocompat={},incompat={8=support monmap features,9=luminous ondisk layout}", "2018-05-30 23:38:01.322998 7ffba9886700  1 mon.node@0(leader).paxosservice(auth 0..0) refresh upgraded, format 2 -> 0", "2018-05-30 23:38:01.323115 7ffba9886700  0 log_channel(cluster) log [INF] : pgmap 0 pgs: ; 0 bytes data, 0 kB used, 0 kB / 0 kB avail", "2018-05-30 23:38:01.332668 7ffba9886700  0 mon.node@0(leader).mds e1 print_map", "e1", "enable_multiple, ever_enabled_multiple: 0,0", "compat: compat={},rocompat={},incompat={1=base v0.20,2=client writeable ranges,3=default file layouts on dirs,4=dir inode in separate object,5=mds uses versioned encoding,6=dirfrag is stored in omap,8=no anchor table,9=file layout v2}", "legacy client fscid: -1", " ", "No filesystems configured", "", "2018-05-30 23:38:01.332844 7ffba9886700  1 mon.node@0(leader).osd e1 e1: 0 total, 0 up, 0 in", "2018-05-30 23:38:01.337177 7ffba9886700  0 mon.node@0(leader).osd e1 crush map has features 288514050185494528, adjusting msgr requires", "2018-05-30 23:38:01.337181 7ffba9886700  0 mon.node@0(leader).osd e1 crush map has features 288514050185494528, adjusting msgr requires", "2018-05-30 23:38:01.337184 7ffba9886700  0 mon.node@0(leader).osd e1 crush map has features 1009089990564790272, adjusting msgr requires", "2018-05-30 23:38:01.337185 7ffba9886700  0 mon.node@0(leader).osd e1 crush map has features 288514050185494528, adjusting msgr requires", "2018-05-30 23:38:01.337335 7ffba9886700  1 mon.node@0(leader).log v1 unable to write to '/var/log/ceph/ceph.log' for channel 'cluster': (2) No such file or directory", "2018-05-30 23:38:01.337402 7ffba9886700  1 mon.node@0(leader).paxosservice(auth 1..1) refresh upgraded, format 0 -> 2", "2018-05-30 23:38:01.342206 7ffba9886700  0 log_channel(cluster) log [DBG] : fsmap ", "2018-05-30 23:38:01.342329 7ffba9886700  1 mon.node@0(leader).pg v1 check_osd_map will clear pg_map data", "2018-05-30 23:38:01.342334 7ffba9886700  1 mon.node@0(leader).pg v1 encode_pending clearing pgmap data at v2", "2018-05-30 23:38:01.346257 7ffba9886700  0 log_channel(cluster) log [DBG] : osdmap e1: 0 total, 0 up, 0 in", "2018-05-30 23:38:01.346508 7ffba9886700  0 log_channel(cluster) log [DBG] : mgrmap e1: no daemons active", "2018-05-30 23:38:01.358592 7ffba9886700  1 mon.node@0(leader).log v2 unable to write to '/var/log/ceph/ceph.log' for channel 'cluster': (2) No such file or directory", "2018-05-30 23:38:02.367975 7ffba9886700  1 mon.node@0(leader).log v3 unable to write to '/var/log/ceph/ceph.log' for channel 'cluster': (2) No such file or directory", "2018-05-30 23:39:01.273324 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12384 MB", "2018-05-30 23:39:56.181642 7ffbae08f700  0 mon.node@0(leader) e1 handle_command mon_command({\"prefix\": \"status\"} v 0) v1", "2018-05-30 23:39:56.181688 7ffbae08f700  0 log_channel(audit) log [DBG] : from='client.? 128.110.153.197:0/1393663213' entity='client.admin' cmd=[{\"prefix\": \"status\"}]: dispatch", "2018-05-30 23:40:01.273746 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12384 MB", "2018-05-30 23:41:01.274140 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12384 MB", "2018-05-30 23:42:01.274534 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12384 MB", "2018-05-30 23:43:01.274958 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12384 MB", "2018-05-30 23:44:01.275346 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12384 MB", "2018-05-30 23:45:01.275748 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12384 MB", "2018-05-30 23:46:01.276160 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12384 MB", "2018-05-30 23:47:01.276549 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12384 MB", "2018-05-30 23:48:01.276939 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12384 MB", "2018-05-30 23:49:01.277379 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12384 MB", "2018-05-30 23:50:01.277802 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12384 MB", "2018-05-30 23:51:01.278223 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12384 MB", "2018-05-30 23:52:01.278588 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12384 MB", "2018-05-30 23:53:01.278994 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12384 MB", "2018-05-30 23:54:01.279365 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12384 MB", "2018-05-30 23:55:01.279768 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12384 MB", "2018-05-30 23:56:01.280148 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12384 MB", "2018-05-30 23:57:01.280583 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12383 MB", "2018-05-30 23:58:01.281014 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12383 MB", "2018-05-30 23:59:01.281412 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12383 MB", "2018-05-31 00:00:01.281798 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12383 MB", "2018-05-31 00:01:01.282198 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12383 MB", "2018-05-31 00:02:01.282589 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12383 MB", "2018-05-31 00:03:01.283031 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12383 MB", "2018-05-31 00:04:01.283496 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12383 MB", "2018-05-31 00:05:01.283889 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12383 MB", "2018-05-31 00:06:01.284282 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12383 MB", "2018-05-31 00:07:01.284677 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12383 MB", "2018-05-31 00:08:01.285100 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12383 MB", "2018-05-31 00:09:01.285480 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12383 MB", "2018-05-31 00:10:01.285898 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12383 MB", "2018-05-31 00:11:01.286271 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12383 MB", "2018-05-31 00:12:01.286677 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12383 MB", "2018-05-31 00:13:01.287060 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12383 MB", "2018-05-31 00:14:01.287548 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12383 MB", "2018-05-31 00:15:01.287959 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12383 MB", "2018-05-31 00:16:01.288332 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12383 MB", "2018-05-31 00:17:01.288709 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12383 MB", "2018-05-31 00:18:01.289123 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12383 MB", "2018-05-31 00:19:01.289504 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12383 MB", "2018-05-31 00:20:01.289866 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12383 MB", "2018-05-31 00:21:01.290273 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12383 MB", "2018-05-31 00:22:01.290651 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12383 MB", "2018-05-31 00:23:01.291032 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12383 MB", "2018-05-31 00:24:01.291450 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12383 MB", "2018-05-31 00:25:01.291833 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12383 MB", "2018-05-31 00:26:01.292226 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12383 MB", "2018-05-31 00:27:01.292614 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12383 MB", "2018-05-31 00:28:01.293028 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12383 MB", "2018-05-31 00:29:01.293459 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12383 MB", "2018-05-31 00:30:01.293893 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12383 MB", "2018-05-31 00:31:01.294304 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12383 MB", "2018-05-31 00:32:01.294721 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12383 MB", "2018-05-31 00:33:01.295104 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12383 MB", "2018-05-31 00:34:01.295519 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12383 MB", "2018-05-31 00:35:01.295900 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12383 MB", "2018-05-31 00:36:01.296312 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12383 MB", "2018-05-31 00:37:01.296697 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12383 MB", "2018-05-31 00:38:01.297150 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12383 MB", "2018-05-31 00:39:01.297526 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12383 MB", "2018-05-31 00:40:01.297906 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12383 MB", "2018-05-31 00:41:01.298351 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12383 MB", "2018-05-31 00:42:01.298731 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12383 MB", "2018-05-31 00:43:01.299151 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12383 MB", "2018-05-31 00:44:01.299600 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12383 MB", "2018-05-31 00:45:01.300016 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12383 MB", "2018-05-31 00:46:01.300408 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12383 MB", "2018-05-31 00:47:01.300807 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12383 MB", "2018-05-31 00:48:01.301188 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12383 MB", "2018-05-31 00:49:01.301565 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12383 MB", "2018-05-31 00:50:01.301991 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12383 MB", "2018-05-31 00:51:01.302371 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12383 MB", "2018-05-31 00:52:01.302788 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12383 MB", "2018-05-31 00:53:01.303213 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12383 MB", "2018-05-31 00:54:01.303671 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12383 MB", "2018-05-31 00:55:01.304091 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12383 MB", "2018-05-31 00:56:01.304481 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12383 MB", "2018-05-31 00:57:01.304856 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12383 MB", "2018-05-31 00:58:01.305238 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12383 MB", "2018-05-31 00:59:01.305651 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12383 MB", "2018-05-31 01:00:01.306026 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12383 MB", "2018-05-31 01:01:01.306473 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12383 MB", "2018-05-31 01:02:01.306856 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12383 MB", "2018-05-31 01:03:01.307336 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12383 MB", "2018-05-31 01:04:01.307757 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12383 MB", "2018-05-31 01:05:01.308178 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12383 MB", "2018-05-31 01:06:01.308555 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12383 MB", "2018-05-31 01:07:01.308944 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12383 MB", "2018-05-31 01:08:01.309360 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12383 MB", "2018-05-31 01:09:01.309736 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12383 MB", "2018-05-31 01:10:01.310182 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12383 MB", "2018-05-31 01:11:01.310563 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12383 MB", "2018-05-31 01:12:01.310978 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12383 MB", "2018-05-31 01:13:01.311355 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12383 MB", "2018-05-31 01:14:01.311765 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12383 MB", "2018-05-31 01:15:01.312145 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12383 MB", "2018-05-31 01:16:01.312594 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2795 MB, avail 12383 MB", "2018-05-31 01:17:01.312974 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2796 MB, avail 12383 MB", "2018-05-31 01:18:01.313403 7ffbb0894700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2796 MB, avail 12383 MB", "2018-05-31 01:18:58.052535 7ffbab88a700 -1 received  signal: Terminated from  PID: 1 task name:  UID: 0", "2018-05-31 01:18:58.052567 7ffbab88a700 -1 mon.node@0(leader) e1 *** Got Signal Terminated ***", "2018-05-31 01:18:58.052573 7ffbab88a700  1 mon.node@0(leader) e1 shutdown"], "stdout": "creating /var/lib/ceph/bootstrap-osd/ceph.keyring\ncreating /var/lib/ceph/bootstrap-mds/ceph.keyring\ncreating /var/lib/ceph/bootstrap-rgw/ceph.keyring\ncreating /var/lib/ceph/bootstrap-rbd/ceph.keyring\nmonmaptool: monmap file /etc/ceph/monmap-ceph\nmonmaptool: set fsid to b2bb6c0b-6cc6-44f1-bf92-1ce81d78fbdc\nmonmaptool: writing epoch 0 to /etc/ceph/monmap-ceph (1 monitors)\nimporting contents of /var/lib/ceph/bootstrap-osd/ceph.keyring into /etc/ceph/ceph.mon.keyring\nimporting contents of /var/lib/ceph/bootstrap-mds/ceph.keyring into /etc/ceph/ceph.mon.keyring\nimporting contents of /var/lib/ceph/bootstrap-rgw/ceph.keyring into /etc/ceph/ceph.mon.keyring\nimporting contents of /var/lib/ceph/bootstrap-rbd/ceph.keyring into /etc/ceph/ceph.mon.keyring\nimporting contents of /etc/ceph/ceph.client.admin.keyring into /etc/ceph/ceph.mon.keyring\n2018-05-30 23:38:01  /entrypoint.sh: SUCCESS\nexec: PID 123: spawning /usr/bin/ceph-mon --cluster ceph --setuser ceph --setgroup ceph -d -i node --mon-data /var/lib/ceph/mon/ceph-node --public-addr 128.110.153.197:6789\nSending SIGTERM to PID 123", "stdout_lines": ["creating /var/lib/ceph/bootstrap-osd/ceph.keyring", "creating /var/lib/ceph/bootstrap-mds/ceph.keyring", "creating /var/lib/ceph/bootstrap-rgw/ceph.keyring", "creating /var/lib/ceph/bootstrap-rbd/ceph.keyring", "monmaptool: monmap file /etc/ceph/monmap-ceph", "monmaptool: set fsid to b2bb6c0b-6cc6-44f1-bf92-1ce81d78fbdc", "monmaptool: writing epoch 0 to /etc/ceph/monmap-ceph (1 monitors)", "importing contents of /var/lib/ceph/bootstrap-osd/ceph.keyring into /etc/ceph/ceph.mon.keyring", "importing contents of /var/lib/ceph/bootstrap-mds/ceph.keyring into /etc/ceph/ceph.mon.keyring", "importing contents of /var/lib/ceph/bootstrap-rgw/ceph.keyring into /etc/ceph/ceph.mon.keyring", "importing contents of /var/lib/ceph/bootstrap-rbd/ceph.keyring into /etc/ceph/ceph.mon.keyring", "importing contents of /etc/ceph/ceph.client.admin.keyring into /etc/ceph/ceph.mon.keyring", "2018-05-30 23:38:01  /entrypoint.sh: SUCCESS", "exec: PID 123: spawning /usr/bin/ceph-mon --cluster ceph --setuser ceph --setgroup ceph -d -i node --mon-data /var/lib/ceph/mon/ceph-node --public-addr 128.110.153.197:6789", "Sending SIGTERM to PID 123"]}
2018-05-31 01:19:11,065 p=10 u=root |  ...ignoring
2018-05-31 01:19:11,076 p=10 u=root |  TASK [baseliner : set fact to store result of bench execution] *****************
2018-05-31 01:19:11,076 p=10 u=root |  Thursday 31 May 2018  01:19:11 +0000 (0:00:00.995)       1:41:27.783 ********** 
2018-05-31 01:19:11,097 p=10 u=root |  ok: [node1 -> localhost]
2018-05-31 01:19:11,108 p=10 u=root |  TASK [baseliner : record elapsed time in output folder] ************************
2018-05-31 01:19:11,108 p=10 u=root |  Thursday 31 May 2018  01:19:11 +0000 (0:00:00.031)       1:41:27.815 ********** 
2018-05-31 01:19:11,349 p=10 u=root |  changed: [node1 -> localhost]
2018-05-31 01:19:11,353 p=10 u=root |  changed: [node2 -> localhost]
2018-05-31 01:19:11,353 p=10 u=root |  changed: [node3 -> localhost]
2018-05-31 01:19:11,363 p=10 u=root |  TASK [baseliner : capture stdout] **********************************************
2018-05-31 01:19:11,363 p=10 u=root |  Thursday 31 May 2018  01:19:11 +0000 (0:00:00.254)       1:41:28.069 ********** 
2018-05-31 01:19:11,597 p=10 u=root |  changed: [node1 -> localhost]
2018-05-31 01:19:11,603 p=10 u=root |  changed: [node2 -> localhost]
2018-05-31 01:19:11,610 p=10 u=root |  changed: [node3 -> localhost]
2018-05-31 01:19:11,621 p=10 u=root |  TASK [baseliner : capture stderr] **********************************************
2018-05-31 01:19:11,621 p=10 u=root |  Thursday 31 May 2018  01:19:11 +0000 (0:00:00.258)       1:41:28.328 ********** 
2018-05-31 01:19:11,845 p=10 u=root |  changed: [node2 -> localhost]
2018-05-31 01:19:11,852 p=10 u=root |  changed: [node1 -> localhost]
2018-05-31 01:19:11,885 p=10 u=root |  changed: [node3 -> localhost]
2018-05-31 01:19:11,892 p=10 u=root |  TASK [baseliner : debug] *******************************************************
2018-05-31 01:19:11,892 p=10 u=root |  Thursday 31 May 2018  01:19:11 +0000 (0:00:00.270)       1:41:28.599 ********** 
2018-05-31 01:19:11,931 p=10 u=root |  TASK [baseliner : run compose benchmark] ***************************************
2018-05-31 01:19:11,932 p=10 u=root |  Thursday 31 May 2018  01:19:11 +0000 (0:00:00.039)       1:41:28.638 ********** 
2018-05-31 01:19:11,974 p=10 u=root |  TASK [baseliner : run script benchmark] ****************************************
2018-05-31 01:19:11,975 p=10 u=root |  Thursday 31 May 2018  01:19:11 +0000 (0:00:00.042)       1:41:28.681 ********** 
2018-05-31 01:19:12,015 p=10 u=root |  TASK [baseliner : download results] ********************************************
2018-05-31 01:19:12,015 p=10 u=root |  Thursday 31 May 2018  01:19:12 +0000 (0:00:00.040)       1:41:28.721 ********** 
2018-05-31 01:19:12,082 p=10 u=root |  included: /etc/ansible/roles/baseliner/tasks/download-output.yml for node1, node2, node3
2018-05-31 01:19:12,179 p=10 u=root |  TASK [baseliner : get any bench-specific files] ********************************
2018-05-31 01:19:12,179 p=10 u=root |  Thursday 31 May 2018  01:19:12 +0000 (0:00:00.164)       1:41:28.886 ********** 
2018-05-31 01:19:13,553 p=10 u=root |  changed: [node1] => (item=/tmp/baseliner_output_node1)
2018-05-31 01:19:13,555 p=10 u=root |  changed: [node2] => (item=/tmp/baseliner_output_node2)
2018-05-31 01:19:13,565 p=10 u=root |  changed: [node3] => (item=/tmp/baseliner_output_node3)
2018-05-31 01:19:13,572 p=10 u=root |  TASK [baseliner : delete remote folder after transfer] *************************
2018-05-31 01:19:13,573 p=10 u=root |  Thursday 31 May 2018  01:19:13 +0000 (0:00:01.393)       1:41:30.279 ********** 
2018-05-31 01:19:14,429 p=10 u=root |  changed: [node3] => (item=/tmp/baseliner_output_node3)
2018-05-31 01:19:14,430 p=10 u=root |  changed: [node2] => (item=/tmp/baseliner_output_node2)
2018-05-31 01:19:14,431 p=10 u=root |  changed: [node1] => (item=/tmp/baseliner_output_node1)
2018-05-31 01:19:14,438 p=10 u=root |  TASK [baseliner : check if we should fail fast] ********************************
2018-05-31 01:19:14,438 p=10 u=root |  Thursday 31 May 2018  01:19:14 +0000 (0:00:00.865)       1:41:31.145 ********** 
2018-05-31 01:19:14,494 p=10 u=root |  included: /etc/ansible/roles/baseliner/tasks/fail-fast.yml for node1, node2, node3
2018-05-31 01:19:14,599 p=10 u=root |  TASK [baseliner : failfast for single-node mode] *******************************
2018-05-31 01:19:14,599 p=10 u=root |  Thursday 31 May 2018  01:19:14 +0000 (0:00:00.160)       1:41:31.305 ********** 
2018-05-31 01:19:14,663 p=10 u=root |  fatal: [node1]: FAILED! => {"changed": false, "failed": true, "msg": "benchmark failed"}
2018-05-31 01:19:14,664 p=10 u=root |  NO MORE HOSTS LEFT *************************************************************
2018-05-31 01:19:14,664 p=10 u=root |  PLAY RECAP *********************************************************************
2018-05-31 01:19:14,664 p=10 u=root |  node1                      : ok=50   changed=17   unreachable=0    failed=1   
2018-05-31 01:19:14,665 p=10 u=root |  node2                      : ok=43   changed=14   unreachable=0    failed=0   
2018-05-31 01:19:14,665 p=10 u=root |  node3                      : ok=43   changed=14   unreachable=0    failed=0   
2018-05-31 01:19:14,665 p=10 u=root |  Thursday 31 May 2018  01:19:14 +0000 (0:00:00.065)       1:41:31.371 ********** 
2018-05-31 01:19:14,665 p=10 u=root |  =============================================================================== 
2018-05-31 01:19:14,665 p=10 u=root |  baseliner : wait for containers in single-node mode and stop/kill them if they timeout  6068.67s
2018-05-31 01:19:14,665 p=10 u=root |  Gathering Facts --------------------------------------------------------- 2.92s
2018-05-31 01:19:14,665 p=10 u=root |  baseliner : run container ----------------------------------------------- 2.03s
2018-05-31 01:19:14,666 p=10 u=root |  baseliner : pull image -------------------------------------------------- 1.68s
2018-05-31 01:19:14,666 p=10 u=root |  baseliner : install facter ---------------------------------------------- 1.45s
2018-05-31 01:19:14,666 p=10 u=root |  baseliner : upload files ------------------------------------------------ 1.44s
2018-05-31 01:19:14,666 p=10 u=root |  baseliner : get any bench-specific files -------------------------------- 1.39s
2018-05-31 01:19:14,666 p=10 u=root |  baseliner : get result of container execution --------------------------- 1.00s
2018-05-31 01:19:14,666 p=10 u=root |  baseliner : stop any running container ---------------------------------- 0.99s
2018-05-31 01:19:14,666 p=10 u=root |  baseliner : check if image already exists ------------------------------- 0.90s
2018-05-31 01:19:14,666 p=10 u=root |  baseliner : remove remote results folder -------------------------------- 0.87s
2018-05-31 01:19:14,666 p=10 u=root |  baseliner : delete remote folder after transfer ------------------------- 0.87s
2018-05-31 01:19:14,666 p=10 u=root |  baseliner : remove containers to avoid name clashes --------------------- 0.85s
2018-05-31 01:19:14,666 p=10 u=root |  baseliner : create remote results folder -------------------------------- 0.81s
2018-05-31 01:19:14,666 p=10 u=root |  baseliner : store facts about remotes ----------------------------------- 0.76s
2018-05-31 01:19:14,667 p=10 u=root |  baseliner : capture stderr ---------------------------------------------- 0.27s
2018-05-31 01:19:14,667 p=10 u=root |  baseliner : capture stdout ---------------------------------------------- 0.26s
2018-05-31 01:19:14,667 p=10 u=root |  baseliner : record elapsed time in output folder ------------------------ 0.25s
2018-05-31 01:19:14,667 p=10 u=root |  baseliner : ensure results folder exists -------------------------------- 0.24s
2018-05-31 01:19:14,667 p=10 u=root |  baseliner : add host-specific environment to docker_flags --------------- 0.19s
2018-05-31 01:19:14,667 p=10 u=root |  Playbook run took 0 days, 1 hours, 41 minutes, 31 seconds
