
2018-06-05 17:13:52,778 p=11 u=root |  PLAY [all] *********************************************************************
2018-06-05 17:13:52,789 p=11 u=root |  TASK [Gathering Facts] *********************************************************
2018-06-05 17:13:52,789 p=11 u=root |  Tuesday 05 June 2018  17:13:52 +0000 (0:00:00.057)       0:00:00.057 ********** 
2018-06-05 17:13:55,735 p=11 u=root |  ok: [node2]
2018-06-05 17:13:55,742 p=11 u=root |  ok: [node3]
2018-06-05 17:13:55,785 p=11 u=root |  ok: [node1]
2018-06-05 17:13:55,793 p=11 u=root |  TASK [baseliner : include_tasks] ***********************************************
2018-06-05 17:13:55,793 p=11 u=root |  Tuesday 05 June 2018  17:13:55 +0000 (0:00:03.003)       0:00:03.061 ********** 
2018-06-05 17:13:55,862 p=11 u=root |  included: /etc/ansible/roles/baseliner/tasks/check_preconditions.yml for node1, node2, node3
2018-06-05 17:13:55,872 p=11 u=root |  TASK [baseliner : ensure expected variables are defined] ***********************
2018-06-05 17:13:55,872 p=11 u=root |  Tuesday 05 June 2018  17:13:55 +0000 (0:00:00.079)       0:00:03.141 ********** 
2018-06-05 17:13:55,932 p=11 u=root |  ok: [node1] => {
    "changed": false, 
    "failed": false, 
    "msg": "All assertions passed"
}
2018-06-05 17:13:55,939 p=11 u=root |  ok: [node2] => {
    "changed": false, 
    "failed": false, 
    "msg": "All assertions passed"
}
2018-06-05 17:13:55,947 p=11 u=root |  ok: [node3] => {
    "changed": false, 
    "failed": false, 
    "msg": "All assertions passed"
}
2018-06-05 17:13:55,954 p=11 u=root |  TASK [baseliner : set remote results path if not defined] **********************
2018-06-05 17:13:55,954 p=11 u=root |  Tuesday 05 June 2018  17:13:55 +0000 (0:00:00.081)       0:00:03.222 ********** 
2018-06-05 17:13:56,009 p=11 u=root |  ok: [node1]
2018-06-05 17:13:56,039 p=11 u=root |  ok: [node2]
2018-06-05 17:13:56,049 p=11 u=root |  ok: [node3]
2018-06-05 17:13:56,058 p=11 u=root |  TASK [baseliner : ensure results folder exists] ********************************
2018-06-05 17:13:56,058 p=11 u=root |  Tuesday 05 June 2018  17:13:56 +0000 (0:00:00.103)       0:00:03.326 ********** 
2018-06-05 17:13:56,292 p=11 u=root |  ok: [node1 -> localhost]
2018-06-05 17:13:56,303 p=11 u=root |  TASK [baseliner : check that pre-tasks file exists] ****************************
2018-06-05 17:13:56,303 p=11 u=root |  Tuesday 05 June 2018  17:13:56 +0000 (0:00:00.245)       0:00:03.572 ********** 
2018-06-05 17:13:56,325 p=11 u=root |  TASK [baseliner : check that post-tasks file exists] ***************************
2018-06-05 17:13:56,326 p=11 u=root |  Tuesday 05 June 2018  17:13:56 +0000 (0:00:00.022)       0:00:03.594 ********** 
2018-06-05 17:13:56,366 p=11 u=root |  TASK [baseliner : ensure docker-engine is installed] ***************************
2018-06-05 17:13:56,367 p=11 u=root |  Tuesday 05 June 2018  17:13:56 +0000 (0:00:00.041)       0:00:03.635 ********** 
2018-06-05 17:13:56,403 p=11 u=root |  TASK [baseliner : install statically linked docker] ****************************
2018-06-05 17:13:56,403 p=11 u=root |  Tuesday 05 June 2018  17:13:56 +0000 (0:00:00.036)       0:00:03.671 ********** 
2018-06-05 17:13:56,442 p=11 u=root |  TASK [baseliner : ensure the docker daemon is running] *************************
2018-06-05 17:13:56,442 p=11 u=root |  Tuesday 05 June 2018  17:13:56 +0000 (0:00:00.039)       0:00:03.711 ********** 
2018-06-05 17:13:56,475 p=11 u=root |  TASK [baseliner : stop any running container] **********************************
2018-06-05 17:13:56,476 p=11 u=root |  Tuesday 05 June 2018  17:13:56 +0000 (0:00:00.033)       0:00:03.744 ********** 
2018-06-05 17:13:57,456 p=11 u=root |  fatal: [node2]: FAILED! => {"changed": true, "cmd": "docker stop `docker ps -q`", "delta": "0:00:00.033624", "end": "2018-06-05 11:13:57.547569", "failed": true, "msg": "non-zero return code", "rc": 1, "start": "2018-06-05 11:13:57.513945", "stderr": "\"docker stop\" requires at least 1 argument(s).\nSee 'docker stop --help'.\n\nUsage:  docker stop [OPTIONS] CONTAINER [CONTAINER...]\n\nStop one or more running containers", "stderr_lines": ["\"docker stop\" requires at least 1 argument(s).", "See 'docker stop --help'.", "", "Usage:  docker stop [OPTIONS] CONTAINER [CONTAINER...]", "", "Stop one or more running containers"], "stdout": "", "stdout_lines": []}
2018-06-05 17:13:57,456 p=11 u=root |  ...ignoring
2018-06-05 17:13:57,460 p=11 u=root |  fatal: [node3]: FAILED! => {"changed": true, "cmd": "docker stop `docker ps -q`", "delta": "0:00:00.038038", "end": "2018-06-05 11:13:57.554690", "failed": true, "msg": "non-zero return code", "rc": 1, "start": "2018-06-05 11:13:57.516652", "stderr": "\"docker stop\" requires at least 1 argument(s).\nSee 'docker stop --help'.\n\nUsage:  docker stop [OPTIONS] CONTAINER [CONTAINER...]\n\nStop one or more running containers", "stderr_lines": ["\"docker stop\" requires at least 1 argument(s).", "See 'docker stop --help'.", "", "Usage:  docker stop [OPTIONS] CONTAINER [CONTAINER...]", "", "Stop one or more running containers"], "stdout": "", "stdout_lines": []}
2018-06-05 17:13:57,460 p=11 u=root |  ...ignoring
2018-06-05 17:13:57,464 p=11 u=root |  fatal: [node1]: FAILED! => {"changed": true, "cmd": "docker stop `docker ps -q`", "delta": "0:00:00.040276", "end": "2018-06-05 11:13:57.554104", "failed": true, "msg": "non-zero return code", "rc": 1, "start": "2018-06-05 11:13:57.513828", "stderr": "\"docker stop\" requires at least 1 argument(s).\nSee 'docker stop --help'.\n\nUsage:  docker stop [OPTIONS] CONTAINER [CONTAINER...]\n\nStop one or more running containers", "stderr_lines": ["\"docker stop\" requires at least 1 argument(s).", "See 'docker stop --help'.", "", "Usage:  docker stop [OPTIONS] CONTAINER [CONTAINER...]", "", "Stop one or more running containers"], "stdout": "", "stdout_lines": []}
2018-06-05 17:13:57,464 p=11 u=root |  ...ignoring
2018-06-05 17:13:57,474 p=11 u=root |  TASK [baseliner : remove containers to avoid name clashes] *********************
2018-06-05 17:13:57,475 p=11 u=root |  Tuesday 05 June 2018  17:13:57 +0000 (0:00:00.999)       0:00:04.743 ********** 
2018-06-05 17:13:58,311 p=11 u=root |  fatal: [node2]: FAILED! => {"changed": true, "cmd": "docker rm `docker ps -qa`", "delta": "0:00:00.036662", "end": "2018-06-05 11:13:58.401169", "failed": true, "msg": "non-zero return code", "rc": 1, "start": "2018-06-05 11:13:58.364507", "stderr": "\"docker rm\" requires at least 1 argument(s).\nSee 'docker rm --help'.\n\nUsage:  docker rm [OPTIONS] CONTAINER [CONTAINER...]\n\nRemove one or more containers", "stderr_lines": ["\"docker rm\" requires at least 1 argument(s).", "See 'docker rm --help'.", "", "Usage:  docker rm [OPTIONS] CONTAINER [CONTAINER...]", "", "Remove one or more containers"], "stdout": "", "stdout_lines": []}
2018-06-05 17:13:58,311 p=11 u=root |  ...ignoring
2018-06-05 17:13:58,312 p=11 u=root |  fatal: [node3]: FAILED! => {"changed": true, "cmd": "docker rm `docker ps -qa`", "delta": "0:00:00.034121", "end": "2018-06-05 11:13:58.406603", "failed": true, "msg": "non-zero return code", "rc": 1, "start": "2018-06-05 11:13:58.372482", "stderr": "\"docker rm\" requires at least 1 argument(s).\nSee 'docker rm --help'.\n\nUsage:  docker rm [OPTIONS] CONTAINER [CONTAINER...]\n\nRemove one or more containers", "stderr_lines": ["\"docker rm\" requires at least 1 argument(s).", "See 'docker rm --help'.", "", "Usage:  docker rm [OPTIONS] CONTAINER [CONTAINER...]", "", "Remove one or more containers"], "stdout": "", "stdout_lines": []}
2018-06-05 17:13:58,312 p=11 u=root |  ...ignoring
2018-06-05 17:13:58,330 p=11 u=root |  fatal: [node1]: FAILED! => {"changed": true, "cmd": "docker rm `docker ps -qa`", "delta": "0:00:00.039422", "end": "2018-06-05 11:13:58.421836", "failed": true, "msg": "non-zero return code", "rc": 1, "start": "2018-06-05 11:13:58.382414", "stderr": "\"docker rm\" requires at least 1 argument(s).\nSee 'docker rm --help'.\n\nUsage:  docker rm [OPTIONS] CONTAINER [CONTAINER...]\n\nRemove one or more containers", "stderr_lines": ["\"docker rm\" requires at least 1 argument(s).", "See 'docker rm --help'.", "", "Usage:  docker rm [OPTIONS] CONTAINER [CONTAINER...]", "", "Remove one or more containers"], "stdout": "", "stdout_lines": []}
2018-06-05 17:13:58,331 p=11 u=root |  ...ignoring
2018-06-05 17:13:58,337 p=11 u=root |  TASK [baseliner : store facts] *************************************************
2018-06-05 17:13:58,337 p=11 u=root |  Tuesday 05 June 2018  17:13:58 +0000 (0:00:00.862)       0:00:05.605 ********** 
2018-06-05 17:13:58,395 p=11 u=root |  included: /etc/ansible/roles/baseliner/tasks/store-facts.yml for node1, node2, node3
2018-06-05 17:13:58,413 p=11 u=root |  TASK [baseliner : install facter] **********************************************
2018-06-05 17:13:58,413 p=11 u=root |  Tuesday 05 June 2018  17:13:58 +0000 (0:00:00.076)       0:00:05.681 ********** 
2018-06-05 17:13:59,878 p=11 u=root |  ok: [node2]
2018-06-05 17:13:59,885 p=11 u=root |  ok: [node1]
2018-06-05 17:13:59,894 p=11 u=root |  ok: [node3]
2018-06-05 17:13:59,903 p=11 u=root |  TASK [baseliner : create facts folder if it doesn't exist] *********************
2018-06-05 17:13:59,903 p=11 u=root |  Tuesday 05 June 2018  17:13:59 +0000 (0:00:01.489)       0:00:07.171 ********** 
2018-06-05 17:14:00,016 p=11 u=root |  changed: [node1 -> localhost]
2018-06-05 17:14:00,026 p=11 u=root |  TASK [baseliner : store facts about remotes] ***********************************
2018-06-05 17:14:00,026 p=11 u=root |  Tuesday 05 June 2018  17:14:00 +0000 (0:00:00.123)       0:00:07.295 ********** 
2018-06-05 17:14:00,415 p=11 u=root |  changed: [node1 -> localhost] => (item=node1)
2018-06-05 17:14:00,613 p=11 u=root |  changed: [node1 -> localhost] => (item=node2)
2018-06-05 17:14:00,802 p=11 u=root |  changed: [node1 -> localhost] => (item=node3)
2018-06-05 17:14:00,808 p=11 u=root |  TASK [baseliner : start monitoring] ********************************************
2018-06-05 17:14:00,808 p=11 u=root |  Tuesday 05 June 2018  17:14:00 +0000 (0:00:00.781)       0:00:08.077 ********** 
2018-06-05 17:14:00,845 p=11 u=root |  TASK [baseliner : include_tasks] ***********************************************
2018-06-05 17:14:00,845 p=11 u=root |  Tuesday 05 June 2018  17:14:00 +0000 (0:00:00.036)       0:00:08.113 ********** 
2018-06-05 17:14:00,880 p=11 u=root |  TASK [baseliner : get number of repetitions] ***********************************
2018-06-05 17:14:00,880 p=11 u=root |  Tuesday 05 June 2018  17:14:00 +0000 (0:00:00.034)       0:00:08.148 ********** 
2018-06-05 17:14:00,936 p=11 u=root |  ok: [node1] => (item=1)
2018-06-05 17:14:00,943 p=11 u=root |  ok: [node2] => (item=1)
2018-06-05 17:14:00,955 p=11 u=root |  ok: [node3] => (item=1)
2018-06-05 17:14:00,962 p=11 u=root |  TASK [baseliner : execute each benchmark] **************************************
2018-06-05 17:14:00,962 p=11 u=root |  Tuesday 05 June 2018  17:14:00 +0000 (0:00:00.082)       0:00:08.231 ********** 
2018-06-05 17:14:01,026 p=11 u=root |  included: /etc/ansible/roles/baseliner/tasks/run-bench.yml for node1, node2, node3
2018-06-05 17:14:01,050 p=11 u=root |  TASK [baseliner : upload files] ************************************************
2018-06-05 17:14:01,050 p=11 u=root |  Tuesday 05 June 2018  17:14:01 +0000 (0:00:00.087)       0:00:08.318 ********** 
2018-06-05 17:14:02,491 p=11 u=root |  changed: [node3] => (item={u'dest': u'/tmp/etc/ceph.conf', u'src': u'/experiment/scripts/docker-bench/ceph/etcceph/ceph.conf'})
2018-06-05 17:14:02,491 p=11 u=root |  changed: [node1] => (item={u'dest': u'/tmp/etc/ceph.conf', u'src': u'/experiment/scripts/docker-bench/ceph/etcceph/ceph.conf'})
2018-06-05 17:14:02,498 p=11 u=root |  changed: [node2] => (item={u'dest': u'/tmp/etc/ceph.conf', u'src': u'/experiment/scripts/docker-bench/ceph/etcceph/ceph.conf'})
2018-06-05 17:14:02,506 p=11 u=root |  TASK [baseliner : initialize parameters and add number of repetitions] *********
2018-06-05 17:14:02,506 p=11 u=root |  Tuesday 05 June 2018  17:14:02 +0000 (0:00:01.456)       0:00:09.775 ********** 
2018-06-05 17:14:02,567 p=11 u=root |  ok: [node1]
2018-06-05 17:14:02,571 p=11 u=root |  ok: [node2]
2018-06-05 17:14:02,586 p=11 u=root |  ok: [node3]
2018-06-05 17:14:02,592 p=11 u=root |  TASK [baseliner : unnest parameters when parameters for benchmark were passed] ***
2018-06-05 17:14:02,593 p=11 u=root |  Tuesday 05 June 2018  17:14:02 +0000 (0:00:00.086)       0:00:09.861 ********** 
2018-06-05 17:14:02,625 p=11 u=root |  TASK [baseliner : parametrized execution] **************************************
2018-06-05 17:14:02,626 p=11 u=root |  Tuesday 05 June 2018  17:14:02 +0000 (0:00:00.032)       0:00:09.894 ********** 
2018-06-05 17:14:02,700 p=11 u=root |  included: /etc/ansible/roles/baseliner/tasks/run-bench-parametrized.yml for node1, node2, node3
2018-06-05 17:14:02,710 p=11 u=root |  TASK [baseliner : remove remote results folder] ********************************
2018-06-05 17:14:02,710 p=11 u=root |  Tuesday 05 June 2018  17:14:02 +0000 (0:00:00.084)       0:00:09.978 ********** 
2018-06-05 17:14:03,539 p=11 u=root |  ok: [node1]
2018-06-05 17:14:03,540 p=11 u=root |  ok: [node2]
2018-06-05 17:14:03,543 p=11 u=root |  ok: [node3]
2018-06-05 17:14:03,553 p=11 u=root |  TASK [baseliner : create remote results folder] ********************************
2018-06-05 17:14:03,553 p=11 u=root |  Tuesday 05 June 2018  17:14:03 +0000 (0:00:00.842)       0:00:10.821 ********** 
2018-06-05 17:14:04,352 p=11 u=root |  changed: [node1]
2018-06-05 17:14:04,353 p=11 u=root |  changed: [node2]
2018-06-05 17:14:04,359 p=11 u=root |  changed: [node3]
2018-06-05 17:14:04,366 p=11 u=root |  TASK [baseliner : initialize parameter dictionary] *****************************
2018-06-05 17:14:04,366 p=11 u=root |  Tuesday 05 June 2018  17:14:04 +0000 (0:00:00.813)       0:00:11.634 ********** 
2018-06-05 17:14:04,424 p=11 u=root |  ok: [node1]
2018-06-05 17:14:04,428 p=11 u=root |  ok: [node2]
2018-06-05 17:14:04,439 p=11 u=root |  ok: [node3]
2018-06-05 17:14:04,446 p=11 u=root |  TASK [baseliner : populate parameter dictionary] *******************************
2018-06-05 17:14:04,446 p=11 u=root |  Tuesday 05 June 2018  17:14:04 +0000 (0:00:00.079)       0:00:11.714 ********** 
2018-06-05 17:14:04,501 p=11 u=root |  ok: [node1] => (item=[u'repetition', u'1'])
2018-06-05 17:14:04,508 p=11 u=root |  ok: [node2] => (item=[u'repetition', u'1'])
2018-06-05 17:14:04,519 p=11 u=root |  ok: [node3] => (item=[u'repetition', u'1'])
2018-06-05 17:14:04,526 p=11 u=root |  TASK [baseliner : merge default variables to the benchmark-specific options] ***
2018-06-05 17:14:04,526 p=11 u=root |  Tuesday 05 June 2018  17:14:04 +0000 (0:00:00.080)       0:00:11.794 ********** 
2018-06-05 17:14:04,587 p=11 u=root |  ok: [node1]
2018-06-05 17:14:04,591 p=11 u=root |  ok: [node2]
2018-06-05 17:14:04,605 p=11 u=root |  ok: [node3]
2018-06-05 17:14:04,612 p=11 u=root |  TASK [baseliner : initialize string for path] **********************************
2018-06-05 17:14:04,612 p=11 u=root |  Tuesday 05 June 2018  17:14:04 +0000 (0:00:00.085)       0:00:11.880 ********** 
2018-06-05 17:14:04,673 p=11 u=root |  ok: [node1]
2018-06-05 17:14:04,677 p=11 u=root |  ok: [node2]
2018-06-05 17:14:04,694 p=11 u=root |  ok: [node3]
2018-06-05 17:14:04,702 p=11 u=root |  TASK [baseliner : add key/value pairs for each parameter (if any)] *************
2018-06-05 17:14:04,703 p=11 u=root |  Tuesday 05 June 2018  17:14:04 +0000 (0:00:00.090)       0:00:11.971 ********** 
2018-06-05 17:14:04,769 p=11 u=root |  ok: [node1] => (item={'key': u'repetition', 'value': u'1'})
2018-06-05 17:14:04,789 p=11 u=root |  ok: [node3] => (item={'key': u'repetition', 'value': u'1'})
2018-06-05 17:14:04,797 p=11 u=root |  ok: [node2] => (item={'key': u'repetition', 'value': u'1'})
2018-06-05 17:14:04,806 p=11 u=root |  TASK [baseliner : ensure output folder exists] *********************************
2018-06-05 17:14:04,806 p=11 u=root |  Tuesday 05 June 2018  17:14:04 +0000 (0:00:00.103)       0:00:12.074 ********** 
2018-06-05 17:14:04,936 p=11 u=root |  changed: [node1 -> localhost]
2018-06-05 17:14:04,942 p=11 u=root |  changed: [node2 -> localhost]
2018-06-05 17:14:04,965 p=11 u=root |  changed: [node3 -> localhost]
2018-06-05 17:14:04,972 p=11 u=root |  TASK [baseliner : run containerized benchmark] *********************************
2018-06-05 17:14:04,972 p=11 u=root |  Tuesday 05 June 2018  17:14:04 +0000 (0:00:00.165)       0:00:12.240 ********** 
2018-06-05 17:14:05,097 p=11 u=root |  included: /etc/ansible/roles/baseliner/tasks/run-container.yml for node1, node2, node3
2018-06-05 17:14:05,118 p=11 u=root |  TASK [baseliner : check if image already exists] *******************************
2018-06-05 17:14:05,118 p=11 u=root |  Tuesday 05 June 2018  17:14:05 +0000 (0:00:00.146)       0:00:12.386 ********** 
2018-06-05 17:14:05,971 p=11 u=root |  changed: [node1]
2018-06-05 17:14:05,972 p=11 u=root |  changed: [node2]
2018-06-05 17:14:05,980 p=11 u=root |  changed: [node3]
2018-06-05 17:14:05,988 p=11 u=root |  TASK [baseliner : pull image] **************************************************
2018-06-05 17:14:05,989 p=11 u=root |  Tuesday 05 June 2018  17:14:05 +0000 (0:00:00.870)       0:00:13.257 ********** 
2018-06-05 17:14:08,475 p=11 u=root |  changed: [node3]
2018-06-05 17:14:08,555 p=11 u=root |  changed: [node1]
2018-06-05 17:14:08,721 p=11 u=root |  changed: [node2]
2018-06-05 17:14:08,728 p=11 u=root |  TASK [baseliner : define name of container] ************************************
2018-06-05 17:14:08,728 p=11 u=root |  Tuesday 05 June 2018  17:14:08 +0000 (0:00:02.739)       0:00:15.997 ********** 
2018-06-05 17:14:08,778 p=11 u=root |  ok: [node1]
2018-06-05 17:14:08,797 p=11 u=root |  ok: [node2]
2018-06-05 17:14:08,801 p=11 u=root |  ok: [node3]
2018-06-05 17:14:08,807 p=11 u=root |  TASK [baseliner : initialize docker_flags variable] ****************************
2018-06-05 17:14:08,808 p=11 u=root |  Tuesday 05 June 2018  17:14:08 +0000 (0:00:00.079)       0:00:16.076 ********** 
2018-06-05 17:14:08,856 p=11 u=root |  ok: [node1]
2018-06-05 17:14:08,864 p=11 u=root |  ok: [node2]
2018-06-05 17:14:08,873 p=11 u=root |  ok: [node3]
2018-06-05 17:14:08,880 p=11 u=root |  TASK [baseliner : add environment to docker_flags] *****************************
2018-06-05 17:14:08,880 p=11 u=root |  Tuesday 05 June 2018  17:14:08 +0000 (0:00:00.072)       0:00:16.148 ********** 
2018-06-05 17:14:08,912 p=11 u=root |  TASK [baseliner : add implicit environment from bench_params variable] *********
2018-06-05 17:14:08,912 p=11 u=root |  Tuesday 05 June 2018  17:14:08 +0000 (0:00:00.031)       0:00:16.180 ********** 
2018-06-05 17:14:08,967 p=11 u=root |  ok: [node1] => (item={'key': u'repetition', 'value': u'1'})
2018-06-05 17:14:08,976 p=11 u=root |  ok: [node2] => (item={'key': u'repetition', 'value': u'1'})
2018-06-05 17:14:08,985 p=11 u=root |  ok: [node3] => (item={'key': u'repetition', 'value': u'1'})
2018-06-05 17:14:08,991 p=11 u=root |  TASK [baseliner : check if we have host-specific ips] **************************
2018-06-05 17:14:08,991 p=11 u=root |  Tuesday 05 June 2018  17:14:08 +0000 (0:00:00.079)       0:00:16.260 ********** 
2018-06-05 17:14:09,043 p=11 u=root |  ok: [node1]
2018-06-05 17:14:09,050 p=11 u=root |  ok: [node2]
2018-06-05 17:14:09,061 p=11 u=root |  ok: [node3]
2018-06-05 17:14:09,068 p=11 u=root |  TASK [baseliner : add host-specific ips to docker_flags] ***********************
2018-06-05 17:14:09,068 p=11 u=root |  Tuesday 05 June 2018  17:14:09 +0000 (0:00:00.076)       0:00:16.336 ********** 
2018-06-05 17:14:09,102 p=11 u=root |  TASK [baseliner : check if we have host-specific environment] ******************
2018-06-05 17:14:09,102 p=11 u=root |  Tuesday 05 June 2018  17:14:09 +0000 (0:00:00.033)       0:00:16.370 ********** 
2018-06-05 17:14:09,156 p=11 u=root |  ok: [node1]
2018-06-05 17:14:09,174 p=11 u=root |  ok: [node3]
2018-06-05 17:14:09,178 p=11 u=root |  ok: [node2]
2018-06-05 17:14:09,185 p=11 u=root |  TASK [baseliner : add host-specific environment to docker_flags] ***************
2018-06-05 17:14:09,185 p=11 u=root |  Tuesday 05 June 2018  17:14:09 +0000 (0:00:00.082)       0:00:16.453 ********** 
2018-06-05 17:14:09,239 p=11 u=root |  ok: [node1] => (item={'key': u'CEPH_PUBLIC_NETWORK', 'value': u'128.110.152.0/22'})
2018-06-05 17:14:09,249 p=11 u=root |  ok: [node2] => (item={'key': u'CEPH_PUBLIC_NETWORK', 'value': u'128.110.152.0/22'})
2018-06-05 17:14:09,259 p=11 u=root |  ok: [node3] => (item={'key': u'CEPH_PUBLIC_NETWORK', 'value': u'128.110.152.0/22'})
2018-06-05 17:14:09,281 p=11 u=root |  ok: [node1] => (item={'key': u'MON_IP', 'value': u'128.110.153.197'})
2018-06-05 17:14:09,289 p=11 u=root |  ok: [node2] => (item={'key': u'MON_IP', 'value': u'128.110.153.197'})
2018-06-05 17:14:09,298 p=11 u=root |  ok: [node3] => (item={'key': u'CLIENT', 'value': True})
2018-06-05 17:14:09,321 p=11 u=root |  ok: [node1] => (item={'key': u'MONITOR', 'value': True})
2018-06-05 17:14:09,328 p=11 u=root |  ok: [node2] => (item={'key': u'OSD', 'value': True})
2018-06-05 17:14:09,336 p=11 u=root |  ok: [node3] => (item={'key': u'MON_IP', 'value': u'128.110.153.197'})
2018-06-05 17:14:09,399 p=11 u=root |  ok: [node2] => (item={'key': u'OSD_DEVICES', 'value': u'/dev/sdb'})
2018-06-05 17:14:09,405 p=11 u=root |  TASK [baseliner : add devices to docker_flags] *********************************
2018-06-05 17:14:09,406 p=11 u=root |  Tuesday 05 June 2018  17:14:09 +0000 (0:00:00.220)       0:00:16.674 ********** 
2018-06-05 17:14:09,439 p=11 u=root |  TASK [baseliner : add network mode to docker_flags] ****************************
2018-06-05 17:14:09,440 p=11 u=root |  Tuesday 05 June 2018  17:14:09 +0000 (0:00:00.033)       0:00:16.708 ********** 
2018-06-05 17:14:09,496 p=11 u=root |  ok: [node1]
2018-06-05 17:14:09,503 p=11 u=root |  ok: [node2]
2018-06-05 17:14:09,515 p=11 u=root |  ok: [node3]
2018-06-05 17:14:09,524 p=11 u=root |  TASK [baseliner : add ipc mode to docker_flags] ********************************
2018-06-05 17:14:09,524 p=11 u=root |  Tuesday 05 June 2018  17:14:09 +0000 (0:00:00.084)       0:00:16.792 ********** 
2018-06-05 17:14:09,559 p=11 u=root |  TASK [baseliner : add ports to docker_flags] ***********************************
2018-06-05 17:14:09,560 p=11 u=root |  Tuesday 05 June 2018  17:14:09 +0000 (0:00:00.035)       0:00:16.828 ********** 
2018-06-05 17:14:09,599 p=11 u=root |  TASK [baseliner : add volumes to docker_flags] *********************************
2018-06-05 17:14:09,599 p=11 u=root |  Tuesday 05 June 2018  17:14:09 +0000 (0:00:00.039)       0:00:16.867 ********** 
2018-06-05 17:14:09,656 p=11 u=root |  ok: [node1] => (item=/tmp/etc:/etc/ceph)
2018-06-05 17:14:09,662 p=11 u=root |  ok: [node2] => (item=/tmp/etc:/etc/ceph)
2018-06-05 17:14:09,671 p=11 u=root |  ok: [node3] => (item=/tmp/etc:/etc/ceph)
2018-06-05 17:14:09,678 p=11 u=root |  TASK [baseliner : add default volumes to docker_flags] *************************
2018-06-05 17:14:09,678 p=11 u=root |  Tuesday 05 June 2018  17:14:09 +0000 (0:00:00.079)       0:00:16.946 ********** 
2018-06-05 17:14:09,729 p=11 u=root |  ok: [node1]
2018-06-05 17:14:09,736 p=11 u=root |  ok: [node2]
2018-06-05 17:14:09,745 p=11 u=root |  ok: [node3]
2018-06-05 17:14:09,751 p=11 u=root |  TASK [baseliner : set entrypoint] **********************************************
2018-06-05 17:14:09,751 p=11 u=root |  Tuesday 05 June 2018  17:14:09 +0000 (0:00:00.073)       0:00:17.020 ********** 
2018-06-05 17:14:09,784 p=11 u=root |  TASK [baseliner : set limits] **************************************************
2018-06-05 17:14:09,785 p=11 u=root |  Tuesday 05 June 2018  17:14:09 +0000 (0:00:00.033)       0:00:17.053 ********** 
2018-06-05 17:14:09,820 p=11 u=root |  TASK [baseliner : add capabilities] ********************************************
2018-06-05 17:14:09,821 p=11 u=root |  Tuesday 05 June 2018  17:14:09 +0000 (0:00:00.036)       0:00:17.089 ********** 
2018-06-05 17:14:09,859 p=11 u=root |  TASK [baseliner : remove capabilities] *****************************************
2018-06-05 17:14:09,859 p=11 u=root |  Tuesday 05 June 2018  17:14:09 +0000 (0:00:00.038)       0:00:17.128 ********** 
2018-06-05 17:14:09,898 p=11 u=root |  TASK [baseliner : set privileged mode] *****************************************
2018-06-05 17:14:09,898 p=11 u=root |  Tuesday 05 June 2018  17:14:09 +0000 (0:00:00.038)       0:00:17.166 ********** 
2018-06-05 17:14:09,935 p=11 u=root |  TASK [baseliner : set memory constraint] ***************************************
2018-06-05 17:14:09,935 p=11 u=root |  Tuesday 05 June 2018  17:14:09 +0000 (0:00:00.037)       0:00:17.203 ********** 
2018-06-05 17:14:09,977 p=11 u=root |  TASK [baseliner : set memory-swap constraint] **********************************
2018-06-05 17:14:09,977 p=11 u=root |  Tuesday 05 June 2018  17:14:09 +0000 (0:00:00.041)       0:00:17.245 ********** 
2018-06-05 17:14:10,012 p=11 u=root |  TASK [baseliner : set parent cgroup] *******************************************
2018-06-05 17:14:10,013 p=11 u=root |  Tuesday 05 June 2018  17:14:10 +0000 (0:00:00.035)       0:00:17.281 ********** 
2018-06-05 17:14:10,048 p=11 u=root |  TASK [baseliner : debug] *******************************************************
2018-06-05 17:14:10,049 p=11 u=root |  Tuesday 05 June 2018  17:14:10 +0000 (0:00:00.036)       0:00:17.317 ********** 
2018-06-05 17:14:10,100 p=11 u=root |  ok: [node1] => {
    "msg": "docker run --rm  -e repetition=\"1\" -e CEPH_PUBLIC_NETWORK=\"128.110.152.0/22\" -e MON_IP=\"128.110.153.197\" -e MONITOR=\"True\" --net=host -v /tmp/etc:/etc/ceph -v /tmp/baseliner_output_node1:/results -v /tmp/baseliner_output_node1:/output mariettesouppe/rados_bench:master-ec8d33f-luminous-ubuntu-16.04-x86_64 "
}
2018-06-05 17:14:10,109 p=11 u=root |  ok: [node2] => {
    "msg": "docker run --rm  -e repetition=\"1\" -e CEPH_PUBLIC_NETWORK=\"128.110.152.0/22\" -e MON_IP=\"128.110.153.197\" -e OSD=\"True\" -e OSD_DEVICES=\"/dev/sdb\" --net=host -v /tmp/etc:/etc/ceph -v /tmp/baseliner_output_node2:/results -v /tmp/baseliner_output_node2:/output mariettesouppe/rados_bench:master-ec8d33f-luminous-ubuntu-16.04-x86_64 "
}
2018-06-05 17:14:10,119 p=11 u=root |  ok: [node3] => {
    "msg": "docker run --rm  -e repetition=\"1\" -e CEPH_PUBLIC_NETWORK=\"128.110.152.0/22\" -e CLIENT=\"True\" -e MON_IP=\"128.110.153.197\" --net=host -v /tmp/etc:/etc/ceph -v /tmp/baseliner_output_node3:/results -v /tmp/baseliner_output_node3:/output mariettesouppe/rados_bench:master-ec8d33f-luminous-ubuntu-16.04-x86_64 "
}
2018-06-05 17:14:10,126 p=11 u=root |  TASK [baseliner : run container] ***********************************************
2018-06-05 17:14:10,126 p=11 u=root |  Tuesday 05 June 2018  17:14:10 +0000 (0:00:00.077)       0:00:17.395 ********** 
2018-06-05 17:14:12,139 p=11 u=root |  changed: [node1]
2018-06-05 17:14:12,140 p=11 u=root |  changed: [node2]
2018-06-05 17:14:12,147 p=11 u=root |  changed: [node3]
2018-06-05 17:14:12,156 p=11 u=root |  TASK [baseliner : wait for containers in parallel mode and stop/kill them if they timeout] ***
2018-06-05 17:14:12,156 p=11 u=root |  Tuesday 05 June 2018  17:14:12 +0000 (0:00:02.029)       0:00:19.425 ********** 
2018-06-05 17:14:12,179 p=11 u=root |  TASK [baseliner : wait for containers in single-node mode and stop/kill them if they timeout] ***
2018-06-05 17:14:12,179 p=11 u=root |  Tuesday 05 June 2018  17:14:12 +0000 (0:00:00.022)       0:00:19.447 ********** 
2018-06-05 18:55:24,921 p=11 u=root |  changed: [node1 -> localhost]
2018-06-05 18:55:24,930 p=11 u=root |  TASK [baseliner : get wait result] *********************************************
2018-06-05 18:55:24,931 p=11 u=root |  Tuesday 05 June 2018  18:55:24 +0000 (1:41:12.751)       1:41:32.199 ********** 
2018-06-05 18:55:24,953 p=11 u=root |  ok: [node1 -> localhost]
2018-06-05 18:55:24,961 p=11 u=root |  TASK [baseliner : debug] *******************************************************
2018-06-05 18:55:24,962 p=11 u=root |  Tuesday 05 June 2018  18:55:24 +0000 (0:00:00.030)       1:41:32.230 ********** 
2018-06-05 18:55:24,980 p=11 u=root |  ok: [node1 -> localhost] => {
    "msg": "WAIT_FOR_RESULT:  WARNING: timeout waiting for 3 hosts"
}
2018-06-05 18:55:24,987 p=11 u=root |  TASK [baseliner : get result of container execution] ***************************
2018-06-05 18:55:24,987 p=11 u=root |  Tuesday 05 June 2018  18:55:24 +0000 (0:00:00.025)       1:41:32.255 ********** 
2018-06-05 18:55:25,932 p=11 u=root |  fatal: [node3]: FAILED! => {"ansible_job_id": "953575438422.8623", "changed": true, "cmd": "docker run --rm --name baseliner_node3  -e repetition=\"1\" -e CEPH_PUBLIC_NETWORK=\"128.110.152.0/22\" -e CLIENT=\"True\" -e MON_IP=\"128.110.153.197\" --net=host -v /tmp/etc:/etc/ceph -v /tmp/baseliner_output_node3:/results -v /tmp/baseliner_output_node3:/output mariettesouppe/rados_bench:master-ec8d33f-luminous-ubuntu-16.04-x86_64 ", "delta": "1:41:11.739995", "end": "2018-06-05 12:55:23.094676", "failed": true, "finished": 1, "msg": "non-zero return code", "rc": 137, "start": "2018-06-05 11:14:11.354681", "stderr": "+ '[' -z '' ']'\n+ '[' -z '' ']'\n+ '[' -z True ']'\n+ '[' -z 128.110.152.0/22 ']'\n+ '[' -z 128.110.153.197 ']'\n+ sed -i -e s@monip@128.110.153.197@ /etc/ceph/ceph.conf\n+ sed -i -e s@cephnet@128.110.152.0/22@ /etc/ceph/ceph.conf\n+ '[' -f /etc/ceph/extra.conf ']'\n+ '[' -n '' ']'\n+ '[' -n '' ']'\n+ '[' -n True ']'\n+ '[' -z '' ']'\n+ echo 'No CLIENT_SCRIPT defined, running built-in radosbench script.'\n+ CLIENT_SCRIPT=run_radosbench\n+ ceph_health_ok\n+ exec run_radosbench\n+ ceph osd pool rm rbd rbd --yes-i-really-really-mean-it\npool 'rbd' does not exist\n+ ceph osd pool create scbench 150 150\npool 'scbench' created\n+ ceph osd pool set scbench size 1\nset pool 1 size to 1\n+ rados bench -p scbench 30 write --no-cleanup", "stderr_lines": ["+ '[' -z '' ']'", "+ '[' -z '' ']'", "+ '[' -z True ']'", "+ '[' -z 128.110.152.0/22 ']'", "+ '[' -z 128.110.153.197 ']'", "+ sed -i -e s@monip@128.110.153.197@ /etc/ceph/ceph.conf", "+ sed -i -e s@cephnet@128.110.152.0/22@ /etc/ceph/ceph.conf", "+ '[' -f /etc/ceph/extra.conf ']'", "+ '[' -n '' ']'", "+ '[' -n '' ']'", "+ '[' -n True ']'", "+ '[' -z '' ']'", "+ echo 'No CLIENT_SCRIPT defined, running built-in radosbench script.'", "+ CLIENT_SCRIPT=run_radosbench", "+ ceph_health_ok", "+ exec run_radosbench", "+ ceph osd pool rm rbd rbd --yes-i-really-really-mean-it", "pool 'rbd' does not exist", "+ ceph osd pool create scbench 150 150", "pool 'scbench' created", "+ ceph osd pool set scbench size 1", "set pool 1 size to 1", "+ rados bench -p scbench 30 write --no-cleanup"], "stdout": "No CLIENT_SCRIPT defined, running built-in radosbench script.", "stdout_lines": ["No CLIENT_SCRIPT defined, running built-in radosbench script."]}
2018-06-05 18:55:25,932 p=11 u=root |  ...ignoring
2018-06-05 18:55:25,933 p=11 u=root |  fatal: [node2]: FAILED! => {"ansible_job_id": "378567698539.8704", "changed": true, "cmd": "docker run --rm --name baseliner_node2  -e repetition=\"1\" -e CEPH_PUBLIC_NETWORK=\"128.110.152.0/22\" -e MON_IP=\"128.110.153.197\" -e OSD=\"True\" -e OSD_DEVICES=\"/dev/sdb\" --net=host -v /tmp/etc:/etc/ceph -v /tmp/baseliner_output_node2:/results -v /tmp/baseliner_output_node2:/output mariettesouppe/rados_bench:master-ec8d33f-luminous-ubuntu-16.04-x86_64 ", "delta": "0:00:00.607924", "end": "2018-06-05 11:14:11.953972", "failed": true, "finished": 1, "msg": "non-zero return code", "rc": 1, "start": "2018-06-05 11:14:11.346048", "stderr": "+ '[' -z '' ']'\n+ '[' -z True ']'\n+ '[' -z 128.110.152.0/22 ']'\n+ '[' -z 128.110.153.197 ']'\n+ sed -i -e s@monip@128.110.153.197@ /etc/ceph/ceph.conf\n+ sed -i -e s@cephnet@128.110.152.0/22@ /etc/ceph/ceph.conf\n+ '[' -f /etc/ceph/extra.conf ']'\n+ '[' -n '' ']'\n+ '[' -n True ']'\n+ '[' -n '' ']'\n+ exec run_osds\n+ '[' -z /dev/sdb ']'\n+ devices=(`echo $OSD_DEVICES | sed 's/,/ /g'`)\n++ echo /dev/sdb\n++ sed 's/,/ /g'\n+ for dev in '\"${devices[@]}\"'\n+ export OSD_FORCE_ZAP=1\n+ OSD_FORCE_ZAP=1\n+ export OSD_DEVICE=/dev//dev/sdb\n+ OSD_DEVICE=/dev//dev/sdb\n+ export OSD_TYPE=disk\n+ OSD_TYPE=disk\n+ '[' /dev/sdb == /dev/sdb ']'\n+ /entrypoint.sh osd", "stderr_lines": ["+ '[' -z '' ']'", "+ '[' -z True ']'", "+ '[' -z 128.110.152.0/22 ']'", "+ '[' -z 128.110.153.197 ']'", "+ sed -i -e s@monip@128.110.153.197@ /etc/ceph/ceph.conf", "+ sed -i -e s@cephnet@128.110.152.0/22@ /etc/ceph/ceph.conf", "+ '[' -f /etc/ceph/extra.conf ']'", "+ '[' -n '' ']'", "+ '[' -n True ']'", "+ '[' -n '' ']'", "+ exec run_osds", "+ '[' -z /dev/sdb ']'", "+ devices=(`echo $OSD_DEVICES | sed 's/,/ /g'`)", "++ echo /dev/sdb", "++ sed 's/,/ /g'", "+ for dev in '\"${devices[@]}\"'", "+ export OSD_FORCE_ZAP=1", "+ OSD_FORCE_ZAP=1", "+ export OSD_DEVICE=/dev//dev/sdb", "+ OSD_DEVICE=/dev//dev/sdb", "+ export OSD_TYPE=disk", "+ OSD_TYPE=disk", "+ '[' /dev/sdb == /dev/sdb ']'", "+ /entrypoint.sh osd"], "stdout": "2018-06-05 17:14:11  /entrypoint.sh: static: does not generate config\n2018-06-05 17:14:11  /entrypoint.sh: ERROR- The device pointed by OSD_DEVICE (/dev//dev/sdb) doesn't exist !", "stdout_lines": ["2018-06-05 17:14:11  /entrypoint.sh: static: does not generate config", "2018-06-05 17:14:11  /entrypoint.sh: ERROR- The device pointed by OSD_DEVICE (/dev//dev/sdb) doesn't exist !"]}
2018-06-05 18:55:25,933 p=11 u=root |  ...ignoring
2018-06-05 18:55:25,961 p=11 u=root |  fatal: [node1]: FAILED! => {"ansible_job_id": "118989881313.3780", "changed": true, "cmd": "docker run --rm --name baseliner_node1  -e repetition=\"1\" -e CEPH_PUBLIC_NETWORK=\"128.110.152.0/22\" -e MON_IP=\"128.110.153.197\" -e MONITOR=\"True\" --net=host -v /tmp/etc:/etc/ceph -v /tmp/baseliner_output_node1:/results -v /tmp/baseliner_output_node1:/output mariettesouppe/rados_bench:master-ec8d33f-luminous-ubuntu-16.04-x86_64 ", "delta": "1:41:01.752659", "end": "2018-06-05 12:55:13.097682", "failed": true, "finished": 1, "msg": "non-zero return code", "rc": 1, "start": "2018-06-05 11:14:11.345023", "stderr": "+ '[' -z True ']'\n+ '[' -z 128.110.152.0/22 ']'\n+ '[' -z 128.110.153.197 ']'\n+ sed -i -e s@monip@128.110.153.197@ /etc/ceph/ceph.conf\n+ sed -i -e s@cephnet@128.110.152.0/22@ /etc/ceph/ceph.conf\n+ '[' -f /etc/ceph/extra.conf ']'\n+ '[' -n True ']'\n+ '[' -n '' ']'\n+ exec /entrypoint.sh mon\n2018-06-05 17:14:12.196603 7fc1d93f7f00  0 set uid:gid to 64045:64045 (ceph:ceph)\n2018-06-05 17:14:12.196620 7fc1d93f7f00  0 ceph version 12.2.5 (cad919881333ac92274171586c827e01f554a70a) luminous (stable), process (unknown), pid 122\n2018-06-05 17:14:12.196712 7fc1d93f7f00  0 pidfile_write: ignore empty --pid-file\n2018-06-05 17:14:12.204633 7fc1d93f7f00  0 load: jerasure load: lrc load: isa \n2018-06-05 17:14:12.204738 7fc1d93f7f00  0  set rocksdb option compression = kNoCompression\n2018-06-05 17:14:12.204751 7fc1d93f7f00  0  set rocksdb option write_buffer_size = 33554432\n2018-06-05 17:14:12.204770 7fc1d93f7f00  0  set rocksdb option compression = kNoCompression\n2018-06-05 17:14:12.204774 7fc1d93f7f00  0  set rocksdb option write_buffer_size = 33554432\n2018-06-05 17:14:12.204894 7fc1d93f7f00  4 rocksdb: RocksDB version: 5.4.0\n\n2018-06-05 17:14:12.204906 7fc1d93f7f00  4 rocksdb: Git sha rocksdb_build_git_sha:@0@\n2018-06-05 17:14:12.204907 7fc1d93f7f00  4 rocksdb: Compile date Apr 23 2018\n2018-06-05 17:14:12.204909 7fc1d93f7f00  4 rocksdb: DB SUMMARY\n\n2018-06-05 17:14:12.204955 7fc1d93f7f00  4 rocksdb: CURRENT file:  CURRENT\n\n2018-06-05 17:14:12.204957 7fc1d93f7f00  4 rocksdb: IDENTITY file:  IDENTITY\n\n2018-06-05 17:14:12.204965 7fc1d93f7f00  4 rocksdb: MANIFEST file:  MANIFEST-000001 size: 13 Bytes\n\n2018-06-05 17:14:12.204967 7fc1d93f7f00  4 rocksdb: SST files in /var/lib/ceph/mon/ceph-node/store.db dir, Total Num: 0, files: \n\n2018-06-05 17:14:12.204969 7fc1d93f7f00  4 rocksdb: Write Ahead Log file in /var/lib/ceph/mon/ceph-node/store.db: 000003.log size: 454 ; \n\n2018-06-05 17:14:12.204971 7fc1d93f7f00  4 rocksdb:                         Options.error_if_exists: 0\n2018-06-05 17:14:12.204972 7fc1d93f7f00  4 rocksdb:                       Options.create_if_missing: 0\n2018-06-05 17:14:12.204973 7fc1d93f7f00  4 rocksdb:                         Options.paranoid_checks: 1\n2018-06-05 17:14:12.204973 7fc1d93f7f00  4 rocksdb:                                     Options.env: 0x55e8c9a64020\n2018-06-05 17:14:12.204974 7fc1d93f7f00  4 rocksdb:                                Options.info_log: 0x55e8cba38c80\n2018-06-05 17:14:12.204976 7fc1d93f7f00  4 rocksdb:                          Options.max_open_files: -1\n2018-06-05 17:14:12.204977 7fc1d93f7f00  4 rocksdb:                Options.max_file_opening_threads: 16\n2018-06-05 17:14:12.204977 7fc1d93f7f00  4 rocksdb:                               Options.use_fsync: 0\n2018-06-05 17:14:12.204978 7fc1d93f7f00  4 rocksdb:                       Options.max_log_file_size: 0\n2018-06-05 17:14:12.204979 7fc1d93f7f00  4 rocksdb:                  Options.max_manifest_file_size: 18446744073709551615\n2018-06-05 17:14:12.204980 7fc1d93f7f00  4 rocksdb:                   Options.log_file_time_to_roll: 0\n2018-06-05 17:14:12.204981 7fc1d93f7f00  4 rocksdb:                       Options.keep_log_file_num: 1000\n2018-06-05 17:14:12.204982 7fc1d93f7f00  4 rocksdb:                    Options.recycle_log_file_num: 0\n2018-06-05 17:14:12.204990 7fc1d93f7f00  4 rocksdb:                         Options.allow_fallocate: 1\n2018-06-05 17:14:12.204990 7fc1d93f7f00  4 rocksdb:                        Options.allow_mmap_reads: 0\n2018-06-05 17:14:12.204991 7fc1d93f7f00  4 rocksdb:                       Options.allow_mmap_writes: 0\n2018-06-05 17:14:12.204992 7fc1d93f7f00  4 rocksdb:                        Options.use_direct_reads: 0\n2018-06-05 17:14:12.204993 7fc1d93f7f00  4 rocksdb:                        Options.use_direct_io_for_flush_and_compaction: 0\n2018-06-05 17:14:12.204994 7fc1d93f7f00  4 rocksdb:          Options.create_missing_column_families: 0\n2018-06-05 17:14:12.204995 7fc1d93f7f00  4 rocksdb:                              Options.db_log_dir: \n2018-06-05 17:14:12.204995 7fc1d93f7f00  4 rocksdb:                                 Options.wal_dir: /var/lib/ceph/mon/ceph-node/store.db\n2018-06-05 17:14:12.204996 7fc1d93f7f00  4 rocksdb:                Options.table_cache_numshardbits: 6\n2018-06-05 17:14:12.204997 7fc1d93f7f00  4 rocksdb:                      Options.max_subcompactions: 1\n2018-06-05 17:14:12.204998 7fc1d93f7f00  4 rocksdb:                  Options.max_background_flushes: 1\n2018-06-05 17:14:12.204999 7fc1d93f7f00  4 rocksdb:                         Options.WAL_ttl_seconds: 0\n2018-06-05 17:14:12.205000 7fc1d93f7f00  4 rocksdb:                       Options.WAL_size_limit_MB: 0\n2018-06-05 17:14:12.205001 7fc1d93f7f00  4 rocksdb:             Options.manifest_preallocation_size: 4194304\n2018-06-05 17:14:12.205002 7fc1d93f7f00  4 rocksdb:                     Options.is_fd_close_on_exec: 1\n2018-06-05 17:14:12.205002 7fc1d93f7f00  4 rocksdb:                   Options.advise_random_on_open: 1\n2018-06-05 17:14:12.205003 7fc1d93f7f00  4 rocksdb:                    Options.db_write_buffer_size: 0\n2018-06-05 17:14:12.205004 7fc1d93f7f00  4 rocksdb:         Options.access_hint_on_compaction_start: 1\n2018-06-05 17:14:12.205005 7fc1d93f7f00  4 rocksdb:  Options.new_table_reader_for_compaction_inputs: 0\n2018-06-05 17:14:12.205005 7fc1d93f7f00  4 rocksdb:               Options.compaction_readahead_size: 0\n2018-06-05 17:14:12.205006 7fc1d93f7f00  4 rocksdb:           Options.random_access_max_buffer_size: 1048576\n2018-06-05 17:14:12.205007 7fc1d93f7f00  4 rocksdb:           Options.writable_file_max_buffer_size: 1048576\n2018-06-05 17:14:12.205008 7fc1d93f7f00  4 rocksdb:                      Options.use_adaptive_mutex: 0\n2018-06-05 17:14:12.205009 7fc1d93f7f00  4 rocksdb:                            Options.rate_limiter: (nil)\n2018-06-05 17:14:12.205010 7fc1d93f7f00  4 rocksdb:     Options.sst_file_manager.rate_bytes_per_sec: 0\n2018-06-05 17:14:12.205011 7fc1d93f7f00  4 rocksdb:                          Options.bytes_per_sync: 0\n2018-06-05 17:14:12.205012 7fc1d93f7f00  4 rocksdb:                      Options.wal_bytes_per_sync: 0\n2018-06-05 17:14:12.205013 7fc1d93f7f00  4 rocksdb:                       Options.wal_recovery_mode: 2\n2018-06-05 17:14:12.205013 7fc1d93f7f00  4 rocksdb:                  Options.enable_thread_tracking: 0\n2018-06-05 17:14:12.205014 7fc1d93f7f00  4 rocksdb:         Options.allow_concurrent_memtable_write: 1\n2018-06-05 17:14:12.205015 7fc1d93f7f00  4 rocksdb:      Options.enable_write_thread_adaptive_yield: 1\n2018-06-05 17:14:12.205016 7fc1d93f7f00  4 rocksdb:             Options.write_thread_max_yield_usec: 100\n2018-06-05 17:14:12.205016 7fc1d93f7f00  4 rocksdb:            Options.write_thread_slow_yield_usec: 3\n2018-06-05 17:14:12.205017 7fc1d93f7f00  4 rocksdb:                               Options.row_cache: None\n2018-06-05 17:14:12.205018 7fc1d93f7f00  4 rocksdb:                              Options.wal_filter: None\n2018-06-05 17:14:12.205019 7fc1d93f7f00  4 rocksdb:             Options.avoid_flush_during_recovery: 0\n2018-06-05 17:14:12.205020 7fc1d93f7f00  4 rocksdb:             Options.base_background_compactions: 1\n2018-06-05 17:14:12.205021 7fc1d93f7f00  4 rocksdb:             Options.max_background_compactions: 1\n2018-06-05 17:14:12.205021 7fc1d93f7f00  4 rocksdb:             Options.avoid_flush_during_shutdown: 0\n2018-06-05 17:14:12.205022 7fc1d93f7f00  4 rocksdb:             Options.delayed_write_rate : 16777216\n2018-06-05 17:14:12.205023 7fc1d93f7f00  4 rocksdb:             Options.max_total_wal_size: 0\n2018-06-05 17:14:12.205024 7fc1d93f7f00  4 rocksdb:             Options.delete_obsolete_files_period_micros: 21600000000\n2018-06-05 17:14:12.205025 7fc1d93f7f00  4 rocksdb:                   Options.stats_dump_period_sec: 600\n2018-06-05 17:14:12.205026 7fc1d93f7f00  4 rocksdb: Compression algorithms supported:\n2018-06-05 17:14:12.205027 7fc1d93f7f00  4 rocksdb: \tSnappy supported: 0\n2018-06-05 17:14:12.205028 7fc1d93f7f00  4 rocksdb: \tZlib supported: 0\n2018-06-05 17:14:12.205029 7fc1d93f7f00  4 rocksdb: \tBzip supported: 0\n2018-06-05 17:14:12.205029 7fc1d93f7f00  4 rocksdb: \tLZ4 supported: 0\n2018-06-05 17:14:12.205030 7fc1d93f7f00  4 rocksdb: \tZSTD supported: 0\n2018-06-05 17:14:12.205031 7fc1d93f7f00  4 rocksdb: Fast CRC32 supported: 1\n2018-06-05 17:14:12.205143 7fc1d93f7f00  4 rocksdb: [/build/ceph-12.2.5/src/rocksdb/db/version_set.cc:2609] Recovering from manifest file: MANIFEST-000001\n\n2018-06-05 17:14:12.205197 7fc1d93f7f00  4 rocksdb: [/build/ceph-12.2.5/src/rocksdb/db/column_family.cc:407] --------------- Options for column family [default]:\n\n2018-06-05 17:14:12.205199 7fc1d93f7f00  4 rocksdb:               Options.comparator: leveldb.BytewiseComparator\n2018-06-05 17:14:12.205201 7fc1d93f7f00  4 rocksdb:           Options.merge_operator: \n2018-06-05 17:14:12.205202 7fc1d93f7f00  4 rocksdb:        Options.compaction_filter: None\n2018-06-05 17:14:12.205203 7fc1d93f7f00  4 rocksdb:        Options.compaction_filter_factory: None\n2018-06-05 17:14:12.205204 7fc1d93f7f00  4 rocksdb:         Options.memtable_factory: SkipListFactory\n2018-06-05 17:14:12.205205 7fc1d93f7f00  4 rocksdb:            Options.table_factory: BlockBasedTable\n2018-06-05 17:14:12.205225 7fc1d93f7f00  4 rocksdb:            table_factory options:   flush_block_policy_factory: FlushBlockBySizePolicyFactory (0x55e8cb7140f8)\n  cache_index_and_filter_blocks: 1\n  cache_index_and_filter_blocks_with_high_priority: 1\n  pin_l0_filter_and_index_blocks_in_cache: 1\n  index_type: 0\n  hash_index_allow_collision: 1\n  checksum: 1\n  no_block_cache: 0\n  block_cache: 0x55e8cba205c0\n  block_cache_name: LRUCache\n  block_cache_options:\n    capacity : 134217728\n    num_shard_bits : 4\n    strict_capacity_limit : 0\n    high_pri_pool_ratio: 0.000\n  block_cache_compressed: (nil)\n  persistent_cache: (nil)\n  block_size: 4096\n  block_size_deviation: 10\n  block_restart_interval: 16\n  index_block_restart_interval: 1\n  filter_policy: rocksdb.BuiltinBloomFilter\n  whole_key_filtering: 1\n  format_version: 2\n\n2018-06-05 17:14:12.205230 7fc1d93f7f00  4 rocksdb:        Options.write_buffer_size: 33554432\n2018-06-05 17:14:12.205231 7fc1d93f7f00  4 rocksdb:  Options.max_write_buffer_number: 2\n2018-06-05 17:14:12.205232 7fc1d93f7f00  4 rocksdb:          Options.compression: NoCompression\n2018-06-05 17:14:12.205233 7fc1d93f7f00  4 rocksdb:                  Options.bottommost_compression: Disabled\n2018-06-05 17:14:12.205233 7fc1d93f7f00  4 rocksdb:       Options.prefix_extractor: nullptr\n2018-06-05 17:14:12.205240 7fc1d93f7f00  4 rocksdb:   Options.memtable_insert_with_hint_prefix_extractor: nullptr\n2018-06-05 17:14:12.205241 7fc1d93f7f00  4 rocksdb:             Options.num_levels: 7\n2018-06-05 17:14:12.205242 7fc1d93f7f00  4 rocksdb:        Options.min_write_buffer_number_to_merge: 1\n2018-06-05 17:14:12.205242 7fc1d93f7f00  4 rocksdb:     Options.max_write_buffer_number_to_maintain: 0\n2018-06-05 17:14:12.205243 7fc1d93f7f00  4 rocksdb:            Options.compression_opts.window_bits: -14\n2018-06-05 17:14:12.205244 7fc1d93f7f00  4 rocksdb:                  Options.compression_opts.level: -1\n2018-06-05 17:14:12.205245 7fc1d93f7f00  4 rocksdb:               Options.compression_opts.strategy: 0\n2018-06-05 17:14:12.205246 7fc1d93f7f00  4 rocksdb:         Options.compression_opts.max_dict_bytes: 0\n2018-06-05 17:14:12.205246 7fc1d93f7f00  4 rocksdb:      Options.level0_file_num_compaction_trigger: 4\n2018-06-05 17:14:12.205247 7fc1d93f7f00  4 rocksdb:          Options.level0_slowdown_writes_trigger: 20\n2018-06-05 17:14:12.205248 7fc1d93f7f00  4 rocksdb:              Options.level0_stop_writes_trigger: 36\n2018-06-05 17:14:12.205248 7fc1d93f7f00  4 rocksdb:                   Options.target_file_size_base: 67108864\n2018-06-05 17:14:12.205249 7fc1d93f7f00  4 rocksdb:             Options.target_file_size_multiplier: 1\n2018-06-05 17:14:12.205250 7fc1d93f7f00  4 rocksdb:                Options.max_bytes_for_level_base: 268435456\n2018-06-05 17:14:12.205250 7fc1d93f7f00  4 rocksdb: Options.level_compaction_dynamic_level_bytes: 0\n2018-06-05 17:14:12.205251 7fc1d93f7f00  4 rocksdb:          Options.max_bytes_for_level_multiplier: 10.000000\n2018-06-05 17:14:12.205254 7fc1d93f7f00  4 rocksdb: Options.max_bytes_for_level_multiplier_addtl[0]: 1\n2018-06-05 17:14:12.205255 7fc1d93f7f00  4 rocksdb: Options.max_bytes_for_level_multiplier_addtl[1]: 1\n2018-06-05 17:14:12.205256 7fc1d93f7f00  4 rocksdb: Options.max_bytes_for_level_multiplier_addtl[2]: 1\n2018-06-05 17:14:12.205256 7fc1d93f7f00  4 rocksdb: Options.max_bytes_for_level_multiplier_addtl[3]: 1\n2018-06-05 17:14:12.205257 7fc1d93f7f00  4 rocksdb: Options.max_bytes_for_level_multiplier_addtl[4]: 1\n2018-06-05 17:14:12.205258 7fc1d93f7f00  4 rocksdb: Options.max_bytes_for_level_multiplier_addtl[5]: 1\n2018-06-05 17:14:12.205258 7fc1d93f7f00  4 rocksdb: Options.max_bytes_for_level_multiplier_addtl[6]: 1\n2018-06-05 17:14:12.205259 7fc1d93f7f00  4 rocksdb:       Options.max_sequential_skip_in_iterations: 8\n2018-06-05 17:14:12.205260 7fc1d93f7f00  4 rocksdb:                    Options.max_compaction_bytes: 1677721600\n2018-06-05 17:14:12.205260 7fc1d93f7f00  4 rocksdb:                        Options.arena_block_size: 4194304\n2018-06-05 17:14:12.205261 7fc1d93f7f00  4 rocksdb:   Options.soft_pending_compaction_bytes_limit: 68719476736\n2018-06-05 17:14:12.205262 7fc1d93f7f00  4 rocksdb:   Options.hard_pending_compaction_bytes_limit: 274877906944\n2018-06-05 17:14:12.205263 7fc1d93f7f00  4 rocksdb:       Options.rate_limit_delay_max_milliseconds: 100\n2018-06-05 17:14:12.205263 7fc1d93f7f00  4 rocksdb:                Options.disable_auto_compactions: 0\n2018-06-05 17:14:12.205264 7fc1d93f7f00  4 rocksdb:                         Options.compaction_style: kCompactionStyleLevel\n2018-06-05 17:14:12.205266 7fc1d93f7f00  4 rocksdb:                           Options.compaction_pri: kByCompensatedSize\n2018-06-05 17:14:12.205266 7fc1d93f7f00  4 rocksdb:  Options.compaction_options_universal.size_ratio: 1\n2018-06-05 17:14:12.205267 7fc1d93f7f00  4 rocksdb: Options.compaction_options_universal.min_merge_width: 2\n2018-06-05 17:14:12.205268 7fc1d93f7f00  4 rocksdb: Options.compaction_options_universal.max_merge_width: 4294967295\n2018-06-05 17:14:12.205269 7fc1d93f7f00  4 rocksdb: Options.compaction_options_universal.max_size_amplification_percent: 200\n2018-06-05 17:14:12.205270 7fc1d93f7f00  4 rocksdb: Options.compaction_options_universal.compression_size_percent: -1\n2018-06-05 17:14:12.205271 7fc1d93f7f00  4 rocksdb: Options.compaction_options_fifo.max_table_files_size: 1073741824\n2018-06-05 17:14:12.205271 7fc1d93f7f00  4 rocksdb:                   Options.table_properties_collectors: \n2018-06-05 17:14:12.205272 7fc1d93f7f00  4 rocksdb:                   Options.inplace_update_support: 0\n2018-06-05 17:14:12.205273 7fc1d93f7f00  4 rocksdb:                 Options.inplace_update_num_locks: 10000\n2018-06-05 17:14:12.205274 7fc1d93f7f00  4 rocksdb:               Options.memtable_prefix_bloom_size_ratio: 0.000000\n2018-06-05 17:14:12.205275 7fc1d93f7f00  4 rocksdb:   Options.memtable_huge_page_size: 0\n2018-06-05 17:14:12.205275 7fc1d93f7f00  4 rocksdb:                           Options.bloom_locality: 0\n2018-06-05 17:14:12.205276 7fc1d93f7f00  4 rocksdb:                    Options.max_successive_merges: 0\n2018-06-05 17:14:12.205277 7fc1d93f7f00  4 rocksdb:                Options.optimize_filters_for_hits: 0\n2018-06-05 17:14:12.205278 7fc1d93f7f00  4 rocksdb:                Options.paranoid_file_checks: 0\n2018-06-05 17:14:12.205278 7fc1d93f7f00  4 rocksdb:                Options.force_consistency_checks: 0\n2018-06-05 17:14:12.205279 7fc1d93f7f00  4 rocksdb:                Options.report_bg_io_stats: 0\n2018-06-05 17:14:12.206750 7fc1d93f7f00  4 rocksdb: [/build/ceph-12.2.5/src/rocksdb/db/version_set.cc:2859] Recovered from manifest file:/var/lib/ceph/mon/ceph-node/store.db/MANIFEST-000001 succeeded,manifest_file_number is 1, next_file_number is 3, last_sequence is 0, log_number is 0,prev_log_number is 0,max_column_family is 0\n\n2018-06-05 17:14:12.206761 7fc1d93f7f00  4 rocksdb: [/build/ceph-12.2.5/src/rocksdb/db/version_set.cc:2867] Column family [default] (ID 0), log number is 0\n\n2018-06-05 17:14:12.206822 7fc1d93f7f00  4 rocksdb: EVENT_LOG_v1 {\"time_micros\": 1528218852206812, \"job\": 1, \"event\": \"recovery_started\", \"log_files\": [3]}\n2018-06-05 17:14:12.206827 7fc1d93f7f00  4 rocksdb: [/build/ceph-12.2.5/src/rocksdb/db/db_impl_open.cc:482] Recovering log #3 mode 2\n2018-06-05 17:14:12.211571 7fc1d93f7f00  4 rocksdb: EVENT_LOG_v1 {\"time_micros\": 1528218852211559, \"cf_name\": \"default\", \"job\": 1, \"event\": \"table_file_creation\", \"file_number\": 4, \"file_size\": 1420, \"table_properties\": {\"data_size\": 463, \"index_size\": 28, \"filter_size\": 20, \"raw_key_size\": 95, \"raw_average_key_size\": 23, \"raw_value_size\": 359, \"raw_average_value_size\": 89, \"num_data_blocks\": 1, \"num_entries\": 4, \"filter_policy_name\": \"rocksdb.BuiltinBloomFilter\", \"kDeletedKeys\": \"0\", \"kMergeOperands\": \"0\"}}\n2018-06-05 17:14:12.211601 7fc1d93f7f00  4 rocksdb: [/build/ceph-12.2.5/src/rocksdb/db/version_set.cc:2395] Creating manifest 5\n\n2018-06-05 17:14:12.220227 7fc1d93f7f00  4 rocksdb: EVENT_LOG_v1 {\"time_micros\": 1528218852220222, \"job\": 1, \"event\": \"recovery_finished\"}\n2018-06-05 17:14:12.230977 7fc1d93f7f00  4 rocksdb: [/build/ceph-12.2.5/src/rocksdb/db/db_impl_open.cc:1063] DB pointer 0x55e8cbb34000\n2018-06-05 17:14:12.232743 7fc1d93f7f00  0 starting mon.node rank 0 at public addr 128.110.153.197:6789/0 at bind addr 128.110.153.197:6789/0 mon_data /var/lib/ceph/mon/ceph-node fsid b2bb6c0b-6cc6-44f1-bf92-1ce81d78fbdc\n2018-06-05 17:14:12.232990 7fc1d93f7f00  0 starting mon.node rank 0 at 128.110.153.197:6789/0 mon_data /var/lib/ceph/mon/ceph-node fsid b2bb6c0b-6cc6-44f1-bf92-1ce81d78fbdc\n2018-06-05 17:14:12.234128 7fc1d93f7f00  1 mon.node@-1(probing) e0 preinit fsid b2bb6c0b-6cc6-44f1-bf92-1ce81d78fbdc\n2018-06-05 17:14:12.234361 7fc1d93f7f00  1 mon.node@-1(probing).mds e0 Unable to load 'last_metadata'\n2018-06-05 17:14:12.235253 7fc1d93f7f00  0 mon.node@-1(probing) e0  my rank is now 0 (was -1)\n2018-06-05 17:14:12.235281 7fc1d93f7f00  1 mon.node@0(probing) e0 win_standalone_election\n2018-06-05 17:14:12.235303 7fc1d93f7f00  1 mon.node@0(probing).elector(0) init, first boot, initializing epoch at 1 \n2018-06-05 17:14:12.244200 7fc1d93f7f00  0 log_channel(cluster) log [INF] : mon.node is new leader, mons node in quorum (ranks 0)\n2018-06-05 17:14:12.248559 7fc1d93f7f00  1 mon.node@0(leader).osd e0 create_pending setting backfillfull_ratio = 0.9\n2018-06-05 17:14:12.248598 7fc1d93f7f00  1 mon.node@0(leader).osd e0 create_pending setting full_ratio = 0.95\n2018-06-05 17:14:12.248603 7fc1d93f7f00  1 mon.node@0(leader).osd e0 create_pending setting nearfull_ratio = 0.85\n2018-06-05 17:14:12.248974 7fc1d93f7f00  1 mon.node@0(leader).osd e0 encode_pending skipping prime_pg_temp; mapping job did not start\n2018-06-05 17:14:12.250812 7fc1d93f7f00  1 mon.node@0(leader) e0 _apply_compatset_features enabling new quorum features: compat={},rocompat={},incompat={4=support erasure code pools,5=new-style osdmap encoding,6=support isa/lrc erasure code,7=support shec erasure code}\n2018-06-05 17:14:12.257261 7fc1cba01700  1 mon.node@0(leader).paxosservice(pgmap 0..0) refresh upgraded, format 1 -> 0\n2018-06-05 17:14:12.257275 7fc1cba01700  1 mon.node@0(leader).pg v0 on_upgrade discarding in-core PGMap\n2018-06-05 17:14:12.262914 7fc1cba01700  1 mon.node@0(leader).paxosservice(auth 0..0) refresh upgraded, format 2 -> 0\n2018-06-05 17:14:12.263130 7fc1cba01700  1 mon.node@0(probing) e1 win_standalone_election\n2018-06-05 17:14:12.263142 7fc1cba01700  1 mon.node@0(probing).elector(2) init, last seen epoch 2\n2018-06-05 17:14:12.267695 7fc1cba01700  0 log_channel(cluster) log [INF] : mon.node is new leader, mons node in quorum (ranks 0)\n2018-06-05 17:14:12.272050 7fc1cba01700  0 log_channel(cluster) log [DBG] : monmap e1: 1 mons at {node=128.110.153.197:6789/0}\n2018-06-05 17:14:12.276445 7fc1cba01700  1 mon.node@0(leader).osd e0 create_pending setting backfillfull_ratio = 0.9\n2018-06-05 17:14:12.276460 7fc1cba01700  1 mon.node@0(leader).osd e0 create_pending setting full_ratio = 0.95\n2018-06-05 17:14:12.276467 7fc1cba01700  1 mon.node@0(leader).osd e0 create_pending setting nearfull_ratio = 0.85\n2018-06-05 17:14:12.276666 7fc1cba01700  1 mon.node@0(leader).osd e0 encode_pending skipping prime_pg_temp; mapping job did not start\n2018-06-05 17:14:12.277309 7fc1cba01700  1 mon.node@0(leader) e1 _apply_compatset_features enabling new quorum features: compat={},rocompat={},incompat={8=support monmap features,9=luminous ondisk layout}\n2018-06-05 17:14:12.286691 7fc1cba01700  1 mon.node@0(leader).paxosservice(auth 0..0) refresh upgraded, format 2 -> 0\n2018-06-05 17:14:12.286805 7fc1cba01700  0 log_channel(cluster) log [INF] : pgmap 0 pgs: ; 0 bytes data, 0 kB used, 0 kB / 0 kB avail\n2018-06-05 17:14:12.295855 7fc1cba01700  0 mon.node@0(leader).mds e1 print_map\ne1\nenable_multiple, ever_enabled_multiple: 0,0\ncompat: compat={},rocompat={},incompat={1=base v0.20,2=client writeable ranges,3=default file layouts on dirs,4=dir inode in separate object,5=mds uses versioned encoding,6=dirfrag is stored in omap,8=no anchor table,9=file layout v2}\nlegacy client fscid: -1\n \nNo filesystems configured\n\n2018-06-05 17:14:12.296024 7fc1cba01700  1 mon.node@0(leader).osd e1 e1: 0 total, 0 up, 0 in\n2018-06-05 17:14:12.300502 7fc1cba01700  0 mon.node@0(leader).osd e1 crush map has features 288514050185494528, adjusting msgr requires\n2018-06-05 17:14:12.300507 7fc1cba01700  0 mon.node@0(leader).osd e1 crush map has features 288514050185494528, adjusting msgr requires\n2018-06-05 17:14:12.300511 7fc1cba01700  0 mon.node@0(leader).osd e1 crush map has features 1009089990564790272, adjusting msgr requires\n2018-06-05 17:14:12.300513 7fc1cba01700  0 mon.node@0(leader).osd e1 crush map has features 288514050185494528, adjusting msgr requires\n2018-06-05 17:14:12.300660 7fc1cba01700  1 mon.node@0(leader).log v1 unable to write to '/var/log/ceph/ceph.log' for channel 'cluster': (2) No such file or directory\n2018-06-05 17:14:12.300727 7fc1cba01700  1 mon.node@0(leader).paxosservice(auth 1..1) refresh upgraded, format 0 -> 2\n2018-06-05 17:14:12.301408 7fc1cba01700  0 log_channel(cluster) log [DBG] : fsmap \n2018-06-05 17:14:12.301723 7fc1cba01700  1 mon.node@0(leader).pg v1 check_osd_map will clear pg_map data\n2018-06-05 17:14:12.301728 7fc1cba01700  1 mon.node@0(leader).pg v1 encode_pending clearing pgmap data at v2\n2018-06-05 17:14:12.305949 7fc1cba01700  0 log_channel(cluster) log [DBG] : osdmap e1: 0 total, 0 up, 0 in\n2018-06-05 17:14:12.306078 7fc1cba01700  0 log_channel(cluster) log [DBG] : mgrmap e1: no daemons active\n2018-06-05 17:14:12.319794 7fc1cba01700  1 mon.node@0(leader).log v2 unable to write to '/var/log/ceph/ceph.log' for channel 'cluster': (2) No such file or directory\n2018-06-05 17:14:12.568034 7fc1d020a700  0 mon.node@0(leader) e1 handle_command mon_command({\"prefix\": \"status\"} v 0) v1\n2018-06-05 17:14:12.568057 7fc1d020a700  0 log_channel(audit) log [DBG] : from='client.? 128.110.153.138:0/2977579145' entity='client.admin' cmd=[{\"prefix\": \"status\"}]: dispatch\n2018-06-05 17:14:12.857199 7fc1d020a700  0 mon.node@0(leader) e1 handle_command mon_command({\"pool2\": \"rbd\", \"prefix\": \"osd pool rm\", \"sure\": \"--yes-i-really-really-mean-it\", \"pool\": \"rbd\"} v 0) v1\n2018-06-05 17:14:12.857274 7fc1d020a700  0 log_channel(audit) log [INF] : from='client.? 128.110.153.138:0/3469416727' entity='client.admin' cmd=[{\"pool2\": \"rbd\", \"prefix\": \"osd pool rm\", \"sure\": \"--yes-i-really-really-mean-it\", \"pool\": \"rbd\"}]: dispatch\n2018-06-05 17:14:13.145405 7fc1d020a700  0 mon.node@0(leader) e1 handle_command mon_command({\"prefix\": \"osd pool create\", \"pg_num\": 150, \"pool\": \"scbench\", \"pgp_num\": 150} v 0) v1\n2018-06-05 17:14:13.145481 7fc1d020a700  0 log_channel(audit) log [INF] : from='client.? 128.110.153.138:0/4026107189' entity='client.admin' cmd=[{\"prefix\": \"osd pool create\", \"pg_num\": 150, \"pool\": \"scbench\", \"pgp_num\": 150}]: dispatch\n2018-06-05 17:14:13.330816 7fc1d020a700  1 mon.node@0(leader).osd e1 encode_pending skipping prime_pg_temp; mapping job did not start\n2018-06-05 17:14:13.339231 7fc1cba01700  1 mon.node@0(leader).osd e2 e2: 0 total, 0 up, 0 in\n2018-06-05 17:14:13.343144 7fc1cba01700  0 mon.node@0(leader).osd e2 crush map has features 288514051259236352, adjusting msgr requires\n2018-06-05 17:14:13.343171 7fc1cba01700  0 mon.node@0(leader).osd e2 crush map has features 288514051259236352, adjusting msgr requires\n2018-06-05 17:14:13.343178 7fc1cba01700  0 mon.node@0(leader).osd e2 crush map has features 1009089991638532096, adjusting msgr requires\n2018-06-05 17:14:13.343181 7fc1cba01700  0 mon.node@0(leader).osd e2 crush map has features 288514051259236352, adjusting msgr requires\n2018-06-05 17:14:13.343533 7fc1cba01700  0 log_channel(audit) log [INF] : from='client.? 128.110.153.138:0/4026107189' entity='client.admin' cmd='[{\"prefix\": \"osd pool create\", \"pg_num\": 150, \"pool\": \"scbench\", \"pgp_num\": 150}]': finished\n2018-06-05 17:14:13.343689 7fc1cba01700  0 log_channel(cluster) log [DBG] : osdmap e2: 0 total, 0 up, 0 in\n2018-06-05 17:14:13.352493 7fc1cba01700  1 mon.node@0(leader).log v3 unable to write to '/var/log/ceph/ceph.audit.log' for channel 'audit': (2) No such file or directory\n2018-06-05 17:14:13.352520 7fc1cba01700  1 mon.node@0(leader).log v3 unable to write to '/var/log/ceph/ceph.log' for channel 'cluster': (2) No such file or directory\n2018-06-05 17:14:13.623468 7fc1d020a700  0 mon.node@0(leader) e1 handle_command mon_command({\"var\": \"size\", \"prefix\": \"osd pool set\", \"pool\": \"scbench\", \"val\": \"1\"} v 0) v1\n2018-06-05 17:14:13.623548 7fc1d020a700  0 log_channel(audit) log [INF] : from='client.? 128.110.153.138:0/64503473' entity='client.admin' cmd=[{\"var\": \"size\", \"prefix\": \"osd pool set\", \"pool\": \"scbench\", \"val\": \"1\"}]: dispatch\n2018-06-05 17:14:14.361902 7fc1cba01700  1 mon.node@0(leader).log v4 unable to write to '/var/log/ceph/ceph.audit.log' for channel 'audit': (2) No such file or directory\n2018-06-05 17:14:14.371260 7fc1cba01700  1 mon.node@0(leader).osd e3 e3: 0 total, 0 up, 0 in\n2018-06-05 17:14:14.375931 7fc1cba01700  0 log_channel(audit) log [INF] : from='client.? 128.110.153.138:0/64503473' entity='client.admin' cmd='[{\"var\": \"size\", \"prefix\": \"osd pool set\", \"pool\": \"scbench\", \"val\": \"1\"}]': finished\n2018-06-05 17:14:14.376108 7fc1cba01700  0 log_channel(cluster) log [DBG] : osdmap e3: 0 total, 0 up, 0 in\n2018-06-05 17:14:15.380967 7fc1cba01700  1 mon.node@0(leader).log v5 unable to write to '/var/log/ceph/ceph.audit.log' for channel 'audit': (2) No such file or directory\n2018-06-05 17:15:12.234799 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12381 MB\n2018-06-05 17:16:12.235190 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12381 MB\n2018-06-05 17:17:12.235603 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12381 MB\n2018-06-05 17:18:12.236028 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12381 MB\n2018-06-05 17:19:12.236427 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12381 MB\n2018-06-05 17:20:12.236848 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12381 MB\n2018-06-05 17:21:12.237247 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12381 MB\n2018-06-05 17:22:12.237700 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12381 MB\n2018-06-05 17:23:12.238053 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12381 MB\n2018-06-05 17:24:12.238504 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12381 MB\n2018-06-05 17:25:12.238922 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12381 MB\n2018-06-05 17:26:12.239320 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12381 MB\n2018-06-05 17:27:12.239729 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12381 MB\n2018-06-05 17:28:12.240145 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB\n2018-06-05 17:29:12.240541 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB\n2018-06-05 17:30:12.240932 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB\n2018-06-05 17:31:12.241347 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB\n2018-06-05 17:32:12.241780 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB\n2018-06-05 17:32:50.194245 7fc1d020a700  0 mon.node@0(leader) e1 handle_command mon_command({\"prefix\": \"status\"} v 0) v1\n2018-06-05 17:32:50.194268 7fc1d020a700  0 log_channel(audit) log [DBG] : from='client.? 128.110.153.197:0/3675727699' entity='client.admin' cmd=[{\"prefix\": \"status\"}]: dispatch\n2018-06-05 17:33:12.242174 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB\n2018-06-05 17:34:12.242567 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB\n2018-06-05 17:35:12.243000 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB\n2018-06-05 17:36:12.243399 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB\n2018-06-05 17:37:12.243812 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB\n2018-06-05 17:38:12.244233 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB\n2018-06-05 17:39:12.244669 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB\n2018-06-05 17:40:12.245090 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB\n2018-06-05 17:41:12.245559 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB\n2018-06-05 17:42:12.245970 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB\n2018-06-05 17:43:12.246402 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB\n2018-06-05 17:44:12.246791 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB\n2018-06-05 17:45:12.247219 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB\n2018-06-05 17:46:12.247617 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB\n2018-06-05 17:47:12.248014 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB\n2018-06-05 17:48:12.248391 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB\n2018-06-05 17:49:12.248767 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB\n2018-06-05 17:50:12.249148 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB\n2018-06-05 17:51:12.249533 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB\n2018-06-05 17:52:12.249918 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB\n2018-06-05 17:53:12.250292 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB\n2018-06-05 17:54:12.250713 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB\n2018-06-05 17:55:12.251134 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB\n2018-06-05 17:56:12.251566 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB\n2018-06-05 17:57:12.251957 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB\n2018-06-05 17:58:12.252386 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB\n2018-06-05 17:59:12.252779 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB\n2018-06-05 18:00:12.253164 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB\n2018-06-05 18:01:12.253584 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB\n2018-06-05 18:02:12.254042 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB\n2018-06-05 18:03:12.254440 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB\n2018-06-05 18:04:12.254859 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB\n2018-06-05 18:05:12.255248 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB\n2018-06-05 18:06:12.255630 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB\n2018-06-05 18:07:12.256065 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB\n2018-06-05 18:08:12.256439 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB\n2018-06-05 18:09:12.256858 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB\n2018-06-05 18:10:12.257280 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB\n2018-06-05 18:11:12.257756 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB\n2018-06-05 18:12:12.258147 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB\n2018-06-05 18:13:12.258602 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB\n2018-06-05 18:14:12.258998 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB\n2018-06-05 18:15:12.259427 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB\n2018-06-05 18:16:12.259847 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB\n2018-06-05 18:17:12.260239 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB\n2018-06-05 18:18:12.260625 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB\n2018-06-05 18:19:12.261045 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB\n2018-06-05 18:20:12.261629 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB\n2018-06-05 18:21:12.262016 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB\n2018-06-05 18:22:12.262385 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB\n2018-06-05 18:23:12.262762 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB\n2018-06-05 18:24:12.263139 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB\n2018-06-05 18:25:12.263543 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB\n2018-06-05 18:26:12.263940 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB\n2018-06-05 18:27:12.264329 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB\n2018-06-05 18:28:12.264808 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB\n2018-06-05 18:29:12.265270 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB\n2018-06-05 18:30:12.265761 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB\n2018-06-05 18:31:12.266179 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB\n2018-06-05 18:32:12.266603 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB\n2018-06-05 18:33:12.266983 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB\n2018-06-05 18:34:12.267456 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB\n2018-06-05 18:35:12.267871 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB\n2018-06-05 18:36:12.268283 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB\n2018-06-05 18:37:12.268693 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB\n2018-06-05 18:38:12.269067 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB\n2018-06-05 18:39:12.269471 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB\n2018-06-05 18:40:12.269888 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB\n2018-06-05 18:41:12.270275 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB\n2018-06-05 18:42:12.270671 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB\n2018-06-05 18:43:12.271096 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB\n2018-06-05 18:44:12.271496 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB\n2018-06-05 18:45:12.271892 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB\n2018-06-05 18:46:12.272315 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB\n2018-06-05 18:47:12.272702 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB\n2018-06-05 18:48:12.273130 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB\n2018-06-05 18:49:12.273516 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB\n2018-06-05 18:50:12.273943 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB\n2018-06-05 18:51:12.274326 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB\n2018-06-05 18:52:12.274697 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB\n2018-06-05 18:53:12.275087 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB\n2018-06-05 18:54:12.275519 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB\n2018-06-05 18:55:12.275943 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB\n2018-06-05 18:55:12.948939 7fc1cda05700 -1 received  signal: Terminated from  PID: 1 task name:  UID: 0\n2018-06-05 18:55:12.948970 7fc1cda05700 -1 mon.node@0(leader) e1 *** Got Signal Terminated ***\n2018-06-05 18:55:12.948977 7fc1cda05700  1 mon.node@0(leader) e1 shutdown", "stderr_lines": ["+ '[' -z True ']'", "+ '[' -z 128.110.152.0/22 ']'", "+ '[' -z 128.110.153.197 ']'", "+ sed -i -e s@monip@128.110.153.197@ /etc/ceph/ceph.conf", "+ sed -i -e s@cephnet@128.110.152.0/22@ /etc/ceph/ceph.conf", "+ '[' -f /etc/ceph/extra.conf ']'", "+ '[' -n True ']'", "+ '[' -n '' ']'", "+ exec /entrypoint.sh mon", "2018-06-05 17:14:12.196603 7fc1d93f7f00  0 set uid:gid to 64045:64045 (ceph:ceph)", "2018-06-05 17:14:12.196620 7fc1d93f7f00  0 ceph version 12.2.5 (cad919881333ac92274171586c827e01f554a70a) luminous (stable), process (unknown), pid 122", "2018-06-05 17:14:12.196712 7fc1d93f7f00  0 pidfile_write: ignore empty --pid-file", "2018-06-05 17:14:12.204633 7fc1d93f7f00  0 load: jerasure load: lrc load: isa ", "2018-06-05 17:14:12.204738 7fc1d93f7f00  0  set rocksdb option compression = kNoCompression", "2018-06-05 17:14:12.204751 7fc1d93f7f00  0  set rocksdb option write_buffer_size = 33554432", "2018-06-05 17:14:12.204770 7fc1d93f7f00  0  set rocksdb option compression = kNoCompression", "2018-06-05 17:14:12.204774 7fc1d93f7f00  0  set rocksdb option write_buffer_size = 33554432", "2018-06-05 17:14:12.204894 7fc1d93f7f00  4 rocksdb: RocksDB version: 5.4.0", "", "2018-06-05 17:14:12.204906 7fc1d93f7f00  4 rocksdb: Git sha rocksdb_build_git_sha:@0@", "2018-06-05 17:14:12.204907 7fc1d93f7f00  4 rocksdb: Compile date Apr 23 2018", "2018-06-05 17:14:12.204909 7fc1d93f7f00  4 rocksdb: DB SUMMARY", "", "2018-06-05 17:14:12.204955 7fc1d93f7f00  4 rocksdb: CURRENT file:  CURRENT", "", "2018-06-05 17:14:12.204957 7fc1d93f7f00  4 rocksdb: IDENTITY file:  IDENTITY", "", "2018-06-05 17:14:12.204965 7fc1d93f7f00  4 rocksdb: MANIFEST file:  MANIFEST-000001 size: 13 Bytes", "", "2018-06-05 17:14:12.204967 7fc1d93f7f00  4 rocksdb: SST files in /var/lib/ceph/mon/ceph-node/store.db dir, Total Num: 0, files: ", "", "2018-06-05 17:14:12.204969 7fc1d93f7f00  4 rocksdb: Write Ahead Log file in /var/lib/ceph/mon/ceph-node/store.db: 000003.log size: 454 ; ", "", "2018-06-05 17:14:12.204971 7fc1d93f7f00  4 rocksdb:                         Options.error_if_exists: 0", "2018-06-05 17:14:12.204972 7fc1d93f7f00  4 rocksdb:                       Options.create_if_missing: 0", "2018-06-05 17:14:12.204973 7fc1d93f7f00  4 rocksdb:                         Options.paranoid_checks: 1", "2018-06-05 17:14:12.204973 7fc1d93f7f00  4 rocksdb:                                     Options.env: 0x55e8c9a64020", "2018-06-05 17:14:12.204974 7fc1d93f7f00  4 rocksdb:                                Options.info_log: 0x55e8cba38c80", "2018-06-05 17:14:12.204976 7fc1d93f7f00  4 rocksdb:                          Options.max_open_files: -1", "2018-06-05 17:14:12.204977 7fc1d93f7f00  4 rocksdb:                Options.max_file_opening_threads: 16", "2018-06-05 17:14:12.204977 7fc1d93f7f00  4 rocksdb:                               Options.use_fsync: 0", "2018-06-05 17:14:12.204978 7fc1d93f7f00  4 rocksdb:                       Options.max_log_file_size: 0", "2018-06-05 17:14:12.204979 7fc1d93f7f00  4 rocksdb:                  Options.max_manifest_file_size: 18446744073709551615", "2018-06-05 17:14:12.204980 7fc1d93f7f00  4 rocksdb:                   Options.log_file_time_to_roll: 0", "2018-06-05 17:14:12.204981 7fc1d93f7f00  4 rocksdb:                       Options.keep_log_file_num: 1000", "2018-06-05 17:14:12.204982 7fc1d93f7f00  4 rocksdb:                    Options.recycle_log_file_num: 0", "2018-06-05 17:14:12.204990 7fc1d93f7f00  4 rocksdb:                         Options.allow_fallocate: 1", "2018-06-05 17:14:12.204990 7fc1d93f7f00  4 rocksdb:                        Options.allow_mmap_reads: 0", "2018-06-05 17:14:12.204991 7fc1d93f7f00  4 rocksdb:                       Options.allow_mmap_writes: 0", "2018-06-05 17:14:12.204992 7fc1d93f7f00  4 rocksdb:                        Options.use_direct_reads: 0", "2018-06-05 17:14:12.204993 7fc1d93f7f00  4 rocksdb:                        Options.use_direct_io_for_flush_and_compaction: 0", "2018-06-05 17:14:12.204994 7fc1d93f7f00  4 rocksdb:          Options.create_missing_column_families: 0", "2018-06-05 17:14:12.204995 7fc1d93f7f00  4 rocksdb:                              Options.db_log_dir: ", "2018-06-05 17:14:12.204995 7fc1d93f7f00  4 rocksdb:                                 Options.wal_dir: /var/lib/ceph/mon/ceph-node/store.db", "2018-06-05 17:14:12.204996 7fc1d93f7f00  4 rocksdb:                Options.table_cache_numshardbits: 6", "2018-06-05 17:14:12.204997 7fc1d93f7f00  4 rocksdb:                      Options.max_subcompactions: 1", "2018-06-05 17:14:12.204998 7fc1d93f7f00  4 rocksdb:                  Options.max_background_flushes: 1", "2018-06-05 17:14:12.204999 7fc1d93f7f00  4 rocksdb:                         Options.WAL_ttl_seconds: 0", "2018-06-05 17:14:12.205000 7fc1d93f7f00  4 rocksdb:                       Options.WAL_size_limit_MB: 0", "2018-06-05 17:14:12.205001 7fc1d93f7f00  4 rocksdb:             Options.manifest_preallocation_size: 4194304", "2018-06-05 17:14:12.205002 7fc1d93f7f00  4 rocksdb:                     Options.is_fd_close_on_exec: 1", "2018-06-05 17:14:12.205002 7fc1d93f7f00  4 rocksdb:                   Options.advise_random_on_open: 1", "2018-06-05 17:14:12.205003 7fc1d93f7f00  4 rocksdb:                    Options.db_write_buffer_size: 0", "2018-06-05 17:14:12.205004 7fc1d93f7f00  4 rocksdb:         Options.access_hint_on_compaction_start: 1", "2018-06-05 17:14:12.205005 7fc1d93f7f00  4 rocksdb:  Options.new_table_reader_for_compaction_inputs: 0", "2018-06-05 17:14:12.205005 7fc1d93f7f00  4 rocksdb:               Options.compaction_readahead_size: 0", "2018-06-05 17:14:12.205006 7fc1d93f7f00  4 rocksdb:           Options.random_access_max_buffer_size: 1048576", "2018-06-05 17:14:12.205007 7fc1d93f7f00  4 rocksdb:           Options.writable_file_max_buffer_size: 1048576", "2018-06-05 17:14:12.205008 7fc1d93f7f00  4 rocksdb:                      Options.use_adaptive_mutex: 0", "2018-06-05 17:14:12.205009 7fc1d93f7f00  4 rocksdb:                            Options.rate_limiter: (nil)", "2018-06-05 17:14:12.205010 7fc1d93f7f00  4 rocksdb:     Options.sst_file_manager.rate_bytes_per_sec: 0", "2018-06-05 17:14:12.205011 7fc1d93f7f00  4 rocksdb:                          Options.bytes_per_sync: 0", "2018-06-05 17:14:12.205012 7fc1d93f7f00  4 rocksdb:                      Options.wal_bytes_per_sync: 0", "2018-06-05 17:14:12.205013 7fc1d93f7f00  4 rocksdb:                       Options.wal_recovery_mode: 2", "2018-06-05 17:14:12.205013 7fc1d93f7f00  4 rocksdb:                  Options.enable_thread_tracking: 0", "2018-06-05 17:14:12.205014 7fc1d93f7f00  4 rocksdb:         Options.allow_concurrent_memtable_write: 1", "2018-06-05 17:14:12.205015 7fc1d93f7f00  4 rocksdb:      Options.enable_write_thread_adaptive_yield: 1", "2018-06-05 17:14:12.205016 7fc1d93f7f00  4 rocksdb:             Options.write_thread_max_yield_usec: 100", "2018-06-05 17:14:12.205016 7fc1d93f7f00  4 rocksdb:            Options.write_thread_slow_yield_usec: 3", "2018-06-05 17:14:12.205017 7fc1d93f7f00  4 rocksdb:                               Options.row_cache: None", "2018-06-05 17:14:12.205018 7fc1d93f7f00  4 rocksdb:                              Options.wal_filter: None", "2018-06-05 17:14:12.205019 7fc1d93f7f00  4 rocksdb:             Options.avoid_flush_during_recovery: 0", "2018-06-05 17:14:12.205020 7fc1d93f7f00  4 rocksdb:             Options.base_background_compactions: 1", "2018-06-05 17:14:12.205021 7fc1d93f7f00  4 rocksdb:             Options.max_background_compactions: 1", "2018-06-05 17:14:12.205021 7fc1d93f7f00  4 rocksdb:             Options.avoid_flush_during_shutdown: 0", "2018-06-05 17:14:12.205022 7fc1d93f7f00  4 rocksdb:             Options.delayed_write_rate : 16777216", "2018-06-05 17:14:12.205023 7fc1d93f7f00  4 rocksdb:             Options.max_total_wal_size: 0", "2018-06-05 17:14:12.205024 7fc1d93f7f00  4 rocksdb:             Options.delete_obsolete_files_period_micros: 21600000000", "2018-06-05 17:14:12.205025 7fc1d93f7f00  4 rocksdb:                   Options.stats_dump_period_sec: 600", "2018-06-05 17:14:12.205026 7fc1d93f7f00  4 rocksdb: Compression algorithms supported:", "2018-06-05 17:14:12.205027 7fc1d93f7f00  4 rocksdb: \tSnappy supported: 0", "2018-06-05 17:14:12.205028 7fc1d93f7f00  4 rocksdb: \tZlib supported: 0", "2018-06-05 17:14:12.205029 7fc1d93f7f00  4 rocksdb: \tBzip supported: 0", "2018-06-05 17:14:12.205029 7fc1d93f7f00  4 rocksdb: \tLZ4 supported: 0", "2018-06-05 17:14:12.205030 7fc1d93f7f00  4 rocksdb: \tZSTD supported: 0", "2018-06-05 17:14:12.205031 7fc1d93f7f00  4 rocksdb: Fast CRC32 supported: 1", "2018-06-05 17:14:12.205143 7fc1d93f7f00  4 rocksdb: [/build/ceph-12.2.5/src/rocksdb/db/version_set.cc:2609] Recovering from manifest file: MANIFEST-000001", "", "2018-06-05 17:14:12.205197 7fc1d93f7f00  4 rocksdb: [/build/ceph-12.2.5/src/rocksdb/db/column_family.cc:407] --------------- Options for column family [default]:", "", "2018-06-05 17:14:12.205199 7fc1d93f7f00  4 rocksdb:               Options.comparator: leveldb.BytewiseComparator", "2018-06-05 17:14:12.205201 7fc1d93f7f00  4 rocksdb:           Options.merge_operator: ", "2018-06-05 17:14:12.205202 7fc1d93f7f00  4 rocksdb:        Options.compaction_filter: None", "2018-06-05 17:14:12.205203 7fc1d93f7f00  4 rocksdb:        Options.compaction_filter_factory: None", "2018-06-05 17:14:12.205204 7fc1d93f7f00  4 rocksdb:         Options.memtable_factory: SkipListFactory", "2018-06-05 17:14:12.205205 7fc1d93f7f00  4 rocksdb:            Options.table_factory: BlockBasedTable", "2018-06-05 17:14:12.205225 7fc1d93f7f00  4 rocksdb:            table_factory options:   flush_block_policy_factory: FlushBlockBySizePolicyFactory (0x55e8cb7140f8)", "  cache_index_and_filter_blocks: 1", "  cache_index_and_filter_blocks_with_high_priority: 1", "  pin_l0_filter_and_index_blocks_in_cache: 1", "  index_type: 0", "  hash_index_allow_collision: 1", "  checksum: 1", "  no_block_cache: 0", "  block_cache: 0x55e8cba205c0", "  block_cache_name: LRUCache", "  block_cache_options:", "    capacity : 134217728", "    num_shard_bits : 4", "    strict_capacity_limit : 0", "    high_pri_pool_ratio: 0.000", "  block_cache_compressed: (nil)", "  persistent_cache: (nil)", "  block_size: 4096", "  block_size_deviation: 10", "  block_restart_interval: 16", "  index_block_restart_interval: 1", "  filter_policy: rocksdb.BuiltinBloomFilter", "  whole_key_filtering: 1", "  format_version: 2", "", "2018-06-05 17:14:12.205230 7fc1d93f7f00  4 rocksdb:        Options.write_buffer_size: 33554432", "2018-06-05 17:14:12.205231 7fc1d93f7f00  4 rocksdb:  Options.max_write_buffer_number: 2", "2018-06-05 17:14:12.205232 7fc1d93f7f00  4 rocksdb:          Options.compression: NoCompression", "2018-06-05 17:14:12.205233 7fc1d93f7f00  4 rocksdb:                  Options.bottommost_compression: Disabled", "2018-06-05 17:14:12.205233 7fc1d93f7f00  4 rocksdb:       Options.prefix_extractor: nullptr", "2018-06-05 17:14:12.205240 7fc1d93f7f00  4 rocksdb:   Options.memtable_insert_with_hint_prefix_extractor: nullptr", "2018-06-05 17:14:12.205241 7fc1d93f7f00  4 rocksdb:             Options.num_levels: 7", "2018-06-05 17:14:12.205242 7fc1d93f7f00  4 rocksdb:        Options.min_write_buffer_number_to_merge: 1", "2018-06-05 17:14:12.205242 7fc1d93f7f00  4 rocksdb:     Options.max_write_buffer_number_to_maintain: 0", "2018-06-05 17:14:12.205243 7fc1d93f7f00  4 rocksdb:            Options.compression_opts.window_bits: -14", "2018-06-05 17:14:12.205244 7fc1d93f7f00  4 rocksdb:                  Options.compression_opts.level: -1", "2018-06-05 17:14:12.205245 7fc1d93f7f00  4 rocksdb:               Options.compression_opts.strategy: 0", "2018-06-05 17:14:12.205246 7fc1d93f7f00  4 rocksdb:         Options.compression_opts.max_dict_bytes: 0", "2018-06-05 17:14:12.205246 7fc1d93f7f00  4 rocksdb:      Options.level0_file_num_compaction_trigger: 4", "2018-06-05 17:14:12.205247 7fc1d93f7f00  4 rocksdb:          Options.level0_slowdown_writes_trigger: 20", "2018-06-05 17:14:12.205248 7fc1d93f7f00  4 rocksdb:              Options.level0_stop_writes_trigger: 36", "2018-06-05 17:14:12.205248 7fc1d93f7f00  4 rocksdb:                   Options.target_file_size_base: 67108864", "2018-06-05 17:14:12.205249 7fc1d93f7f00  4 rocksdb:             Options.target_file_size_multiplier: 1", "2018-06-05 17:14:12.205250 7fc1d93f7f00  4 rocksdb:                Options.max_bytes_for_level_base: 268435456", "2018-06-05 17:14:12.205250 7fc1d93f7f00  4 rocksdb: Options.level_compaction_dynamic_level_bytes: 0", "2018-06-05 17:14:12.205251 7fc1d93f7f00  4 rocksdb:          Options.max_bytes_for_level_multiplier: 10.000000", "2018-06-05 17:14:12.205254 7fc1d93f7f00  4 rocksdb: Options.max_bytes_for_level_multiplier_addtl[0]: 1", "2018-06-05 17:14:12.205255 7fc1d93f7f00  4 rocksdb: Options.max_bytes_for_level_multiplier_addtl[1]: 1", "2018-06-05 17:14:12.205256 7fc1d93f7f00  4 rocksdb: Options.max_bytes_for_level_multiplier_addtl[2]: 1", "2018-06-05 17:14:12.205256 7fc1d93f7f00  4 rocksdb: Options.max_bytes_for_level_multiplier_addtl[3]: 1", "2018-06-05 17:14:12.205257 7fc1d93f7f00  4 rocksdb: Options.max_bytes_for_level_multiplier_addtl[4]: 1", "2018-06-05 17:14:12.205258 7fc1d93f7f00  4 rocksdb: Options.max_bytes_for_level_multiplier_addtl[5]: 1", "2018-06-05 17:14:12.205258 7fc1d93f7f00  4 rocksdb: Options.max_bytes_for_level_multiplier_addtl[6]: 1", "2018-06-05 17:14:12.205259 7fc1d93f7f00  4 rocksdb:       Options.max_sequential_skip_in_iterations: 8", "2018-06-05 17:14:12.205260 7fc1d93f7f00  4 rocksdb:                    Options.max_compaction_bytes: 1677721600", "2018-06-05 17:14:12.205260 7fc1d93f7f00  4 rocksdb:                        Options.arena_block_size: 4194304", "2018-06-05 17:14:12.205261 7fc1d93f7f00  4 rocksdb:   Options.soft_pending_compaction_bytes_limit: 68719476736", "2018-06-05 17:14:12.205262 7fc1d93f7f00  4 rocksdb:   Options.hard_pending_compaction_bytes_limit: 274877906944", "2018-06-05 17:14:12.205263 7fc1d93f7f00  4 rocksdb:       Options.rate_limit_delay_max_milliseconds: 100", "2018-06-05 17:14:12.205263 7fc1d93f7f00  4 rocksdb:                Options.disable_auto_compactions: 0", "2018-06-05 17:14:12.205264 7fc1d93f7f00  4 rocksdb:                         Options.compaction_style: kCompactionStyleLevel", "2018-06-05 17:14:12.205266 7fc1d93f7f00  4 rocksdb:                           Options.compaction_pri: kByCompensatedSize", "2018-06-05 17:14:12.205266 7fc1d93f7f00  4 rocksdb:  Options.compaction_options_universal.size_ratio: 1", "2018-06-05 17:14:12.205267 7fc1d93f7f00  4 rocksdb: Options.compaction_options_universal.min_merge_width: 2", "2018-06-05 17:14:12.205268 7fc1d93f7f00  4 rocksdb: Options.compaction_options_universal.max_merge_width: 4294967295", "2018-06-05 17:14:12.205269 7fc1d93f7f00  4 rocksdb: Options.compaction_options_universal.max_size_amplification_percent: 200", "2018-06-05 17:14:12.205270 7fc1d93f7f00  4 rocksdb: Options.compaction_options_universal.compression_size_percent: -1", "2018-06-05 17:14:12.205271 7fc1d93f7f00  4 rocksdb: Options.compaction_options_fifo.max_table_files_size: 1073741824", "2018-06-05 17:14:12.205271 7fc1d93f7f00  4 rocksdb:                   Options.table_properties_collectors: ", "2018-06-05 17:14:12.205272 7fc1d93f7f00  4 rocksdb:                   Options.inplace_update_support: 0", "2018-06-05 17:14:12.205273 7fc1d93f7f00  4 rocksdb:                 Options.inplace_update_num_locks: 10000", "2018-06-05 17:14:12.205274 7fc1d93f7f00  4 rocksdb:               Options.memtable_prefix_bloom_size_ratio: 0.000000", "2018-06-05 17:14:12.205275 7fc1d93f7f00  4 rocksdb:   Options.memtable_huge_page_size: 0", "2018-06-05 17:14:12.205275 7fc1d93f7f00  4 rocksdb:                           Options.bloom_locality: 0", "2018-06-05 17:14:12.205276 7fc1d93f7f00  4 rocksdb:                    Options.max_successive_merges: 0", "2018-06-05 17:14:12.205277 7fc1d93f7f00  4 rocksdb:                Options.optimize_filters_for_hits: 0", "2018-06-05 17:14:12.205278 7fc1d93f7f00  4 rocksdb:                Options.paranoid_file_checks: 0", "2018-06-05 17:14:12.205278 7fc1d93f7f00  4 rocksdb:                Options.force_consistency_checks: 0", "2018-06-05 17:14:12.205279 7fc1d93f7f00  4 rocksdb:                Options.report_bg_io_stats: 0", "2018-06-05 17:14:12.206750 7fc1d93f7f00  4 rocksdb: [/build/ceph-12.2.5/src/rocksdb/db/version_set.cc:2859] Recovered from manifest file:/var/lib/ceph/mon/ceph-node/store.db/MANIFEST-000001 succeeded,manifest_file_number is 1, next_file_number is 3, last_sequence is 0, log_number is 0,prev_log_number is 0,max_column_family is 0", "", "2018-06-05 17:14:12.206761 7fc1d93f7f00  4 rocksdb: [/build/ceph-12.2.5/src/rocksdb/db/version_set.cc:2867] Column family [default] (ID 0), log number is 0", "", "2018-06-05 17:14:12.206822 7fc1d93f7f00  4 rocksdb: EVENT_LOG_v1 {\"time_micros\": 1528218852206812, \"job\": 1, \"event\": \"recovery_started\", \"log_files\": [3]}", "2018-06-05 17:14:12.206827 7fc1d93f7f00  4 rocksdb: [/build/ceph-12.2.5/src/rocksdb/db/db_impl_open.cc:482] Recovering log #3 mode 2", "2018-06-05 17:14:12.211571 7fc1d93f7f00  4 rocksdb: EVENT_LOG_v1 {\"time_micros\": 1528218852211559, \"cf_name\": \"default\", \"job\": 1, \"event\": \"table_file_creation\", \"file_number\": 4, \"file_size\": 1420, \"table_properties\": {\"data_size\": 463, \"index_size\": 28, \"filter_size\": 20, \"raw_key_size\": 95, \"raw_average_key_size\": 23, \"raw_value_size\": 359, \"raw_average_value_size\": 89, \"num_data_blocks\": 1, \"num_entries\": 4, \"filter_policy_name\": \"rocksdb.BuiltinBloomFilter\", \"kDeletedKeys\": \"0\", \"kMergeOperands\": \"0\"}}", "2018-06-05 17:14:12.211601 7fc1d93f7f00  4 rocksdb: [/build/ceph-12.2.5/src/rocksdb/db/version_set.cc:2395] Creating manifest 5", "", "2018-06-05 17:14:12.220227 7fc1d93f7f00  4 rocksdb: EVENT_LOG_v1 {\"time_micros\": 1528218852220222, \"job\": 1, \"event\": \"recovery_finished\"}", "2018-06-05 17:14:12.230977 7fc1d93f7f00  4 rocksdb: [/build/ceph-12.2.5/src/rocksdb/db/db_impl_open.cc:1063] DB pointer 0x55e8cbb34000", "2018-06-05 17:14:12.232743 7fc1d93f7f00  0 starting mon.node rank 0 at public addr 128.110.153.197:6789/0 at bind addr 128.110.153.197:6789/0 mon_data /var/lib/ceph/mon/ceph-node fsid b2bb6c0b-6cc6-44f1-bf92-1ce81d78fbdc", "2018-06-05 17:14:12.232990 7fc1d93f7f00  0 starting mon.node rank 0 at 128.110.153.197:6789/0 mon_data /var/lib/ceph/mon/ceph-node fsid b2bb6c0b-6cc6-44f1-bf92-1ce81d78fbdc", "2018-06-05 17:14:12.234128 7fc1d93f7f00  1 mon.node@-1(probing) e0 preinit fsid b2bb6c0b-6cc6-44f1-bf92-1ce81d78fbdc", "2018-06-05 17:14:12.234361 7fc1d93f7f00  1 mon.node@-1(probing).mds e0 Unable to load 'last_metadata'", "2018-06-05 17:14:12.235253 7fc1d93f7f00  0 mon.node@-1(probing) e0  my rank is now 0 (was -1)", "2018-06-05 17:14:12.235281 7fc1d93f7f00  1 mon.node@0(probing) e0 win_standalone_election", "2018-06-05 17:14:12.235303 7fc1d93f7f00  1 mon.node@0(probing).elector(0) init, first boot, initializing epoch at 1 ", "2018-06-05 17:14:12.244200 7fc1d93f7f00  0 log_channel(cluster) log [INF] : mon.node is new leader, mons node in quorum (ranks 0)", "2018-06-05 17:14:12.248559 7fc1d93f7f00  1 mon.node@0(leader).osd e0 create_pending setting backfillfull_ratio = 0.9", "2018-06-05 17:14:12.248598 7fc1d93f7f00  1 mon.node@0(leader).osd e0 create_pending setting full_ratio = 0.95", "2018-06-05 17:14:12.248603 7fc1d93f7f00  1 mon.node@0(leader).osd e0 create_pending setting nearfull_ratio = 0.85", "2018-06-05 17:14:12.248974 7fc1d93f7f00  1 mon.node@0(leader).osd e0 encode_pending skipping prime_pg_temp; mapping job did not start", "2018-06-05 17:14:12.250812 7fc1d93f7f00  1 mon.node@0(leader) e0 _apply_compatset_features enabling new quorum features: compat={},rocompat={},incompat={4=support erasure code pools,5=new-style osdmap encoding,6=support isa/lrc erasure code,7=support shec erasure code}", "2018-06-05 17:14:12.257261 7fc1cba01700  1 mon.node@0(leader).paxosservice(pgmap 0..0) refresh upgraded, format 1 -> 0", "2018-06-05 17:14:12.257275 7fc1cba01700  1 mon.node@0(leader).pg v0 on_upgrade discarding in-core PGMap", "2018-06-05 17:14:12.262914 7fc1cba01700  1 mon.node@0(leader).paxosservice(auth 0..0) refresh upgraded, format 2 -> 0", "2018-06-05 17:14:12.263130 7fc1cba01700  1 mon.node@0(probing) e1 win_standalone_election", "2018-06-05 17:14:12.263142 7fc1cba01700  1 mon.node@0(probing).elector(2) init, last seen epoch 2", "2018-06-05 17:14:12.267695 7fc1cba01700  0 log_channel(cluster) log [INF] : mon.node is new leader, mons node in quorum (ranks 0)", "2018-06-05 17:14:12.272050 7fc1cba01700  0 log_channel(cluster) log [DBG] : monmap e1: 1 mons at {node=128.110.153.197:6789/0}", "2018-06-05 17:14:12.276445 7fc1cba01700  1 mon.node@0(leader).osd e0 create_pending setting backfillfull_ratio = 0.9", "2018-06-05 17:14:12.276460 7fc1cba01700  1 mon.node@0(leader).osd e0 create_pending setting full_ratio = 0.95", "2018-06-05 17:14:12.276467 7fc1cba01700  1 mon.node@0(leader).osd e0 create_pending setting nearfull_ratio = 0.85", "2018-06-05 17:14:12.276666 7fc1cba01700  1 mon.node@0(leader).osd e0 encode_pending skipping prime_pg_temp; mapping job did not start", "2018-06-05 17:14:12.277309 7fc1cba01700  1 mon.node@0(leader) e1 _apply_compatset_features enabling new quorum features: compat={},rocompat={},incompat={8=support monmap features,9=luminous ondisk layout}", "2018-06-05 17:14:12.286691 7fc1cba01700  1 mon.node@0(leader).paxosservice(auth 0..0) refresh upgraded, format 2 -> 0", "2018-06-05 17:14:12.286805 7fc1cba01700  0 log_channel(cluster) log [INF] : pgmap 0 pgs: ; 0 bytes data, 0 kB used, 0 kB / 0 kB avail", "2018-06-05 17:14:12.295855 7fc1cba01700  0 mon.node@0(leader).mds e1 print_map", "e1", "enable_multiple, ever_enabled_multiple: 0,0", "compat: compat={},rocompat={},incompat={1=base v0.20,2=client writeable ranges,3=default file layouts on dirs,4=dir inode in separate object,5=mds uses versioned encoding,6=dirfrag is stored in omap,8=no anchor table,9=file layout v2}", "legacy client fscid: -1", " ", "No filesystems configured", "", "2018-06-05 17:14:12.296024 7fc1cba01700  1 mon.node@0(leader).osd e1 e1: 0 total, 0 up, 0 in", "2018-06-05 17:14:12.300502 7fc1cba01700  0 mon.node@0(leader).osd e1 crush map has features 288514050185494528, adjusting msgr requires", "2018-06-05 17:14:12.300507 7fc1cba01700  0 mon.node@0(leader).osd e1 crush map has features 288514050185494528, adjusting msgr requires", "2018-06-05 17:14:12.300511 7fc1cba01700  0 mon.node@0(leader).osd e1 crush map has features 1009089990564790272, adjusting msgr requires", "2018-06-05 17:14:12.300513 7fc1cba01700  0 mon.node@0(leader).osd e1 crush map has features 288514050185494528, adjusting msgr requires", "2018-06-05 17:14:12.300660 7fc1cba01700  1 mon.node@0(leader).log v1 unable to write to '/var/log/ceph/ceph.log' for channel 'cluster': (2) No such file or directory", "2018-06-05 17:14:12.300727 7fc1cba01700  1 mon.node@0(leader).paxosservice(auth 1..1) refresh upgraded, format 0 -> 2", "2018-06-05 17:14:12.301408 7fc1cba01700  0 log_channel(cluster) log [DBG] : fsmap ", "2018-06-05 17:14:12.301723 7fc1cba01700  1 mon.node@0(leader).pg v1 check_osd_map will clear pg_map data", "2018-06-05 17:14:12.301728 7fc1cba01700  1 mon.node@0(leader).pg v1 encode_pending clearing pgmap data at v2", "2018-06-05 17:14:12.305949 7fc1cba01700  0 log_channel(cluster) log [DBG] : osdmap e1: 0 total, 0 up, 0 in", "2018-06-05 17:14:12.306078 7fc1cba01700  0 log_channel(cluster) log [DBG] : mgrmap e1: no daemons active", "2018-06-05 17:14:12.319794 7fc1cba01700  1 mon.node@0(leader).log v2 unable to write to '/var/log/ceph/ceph.log' for channel 'cluster': (2) No such file or directory", "2018-06-05 17:14:12.568034 7fc1d020a700  0 mon.node@0(leader) e1 handle_command mon_command({\"prefix\": \"status\"} v 0) v1", "2018-06-05 17:14:12.568057 7fc1d020a700  0 log_channel(audit) log [DBG] : from='client.? 128.110.153.138:0/2977579145' entity='client.admin' cmd=[{\"prefix\": \"status\"}]: dispatch", "2018-06-05 17:14:12.857199 7fc1d020a700  0 mon.node@0(leader) e1 handle_command mon_command({\"pool2\": \"rbd\", \"prefix\": \"osd pool rm\", \"sure\": \"--yes-i-really-really-mean-it\", \"pool\": \"rbd\"} v 0) v1", "2018-06-05 17:14:12.857274 7fc1d020a700  0 log_channel(audit) log [INF] : from='client.? 128.110.153.138:0/3469416727' entity='client.admin' cmd=[{\"pool2\": \"rbd\", \"prefix\": \"osd pool rm\", \"sure\": \"--yes-i-really-really-mean-it\", \"pool\": \"rbd\"}]: dispatch", "2018-06-05 17:14:13.145405 7fc1d020a700  0 mon.node@0(leader) e1 handle_command mon_command({\"prefix\": \"osd pool create\", \"pg_num\": 150, \"pool\": \"scbench\", \"pgp_num\": 150} v 0) v1", "2018-06-05 17:14:13.145481 7fc1d020a700  0 log_channel(audit) log [INF] : from='client.? 128.110.153.138:0/4026107189' entity='client.admin' cmd=[{\"prefix\": \"osd pool create\", \"pg_num\": 150, \"pool\": \"scbench\", \"pgp_num\": 150}]: dispatch", "2018-06-05 17:14:13.330816 7fc1d020a700  1 mon.node@0(leader).osd e1 encode_pending skipping prime_pg_temp; mapping job did not start", "2018-06-05 17:14:13.339231 7fc1cba01700  1 mon.node@0(leader).osd e2 e2: 0 total, 0 up, 0 in", "2018-06-05 17:14:13.343144 7fc1cba01700  0 mon.node@0(leader).osd e2 crush map has features 288514051259236352, adjusting msgr requires", "2018-06-05 17:14:13.343171 7fc1cba01700  0 mon.node@0(leader).osd e2 crush map has features 288514051259236352, adjusting msgr requires", "2018-06-05 17:14:13.343178 7fc1cba01700  0 mon.node@0(leader).osd e2 crush map has features 1009089991638532096, adjusting msgr requires", "2018-06-05 17:14:13.343181 7fc1cba01700  0 mon.node@0(leader).osd e2 crush map has features 288514051259236352, adjusting msgr requires", "2018-06-05 17:14:13.343533 7fc1cba01700  0 log_channel(audit) log [INF] : from='client.? 128.110.153.138:0/4026107189' entity='client.admin' cmd='[{\"prefix\": \"osd pool create\", \"pg_num\": 150, \"pool\": \"scbench\", \"pgp_num\": 150}]': finished", "2018-06-05 17:14:13.343689 7fc1cba01700  0 log_channel(cluster) log [DBG] : osdmap e2: 0 total, 0 up, 0 in", "2018-06-05 17:14:13.352493 7fc1cba01700  1 mon.node@0(leader).log v3 unable to write to '/var/log/ceph/ceph.audit.log' for channel 'audit': (2) No such file or directory", "2018-06-05 17:14:13.352520 7fc1cba01700  1 mon.node@0(leader).log v3 unable to write to '/var/log/ceph/ceph.log' for channel 'cluster': (2) No such file or directory", "2018-06-05 17:14:13.623468 7fc1d020a700  0 mon.node@0(leader) e1 handle_command mon_command({\"var\": \"size\", \"prefix\": \"osd pool set\", \"pool\": \"scbench\", \"val\": \"1\"} v 0) v1", "2018-06-05 17:14:13.623548 7fc1d020a700  0 log_channel(audit) log [INF] : from='client.? 128.110.153.138:0/64503473' entity='client.admin' cmd=[{\"var\": \"size\", \"prefix\": \"osd pool set\", \"pool\": \"scbench\", \"val\": \"1\"}]: dispatch", "2018-06-05 17:14:14.361902 7fc1cba01700  1 mon.node@0(leader).log v4 unable to write to '/var/log/ceph/ceph.audit.log' for channel 'audit': (2) No such file or directory", "2018-06-05 17:14:14.371260 7fc1cba01700  1 mon.node@0(leader).osd e3 e3: 0 total, 0 up, 0 in", "2018-06-05 17:14:14.375931 7fc1cba01700  0 log_channel(audit) log [INF] : from='client.? 128.110.153.138:0/64503473' entity='client.admin' cmd='[{\"var\": \"size\", \"prefix\": \"osd pool set\", \"pool\": \"scbench\", \"val\": \"1\"}]': finished", "2018-06-05 17:14:14.376108 7fc1cba01700  0 log_channel(cluster) log [DBG] : osdmap e3: 0 total, 0 up, 0 in", "2018-06-05 17:14:15.380967 7fc1cba01700  1 mon.node@0(leader).log v5 unable to write to '/var/log/ceph/ceph.audit.log' for channel 'audit': (2) No such file or directory", "2018-06-05 17:15:12.234799 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12381 MB", "2018-06-05 17:16:12.235190 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12381 MB", "2018-06-05 17:17:12.235603 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12381 MB", "2018-06-05 17:18:12.236028 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12381 MB", "2018-06-05 17:19:12.236427 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12381 MB", "2018-06-05 17:20:12.236848 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12381 MB", "2018-06-05 17:21:12.237247 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12381 MB", "2018-06-05 17:22:12.237700 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12381 MB", "2018-06-05 17:23:12.238053 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12381 MB", "2018-06-05 17:24:12.238504 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12381 MB", "2018-06-05 17:25:12.238922 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12381 MB", "2018-06-05 17:26:12.239320 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12381 MB", "2018-06-05 17:27:12.239729 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2797 MB, avail 12381 MB", "2018-06-05 17:28:12.240145 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB", "2018-06-05 17:29:12.240541 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB", "2018-06-05 17:30:12.240932 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB", "2018-06-05 17:31:12.241347 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB", "2018-06-05 17:32:12.241780 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB", "2018-06-05 17:32:50.194245 7fc1d020a700  0 mon.node@0(leader) e1 handle_command mon_command({\"prefix\": \"status\"} v 0) v1", "2018-06-05 17:32:50.194268 7fc1d020a700  0 log_channel(audit) log [DBG] : from='client.? 128.110.153.197:0/3675727699' entity='client.admin' cmd=[{\"prefix\": \"status\"}]: dispatch", "2018-06-05 17:33:12.242174 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB", "2018-06-05 17:34:12.242567 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB", "2018-06-05 17:35:12.243000 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB", "2018-06-05 17:36:12.243399 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB", "2018-06-05 17:37:12.243812 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB", "2018-06-05 17:38:12.244233 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB", "2018-06-05 17:39:12.244669 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB", "2018-06-05 17:40:12.245090 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB", "2018-06-05 17:41:12.245559 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB", "2018-06-05 17:42:12.245970 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB", "2018-06-05 17:43:12.246402 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB", "2018-06-05 17:44:12.246791 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB", "2018-06-05 17:45:12.247219 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB", "2018-06-05 17:46:12.247617 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB", "2018-06-05 17:47:12.248014 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB", "2018-06-05 17:48:12.248391 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB", "2018-06-05 17:49:12.248767 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB", "2018-06-05 17:50:12.249148 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB", "2018-06-05 17:51:12.249533 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB", "2018-06-05 17:52:12.249918 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB", "2018-06-05 17:53:12.250292 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB", "2018-06-05 17:54:12.250713 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB", "2018-06-05 17:55:12.251134 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB", "2018-06-05 17:56:12.251566 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB", "2018-06-05 17:57:12.251957 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB", "2018-06-05 17:58:12.252386 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB", "2018-06-05 17:59:12.252779 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB", "2018-06-05 18:00:12.253164 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB", "2018-06-05 18:01:12.253584 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB", "2018-06-05 18:02:12.254042 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB", "2018-06-05 18:03:12.254440 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB", "2018-06-05 18:04:12.254859 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB", "2018-06-05 18:05:12.255248 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB", "2018-06-05 18:06:12.255630 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB", "2018-06-05 18:07:12.256065 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB", "2018-06-05 18:08:12.256439 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB", "2018-06-05 18:09:12.256858 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB", "2018-06-05 18:10:12.257280 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB", "2018-06-05 18:11:12.257756 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB", "2018-06-05 18:12:12.258147 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB", "2018-06-05 18:13:12.258602 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB", "2018-06-05 18:14:12.258998 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB", "2018-06-05 18:15:12.259427 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB", "2018-06-05 18:16:12.259847 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB", "2018-06-05 18:17:12.260239 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB", "2018-06-05 18:18:12.260625 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB", "2018-06-05 18:19:12.261045 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB", "2018-06-05 18:20:12.261629 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB", "2018-06-05 18:21:12.262016 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB", "2018-06-05 18:22:12.262385 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB", "2018-06-05 18:23:12.262762 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB", "2018-06-05 18:24:12.263139 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB", "2018-06-05 18:25:12.263543 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB", "2018-06-05 18:26:12.263940 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB", "2018-06-05 18:27:12.264329 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB", "2018-06-05 18:28:12.264808 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB", "2018-06-05 18:29:12.265270 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB", "2018-06-05 18:30:12.265761 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB", "2018-06-05 18:31:12.266179 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB", "2018-06-05 18:32:12.266603 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB", "2018-06-05 18:33:12.266983 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB", "2018-06-05 18:34:12.267456 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB", "2018-06-05 18:35:12.267871 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB", "2018-06-05 18:36:12.268283 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB", "2018-06-05 18:37:12.268693 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB", "2018-06-05 18:38:12.269067 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB", "2018-06-05 18:39:12.269471 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB", "2018-06-05 18:40:12.269888 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB", "2018-06-05 18:41:12.270275 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB", "2018-06-05 18:42:12.270671 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB", "2018-06-05 18:43:12.271096 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB", "2018-06-05 18:44:12.271496 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB", "2018-06-05 18:45:12.271892 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB", "2018-06-05 18:46:12.272315 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB", "2018-06-05 18:47:12.272702 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB", "2018-06-05 18:48:12.273130 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB", "2018-06-05 18:49:12.273516 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB", "2018-06-05 18:50:12.273943 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB", "2018-06-05 18:51:12.274326 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB", "2018-06-05 18:52:12.274697 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB", "2018-06-05 18:53:12.275087 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB", "2018-06-05 18:54:12.275519 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB", "2018-06-05 18:55:12.275943 7fc1d2a0f700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB", "2018-06-05 18:55:12.948939 7fc1cda05700 -1 received  signal: Terminated from  PID: 1 task name:  UID: 0", "2018-06-05 18:55:12.948970 7fc1cda05700 -1 mon.node@0(leader) e1 *** Got Signal Terminated ***", "2018-06-05 18:55:12.948977 7fc1cda05700  1 mon.node@0(leader) e1 shutdown"], "stdout": "creating /var/lib/ceph/bootstrap-osd/ceph.keyring\ncreating /var/lib/ceph/bootstrap-mds/ceph.keyring\ncreating /var/lib/ceph/bootstrap-rgw/ceph.keyring\ncreating /var/lib/ceph/bootstrap-rbd/ceph.keyring\nmonmaptool: monmap file /etc/ceph/monmap-ceph\nmonmaptool: set fsid to b2bb6c0b-6cc6-44f1-bf92-1ce81d78fbdc\nmonmaptool: writing epoch 0 to /etc/ceph/monmap-ceph (1 monitors)\nimporting contents of /var/lib/ceph/bootstrap-osd/ceph.keyring into /etc/ceph/ceph.mon.keyring\nimporting contents of /var/lib/ceph/bootstrap-mds/ceph.keyring into /etc/ceph/ceph.mon.keyring\nimporting contents of /var/lib/ceph/bootstrap-rgw/ceph.keyring into /etc/ceph/ceph.mon.keyring\nimporting contents of /var/lib/ceph/bootstrap-rbd/ceph.keyring into /etc/ceph/ceph.mon.keyring\nimporting contents of /etc/ceph/ceph.client.admin.keyring into /etc/ceph/ceph.mon.keyring\n2018-06-05 17:14:12  /entrypoint.sh: SUCCESS\nexec: PID 122: spawning /usr/bin/ceph-mon --cluster ceph --setuser ceph --setgroup ceph -d -i node --mon-data /var/lib/ceph/mon/ceph-node --public-addr 128.110.153.197:6789\nSending SIGTERM to PID 122", "stdout_lines": ["creating /var/lib/ceph/bootstrap-osd/ceph.keyring", "creating /var/lib/ceph/bootstrap-mds/ceph.keyring", "creating /var/lib/ceph/bootstrap-rgw/ceph.keyring", "creating /var/lib/ceph/bootstrap-rbd/ceph.keyring", "monmaptool: monmap file /etc/ceph/monmap-ceph", "monmaptool: set fsid to b2bb6c0b-6cc6-44f1-bf92-1ce81d78fbdc", "monmaptool: writing epoch 0 to /etc/ceph/monmap-ceph (1 monitors)", "importing contents of /var/lib/ceph/bootstrap-osd/ceph.keyring into /etc/ceph/ceph.mon.keyring", "importing contents of /var/lib/ceph/bootstrap-mds/ceph.keyring into /etc/ceph/ceph.mon.keyring", "importing contents of /var/lib/ceph/bootstrap-rgw/ceph.keyring into /etc/ceph/ceph.mon.keyring", "importing contents of /var/lib/ceph/bootstrap-rbd/ceph.keyring into /etc/ceph/ceph.mon.keyring", "importing contents of /etc/ceph/ceph.client.admin.keyring into /etc/ceph/ceph.mon.keyring", "2018-06-05 17:14:12  /entrypoint.sh: SUCCESS", "exec: PID 122: spawning /usr/bin/ceph-mon --cluster ceph --setuser ceph --setgroup ceph -d -i node --mon-data /var/lib/ceph/mon/ceph-node --public-addr 128.110.153.197:6789", "Sending SIGTERM to PID 122"]}
2018-06-05 18:55:25,961 p=11 u=root |  ...ignoring
2018-06-05 18:55:25,973 p=11 u=root |  TASK [baseliner : set fact to store result of bench execution] *****************
2018-06-05 18:55:25,973 p=11 u=root |  Tuesday 05 June 2018  18:55:25 +0000 (0:00:00.986)       1:41:33.241 ********** 
2018-06-05 18:55:25,992 p=11 u=root |  ok: [node1 -> localhost]
2018-06-05 18:55:26,001 p=11 u=root |  TASK [baseliner : record elapsed time in output folder] ************************
2018-06-05 18:55:26,001 p=11 u=root |  Tuesday 05 June 2018  18:55:26 +0000 (0:00:00.027)       1:41:33.269 ********** 
2018-06-05 18:55:26,215 p=11 u=root |  changed: [node1 -> localhost]
2018-06-05 18:55:26,215 p=11 u=root |  changed: [node2 -> localhost]
2018-06-05 18:55:26,229 p=11 u=root |  changed: [node3 -> localhost]
2018-06-05 18:55:26,238 p=11 u=root |  TASK [baseliner : capture stdout] **********************************************
2018-06-05 18:55:26,238 p=11 u=root |  Tuesday 05 June 2018  18:55:26 +0000 (0:00:00.237)       1:41:33.507 ********** 
2018-06-05 18:55:26,445 p=11 u=root |  changed: [node1 -> localhost]
2018-06-05 18:55:26,468 p=11 u=root |  changed: [node2 -> localhost]
2018-06-05 18:55:26,484 p=11 u=root |  changed: [node3 -> localhost]
2018-06-05 18:55:26,493 p=11 u=root |  TASK [baseliner : capture stderr] **********************************************
2018-06-05 18:55:26,493 p=11 u=root |  Tuesday 05 June 2018  18:55:26 +0000 (0:00:00.254)       1:41:33.761 ********** 
2018-06-05 18:55:26,724 p=11 u=root |  changed: [node1 -> localhost]
2018-06-05 18:55:26,726 p=11 u=root |  changed: [node3 -> localhost]
2018-06-05 18:55:26,730 p=11 u=root |  changed: [node2 -> localhost]
2018-06-05 18:55:26,737 p=11 u=root |  TASK [baseliner : debug] *******************************************************
2018-06-05 18:55:26,737 p=11 u=root |  Tuesday 05 June 2018  18:55:26 +0000 (0:00:00.243)       1:41:34.005 ********** 
2018-06-05 18:55:26,771 p=11 u=root |  TASK [baseliner : run compose benchmark] ***************************************
2018-06-05 18:55:26,772 p=11 u=root |  Tuesday 05 June 2018  18:55:26 +0000 (0:00:00.034)       1:41:34.040 ********** 
2018-06-05 18:55:26,805 p=11 u=root |  TASK [baseliner : run script benchmark] ****************************************
2018-06-05 18:55:26,805 p=11 u=root |  Tuesday 05 June 2018  18:55:26 +0000 (0:00:00.033)       1:41:34.073 ********** 
2018-06-05 18:55:26,838 p=11 u=root |  TASK [baseliner : download results] ********************************************
2018-06-05 18:55:26,839 p=11 u=root |  Tuesday 05 June 2018  18:55:26 +0000 (0:00:00.033)       1:41:34.107 ********** 
2018-06-05 18:55:26,893 p=11 u=root |  included: /etc/ansible/roles/baseliner/tasks/download-output.yml for node1, node2, node3
2018-06-05 18:55:26,985 p=11 u=root |  TASK [baseliner : get any bench-specific files] ********************************
2018-06-05 18:55:26,985 p=11 u=root |  Tuesday 05 June 2018  18:55:26 +0000 (0:00:00.146)       1:41:34.253 ********** 
2018-06-05 18:55:28,349 p=11 u=root |  changed: [node1] => (item=/tmp/baseliner_output_node1)
2018-06-05 18:55:28,350 p=11 u=root |  changed: [node2] => (item=/tmp/baseliner_output_node2)
2018-06-05 18:55:28,372 p=11 u=root |  changed: [node3] => (item=/tmp/baseliner_output_node3)
2018-06-05 18:55:28,381 p=11 u=root |  TASK [baseliner : delete remote folder after transfer] *************************
2018-06-05 18:55:28,382 p=11 u=root |  Tuesday 05 June 2018  18:55:28 +0000 (0:00:01.396)       1:41:35.650 ********** 
2018-06-05 18:55:29,213 p=11 u=root |  changed: [node1] => (item=/tmp/baseliner_output_node1)
2018-06-05 18:55:29,225 p=11 u=root |  changed: [node2] => (item=/tmp/baseliner_output_node2)
2018-06-05 18:55:29,226 p=11 u=root |  changed: [node3] => (item=/tmp/baseliner_output_node3)
2018-06-05 18:55:29,237 p=11 u=root |  TASK [baseliner : check if we should fail fast] ********************************
2018-06-05 18:55:29,237 p=11 u=root |  Tuesday 05 June 2018  18:55:29 +0000 (0:00:00.855)       1:41:36.505 ********** 
2018-06-05 18:55:29,291 p=11 u=root |  included: /etc/ansible/roles/baseliner/tasks/fail-fast.yml for node1, node2, node3
2018-06-05 18:55:29,387 p=11 u=root |  TASK [baseliner : failfast for single-node mode] *******************************
2018-06-05 18:55:29,387 p=11 u=root |  Tuesday 05 June 2018  18:55:29 +0000 (0:00:00.149)       1:41:36.655 ********** 
2018-06-05 18:55:29,440 p=11 u=root |  fatal: [node1]: FAILED! => {"changed": false, "failed": true, "msg": "benchmark failed"}
2018-06-05 18:55:29,441 p=11 u=root |  NO MORE HOSTS LEFT *************************************************************
2018-06-05 18:55:29,442 p=11 u=root |  PLAY RECAP *********************************************************************
2018-06-05 18:55:29,442 p=11 u=root |  node1                      : ok=50   changed=17   unreachable=0    failed=1   
2018-06-05 18:55:29,442 p=11 u=root |  node2                      : ok=43   changed=14   unreachable=0    failed=0   
2018-06-05 18:55:29,442 p=11 u=root |  node3                      : ok=43   changed=14   unreachable=0    failed=0   
2018-06-05 18:55:29,442 p=11 u=root |  Tuesday 05 June 2018  18:55:29 +0000 (0:00:00.055)       1:41:36.711 ********** 
2018-06-05 18:55:29,442 p=11 u=root |  =============================================================================== 
2018-06-05 18:55:29,443 p=11 u=root |  baseliner : wait for containers in single-node mode and stop/kill them if they timeout  6072.75s
2018-06-05 18:55:29,443 p=11 u=root |  Gathering Facts --------------------------------------------------------- 3.00s
2018-06-05 18:55:29,443 p=11 u=root |  baseliner : pull image -------------------------------------------------- 2.74s
2018-06-05 18:55:29,443 p=11 u=root |  baseliner : run container ----------------------------------------------- 2.03s
2018-06-05 18:55:29,444 p=11 u=root |  baseliner : install facter ---------------------------------------------- 1.49s
2018-06-05 18:55:29,444 p=11 u=root |  baseliner : upload files ------------------------------------------------ 1.46s
2018-06-05 18:55:29,444 p=11 u=root |  baseliner : get any bench-specific files -------------------------------- 1.40s
2018-06-05 18:55:29,444 p=11 u=root |  baseliner : stop any running container ---------------------------------- 1.00s
2018-06-05 18:55:29,444 p=11 u=root |  baseliner : get result of container execution --------------------------- 0.99s
2018-06-05 18:55:29,444 p=11 u=root |  baseliner : check if image already exists ------------------------------- 0.87s
2018-06-05 18:55:29,444 p=11 u=root |  baseliner : remove containers to avoid name clashes --------------------- 0.86s
2018-06-05 18:55:29,444 p=11 u=root |  baseliner : delete remote folder after transfer ------------------------- 0.86s
2018-06-05 18:55:29,444 p=11 u=root |  baseliner : remove remote results folder -------------------------------- 0.84s
2018-06-05 18:55:29,445 p=11 u=root |  baseliner : create remote results folder -------------------------------- 0.81s
2018-06-05 18:55:29,445 p=11 u=root |  baseliner : store facts about remotes ----------------------------------- 0.78s
2018-06-05 18:55:29,445 p=11 u=root |  baseliner : capture stdout ---------------------------------------------- 0.25s
2018-06-05 18:55:29,445 p=11 u=root |  baseliner : ensure results folder exists -------------------------------- 0.25s
2018-06-05 18:55:29,445 p=11 u=root |  baseliner : capture stderr ---------------------------------------------- 0.24s
2018-06-05 18:55:29,445 p=11 u=root |  baseliner : record elapsed time in output folder ------------------------ 0.24s
2018-06-05 18:55:29,445 p=11 u=root |  baseliner : add host-specific environment to docker_flags --------------- 0.22s
2018-06-05 18:55:29,445 p=11 u=root |  Playbook run took 0 days, 1 hours, 41 minutes, 36 seconds
