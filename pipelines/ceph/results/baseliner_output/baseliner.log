
2018-06-05 20:46:31,866 p=10 u=root |  PLAY [all] *********************************************************************
2018-06-05 20:46:31,875 p=10 u=root |  TASK [Gathering Facts] *********************************************************
2018-06-05 20:46:31,875 p=10 u=root |  Tuesday 05 June 2018  20:46:31 +0000 (0:00:00.051)       0:00:00.051 ********** 
2018-06-05 20:46:34,815 p=10 u=root |  ok: [node3]
2018-06-05 20:46:34,833 p=10 u=root |  ok: [node1]
2018-06-05 20:46:34,838 p=10 u=root |  ok: [node2]
2018-06-05 20:46:34,845 p=10 u=root |  TASK [baseliner : include_tasks] ***********************************************
2018-06-05 20:46:34,845 p=10 u=root |  Tuesday 05 June 2018  20:46:34 +0000 (0:00:02.970)       0:00:03.022 ********** 
2018-06-05 20:46:34,910 p=10 u=root |  included: /etc/ansible/roles/baseliner/tasks/check_preconditions.yml for node1, node2, node3
2018-06-05 20:46:34,919 p=10 u=root |  TASK [baseliner : ensure expected variables are defined] ***********************
2018-06-05 20:46:34,919 p=10 u=root |  Tuesday 05 June 2018  20:46:34 +0000 (0:00:00.073)       0:00:03.096 ********** 
2018-06-05 20:46:34,975 p=10 u=root |  ok: [node1] => {
    "changed": false, 
    "failed": false, 
    "msg": "All assertions passed"
}
2018-06-05 20:46:34,980 p=10 u=root |  ok: [node2] => {
    "changed": false, 
    "failed": false, 
    "msg": "All assertions passed"
}
2018-06-05 20:46:34,992 p=10 u=root |  ok: [node3] => {
    "changed": false, 
    "failed": false, 
    "msg": "All assertions passed"
}
2018-06-05 20:46:35,001 p=10 u=root |  TASK [baseliner : set remote results path if not defined] **********************
2018-06-05 20:46:35,001 p=10 u=root |  Tuesday 05 June 2018  20:46:35 +0000 (0:00:00.081)       0:00:03.178 ********** 
2018-06-05 20:46:35,068 p=10 u=root |  ok: [node1]
2018-06-05 20:46:35,074 p=10 u=root |  ok: [node2]
2018-06-05 20:46:35,085 p=10 u=root |  ok: [node3]
2018-06-05 20:46:35,098 p=10 u=root |  TASK [baseliner : ensure results folder exists] ********************************
2018-06-05 20:46:35,098 p=10 u=root |  Tuesday 05 June 2018  20:46:35 +0000 (0:00:00.097)       0:00:03.275 ********** 
2018-06-05 20:46:35,375 p=10 u=root |  ok: [node1 -> localhost]
2018-06-05 20:46:35,384 p=10 u=root |  TASK [baseliner : check that pre-tasks file exists] ****************************
2018-06-05 20:46:35,385 p=10 u=root |  Tuesday 05 June 2018  20:46:35 +0000 (0:00:00.286)       0:00:03.561 ********** 
2018-06-05 20:46:35,405 p=10 u=root |  TASK [baseliner : check that post-tasks file exists] ***************************
2018-06-05 20:46:35,405 p=10 u=root |  Tuesday 05 June 2018  20:46:35 +0000 (0:00:00.020)       0:00:03.582 ********** 
2018-06-05 20:46:35,448 p=10 u=root |  TASK [baseliner : ensure docker-engine is installed] ***************************
2018-06-05 20:46:35,449 p=10 u=root |  Tuesday 05 June 2018  20:46:35 +0000 (0:00:00.043)       0:00:03.625 ********** 
2018-06-05 20:46:35,490 p=10 u=root |  TASK [baseliner : install statically linked docker] ****************************
2018-06-05 20:46:35,490 p=10 u=root |  Tuesday 05 June 2018  20:46:35 +0000 (0:00:00.041)       0:00:03.667 ********** 
2018-06-05 20:46:35,530 p=10 u=root |  TASK [baseliner : ensure the docker daemon is running] *************************
2018-06-05 20:46:35,530 p=10 u=root |  Tuesday 05 June 2018  20:46:35 +0000 (0:00:00.039)       0:00:03.707 ********** 
2018-06-05 20:46:35,568 p=10 u=root |  TASK [baseliner : stop any running container] **********************************
2018-06-05 20:46:35,568 p=10 u=root |  Tuesday 05 June 2018  20:46:35 +0000 (0:00:00.037)       0:00:03.745 ********** 
2018-06-05 20:46:36,557 p=10 u=root |  fatal: [node1]: FAILED! => {"changed": true, "cmd": "docker stop `docker ps -q`", "delta": "0:00:00.032097", "end": "2018-06-05 14:46:36.512432", "failed": true, "msg": "non-zero return code", "rc": 1, "start": "2018-06-05 14:46:36.480335", "stderr": "\"docker stop\" requires at least 1 argument(s).\nSee 'docker stop --help'.\n\nUsage:  docker stop [OPTIONS] CONTAINER [CONTAINER...]\n\nStop one or more running containers", "stderr_lines": ["\"docker stop\" requires at least 1 argument(s).", "See 'docker stop --help'.", "", "Usage:  docker stop [OPTIONS] CONTAINER [CONTAINER...]", "", "Stop one or more running containers"], "stdout": "", "stdout_lines": []}
2018-06-05 20:46:36,558 p=10 u=root |  ...ignoring
2018-06-05 20:46:36,562 p=10 u=root |  fatal: [node2]: FAILED! => {"changed": true, "cmd": "docker stop `docker ps -q`", "delta": "0:00:00.035464", "end": "2018-06-05 14:46:36.516798", "failed": true, "msg": "non-zero return code", "rc": 1, "start": "2018-06-05 14:46:36.481334", "stderr": "\"docker stop\" requires at least 1 argument(s).\nSee 'docker stop --help'.\n\nUsage:  docker stop [OPTIONS] CONTAINER [CONTAINER...]\n\nStop one or more running containers", "stderr_lines": ["\"docker stop\" requires at least 1 argument(s).", "See 'docker stop --help'.", "", "Usage:  docker stop [OPTIONS] CONTAINER [CONTAINER...]", "", "Stop one or more running containers"], "stdout": "", "stdout_lines": []}
2018-06-05 20:46:36,563 p=10 u=root |  ...ignoring
2018-06-05 20:46:36,563 p=10 u=root |  fatal: [node3]: FAILED! => {"changed": true, "cmd": "docker stop `docker ps -q`", "delta": "0:00:00.040055", "end": "2018-06-05 14:46:36.521292", "failed": true, "msg": "non-zero return code", "rc": 1, "start": "2018-06-05 14:46:36.481237", "stderr": "\"docker stop\" requires at least 1 argument(s).\nSee 'docker stop --help'.\n\nUsage:  docker stop [OPTIONS] CONTAINER [CONTAINER...]\n\nStop one or more running containers", "stderr_lines": ["\"docker stop\" requires at least 1 argument(s).", "See 'docker stop --help'.", "", "Usage:  docker stop [OPTIONS] CONTAINER [CONTAINER...]", "", "Stop one or more running containers"], "stdout": "", "stdout_lines": []}
2018-06-05 20:46:36,564 p=10 u=root |  ...ignoring
2018-06-05 20:46:36,575 p=10 u=root |  TASK [baseliner : remove containers to avoid name clashes] *********************
2018-06-05 20:46:36,575 p=10 u=root |  Tuesday 05 June 2018  20:46:36 +0000 (0:00:01.006)       0:00:04.752 ********** 
2018-06-05 20:46:37,398 p=10 u=root |  fatal: [node3]: FAILED! => {"changed": true, "cmd": "docker rm `docker ps -qa`", "delta": "0:00:00.034841", "end": "2018-06-05 14:46:37.356552", "failed": true, "msg": "non-zero return code", "rc": 1, "start": "2018-06-05 14:46:37.321711", "stderr": "\"docker rm\" requires at least 1 argument(s).\nSee 'docker rm --help'.\n\nUsage:  docker rm [OPTIONS] CONTAINER [CONTAINER...]\n\nRemove one or more containers", "stderr_lines": ["\"docker rm\" requires at least 1 argument(s).", "See 'docker rm --help'.", "", "Usage:  docker rm [OPTIONS] CONTAINER [CONTAINER...]", "", "Remove one or more containers"], "stdout": "", "stdout_lines": []}
2018-06-05 20:46:37,398 p=10 u=root |  ...ignoring
2018-06-05 20:46:37,402 p=10 u=root |  fatal: [node1]: FAILED! => {"changed": true, "cmd": "docker rm `docker ps -qa`", "delta": "0:00:00.041631", "end": "2018-06-05 14:46:37.361877", "failed": true, "msg": "non-zero return code", "rc": 1, "start": "2018-06-05 14:46:37.320246", "stderr": "\"docker rm\" requires at least 1 argument(s).\nSee 'docker rm --help'.\n\nUsage:  docker rm [OPTIONS] CONTAINER [CONTAINER...]\n\nRemove one or more containers", "stderr_lines": ["\"docker rm\" requires at least 1 argument(s).", "See 'docker rm --help'.", "", "Usage:  docker rm [OPTIONS] CONTAINER [CONTAINER...]", "", "Remove one or more containers"], "stdout": "", "stdout_lines": []}
2018-06-05 20:46:37,402 p=10 u=root |  ...ignoring
2018-06-05 20:46:37,412 p=10 u=root |  fatal: [node2]: FAILED! => {"changed": true, "cmd": "docker rm `docker ps -qa`", "delta": "0:00:00.039422", "end": "2018-06-05 14:46:37.367415", "failed": true, "msg": "non-zero return code", "rc": 1, "start": "2018-06-05 14:46:37.327993", "stderr": "\"docker rm\" requires at least 1 argument(s).\nSee 'docker rm --help'.\n\nUsage:  docker rm [OPTIONS] CONTAINER [CONTAINER...]\n\nRemove one or more containers", "stderr_lines": ["\"docker rm\" requires at least 1 argument(s).", "See 'docker rm --help'.", "", "Usage:  docker rm [OPTIONS] CONTAINER [CONTAINER...]", "", "Remove one or more containers"], "stdout": "", "stdout_lines": []}
2018-06-05 20:46:37,412 p=10 u=root |  ...ignoring
2018-06-05 20:46:37,419 p=10 u=root |  TASK [baseliner : store facts] *************************************************
2018-06-05 20:46:37,419 p=10 u=root |  Tuesday 05 June 2018  20:46:37 +0000 (0:00:00.844)       0:00:05.596 ********** 
2018-06-05 20:46:37,474 p=10 u=root |  included: /etc/ansible/roles/baseliner/tasks/store-facts.yml for node1, node2, node3
2018-06-05 20:46:37,492 p=10 u=root |  TASK [baseliner : install facter] **********************************************
2018-06-05 20:46:37,492 p=10 u=root |  Tuesday 05 June 2018  20:46:37 +0000 (0:00:00.072)       0:00:05.669 ********** 
2018-06-05 20:46:38,957 p=10 u=root |  ok: [node3]
2018-06-05 20:46:38,958 p=10 u=root |  ok: [node1]
2018-06-05 20:46:38,965 p=10 u=root |  ok: [node2]
2018-06-05 20:46:38,976 p=10 u=root |  TASK [baseliner : create facts folder if it doesn't exist] *********************
2018-06-05 20:46:38,976 p=10 u=root |  Tuesday 05 June 2018  20:46:38 +0000 (0:00:01.484)       0:00:07.153 ********** 
2018-06-05 20:46:39,083 p=10 u=root |  changed: [node1 -> localhost]
2018-06-05 20:46:39,095 p=10 u=root |  TASK [baseliner : store facts about remotes] ***********************************
2018-06-05 20:46:39,095 p=10 u=root |  Tuesday 05 June 2018  20:46:39 +0000 (0:00:00.118)       0:00:07.271 ********** 
2018-06-05 20:46:39,466 p=10 u=root |  changed: [node1 -> localhost] => (item=node1)
2018-06-05 20:46:39,656 p=10 u=root |  changed: [node1 -> localhost] => (item=node2)
2018-06-05 20:46:39,845 p=10 u=root |  changed: [node1 -> localhost] => (item=node3)
2018-06-05 20:46:39,852 p=10 u=root |  TASK [baseliner : start monitoring] ********************************************
2018-06-05 20:46:39,852 p=10 u=root |  Tuesday 05 June 2018  20:46:39 +0000 (0:00:00.757)       0:00:08.029 ********** 
2018-06-05 20:46:39,886 p=10 u=root |  TASK [baseliner : include_tasks] ***********************************************
2018-06-05 20:46:39,886 p=10 u=root |  Tuesday 05 June 2018  20:46:39 +0000 (0:00:00.033)       0:00:08.063 ********** 
2018-06-05 20:46:39,919 p=10 u=root |  TASK [baseliner : get number of repetitions] ***********************************
2018-06-05 20:46:39,919 p=10 u=root |  Tuesday 05 June 2018  20:46:39 +0000 (0:00:00.032)       0:00:08.095 ********** 
2018-06-05 20:46:39,977 p=10 u=root |  ok: [node1] => (item=1)
2018-06-05 20:46:39,983 p=10 u=root |  ok: [node2] => (item=1)
2018-06-05 20:46:39,991 p=10 u=root |  ok: [node3] => (item=1)
2018-06-05 20:46:39,998 p=10 u=root |  TASK [baseliner : execute each benchmark] **************************************
2018-06-05 20:46:39,998 p=10 u=root |  Tuesday 05 June 2018  20:46:39 +0000 (0:00:00.078)       0:00:08.174 ********** 
2018-06-05 20:46:40,057 p=10 u=root |  included: /etc/ansible/roles/baseliner/tasks/run-bench.yml for node1, node2, node3
2018-06-05 20:46:40,078 p=10 u=root |  TASK [baseliner : upload files] ************************************************
2018-06-05 20:46:40,078 p=10 u=root |  Tuesday 05 June 2018  20:46:40 +0000 (0:00:00.080)       0:00:08.255 ********** 
2018-06-05 20:46:41,541 p=10 u=root |  changed: [node1] => (item={u'dest': u'/tmp/etc/ceph.conf', u'src': u'/experiment/scripts/docker-bench/ceph/etcceph/ceph.conf'})
2018-06-05 20:46:41,546 p=10 u=root |  changed: [node2] => (item={u'dest': u'/tmp/etc/ceph.conf', u'src': u'/experiment/scripts/docker-bench/ceph/etcceph/ceph.conf'})
2018-06-05 20:46:41,547 p=10 u=root |  changed: [node3] => (item={u'dest': u'/tmp/etc/ceph.conf', u'src': u'/experiment/scripts/docker-bench/ceph/etcceph/ceph.conf'})
2018-06-05 20:46:41,554 p=10 u=root |  TASK [baseliner : initialize parameters and add number of repetitions] *********
2018-06-05 20:46:41,555 p=10 u=root |  Tuesday 05 June 2018  20:46:41 +0000 (0:00:01.476)       0:00:09.731 ********** 
2018-06-05 20:46:41,607 p=10 u=root |  ok: [node1]
2018-06-05 20:46:41,615 p=10 u=root |  ok: [node2]
2018-06-05 20:46:41,623 p=10 u=root |  ok: [node3]
2018-06-05 20:46:41,629 p=10 u=root |  TASK [baseliner : unnest parameters when parameters for benchmark were passed] ***
2018-06-05 20:46:41,630 p=10 u=root |  Tuesday 05 June 2018  20:46:41 +0000 (0:00:00.074)       0:00:09.806 ********** 
2018-06-05 20:46:41,662 p=10 u=root |  TASK [baseliner : parametrized execution] **************************************
2018-06-05 20:46:41,662 p=10 u=root |  Tuesday 05 June 2018  20:46:41 +0000 (0:00:00.032)       0:00:09.839 ********** 
2018-06-05 20:46:41,738 p=10 u=root |  included: /etc/ansible/roles/baseliner/tasks/run-bench-parametrized.yml for node1, node2, node3
2018-06-05 20:46:41,747 p=10 u=root |  TASK [baseliner : remove remote results folder] ********************************
2018-06-05 20:46:41,747 p=10 u=root |  Tuesday 05 June 2018  20:46:41 +0000 (0:00:00.084)       0:00:09.924 ********** 
2018-06-05 20:46:42,563 p=10 u=root |  ok: [node1]
2018-06-05 20:46:42,570 p=10 u=root |  ok: [node3]
2018-06-05 20:46:42,573 p=10 u=root |  ok: [node2]
2018-06-05 20:46:42,579 p=10 u=root |  TASK [baseliner : create remote results folder] ********************************
2018-06-05 20:46:42,579 p=10 u=root |  Tuesday 05 June 2018  20:46:42 +0000 (0:00:00.832)       0:00:10.756 ********** 
2018-06-05 20:46:43,381 p=10 u=root |  changed: [node1]
2018-06-05 20:46:43,382 p=10 u=root |  changed: [node2]
2018-06-05 20:46:43,406 p=10 u=root |  changed: [node3]
2018-06-05 20:46:43,418 p=10 u=root |  TASK [baseliner : initialize parameter dictionary] *****************************
2018-06-05 20:46:43,418 p=10 u=root |  Tuesday 05 June 2018  20:46:43 +0000 (0:00:00.838)       0:00:11.595 ********** 
2018-06-05 20:46:43,465 p=10 u=root |  ok: [node1]
2018-06-05 20:46:43,473 p=10 u=root |  ok: [node2]
2018-06-05 20:46:43,484 p=10 u=root |  ok: [node3]
2018-06-05 20:46:43,490 p=10 u=root |  TASK [baseliner : populate parameter dictionary] *******************************
2018-06-05 20:46:43,490 p=10 u=root |  Tuesday 05 June 2018  20:46:43 +0000 (0:00:00.072)       0:00:11.667 ********** 
2018-06-05 20:46:43,548 p=10 u=root |  ok: [node1] => (item=[u'repetition', u'1'])
2018-06-05 20:46:43,552 p=10 u=root |  ok: [node2] => (item=[u'repetition', u'1'])
2018-06-05 20:46:43,563 p=10 u=root |  ok: [node3] => (item=[u'repetition', u'1'])
2018-06-05 20:46:43,570 p=10 u=root |  TASK [baseliner : merge default variables to the benchmark-specific options] ***
2018-06-05 20:46:43,570 p=10 u=root |  Tuesday 05 June 2018  20:46:43 +0000 (0:00:00.079)       0:00:11.746 ********** 
2018-06-05 20:46:43,621 p=10 u=root |  ok: [node1]
2018-06-05 20:46:43,628 p=10 u=root |  ok: [node2]
2018-06-05 20:46:43,638 p=10 u=root |  ok: [node3]
2018-06-05 20:46:43,645 p=10 u=root |  TASK [baseliner : initialize string for path] **********************************
2018-06-05 20:46:43,645 p=10 u=root |  Tuesday 05 June 2018  20:46:43 +0000 (0:00:00.074)       0:00:11.821 ********** 
2018-06-05 20:46:43,695 p=10 u=root |  ok: [node1]
2018-06-05 20:46:43,703 p=10 u=root |  ok: [node2]
2018-06-05 20:46:43,711 p=10 u=root |  ok: [node3]
2018-06-05 20:46:43,718 p=10 u=root |  TASK [baseliner : add key/value pairs for each parameter (if any)] *************
2018-06-05 20:46:43,718 p=10 u=root |  Tuesday 05 June 2018  20:46:43 +0000 (0:00:00.073)       0:00:11.895 ********** 
2018-06-05 20:46:43,774 p=10 u=root |  ok: [node1] => (item={'key': u'repetition', 'value': u'1'})
2018-06-05 20:46:43,781 p=10 u=root |  ok: [node2] => (item={'key': u'repetition', 'value': u'1'})
2018-06-05 20:46:43,789 p=10 u=root |  ok: [node3] => (item={'key': u'repetition', 'value': u'1'})
2018-06-05 20:46:43,797 p=10 u=root |  TASK [baseliner : ensure output folder exists] *********************************
2018-06-05 20:46:43,798 p=10 u=root |  Tuesday 05 June 2018  20:46:43 +0000 (0:00:00.079)       0:00:11.974 ********** 
2018-06-05 20:46:43,924 p=10 u=root |  changed: [node2 -> localhost]
2018-06-05 20:46:43,925 p=10 u=root |  changed: [node1 -> localhost]
2018-06-05 20:46:43,942 p=10 u=root |  changed: [node3 -> localhost]
2018-06-05 20:46:43,949 p=10 u=root |  TASK [baseliner : run containerized benchmark] *********************************
2018-06-05 20:46:43,949 p=10 u=root |  Tuesday 05 June 2018  20:46:43 +0000 (0:00:00.151)       0:00:12.126 ********** 
2018-06-05 20:46:44,075 p=10 u=root |  included: /etc/ansible/roles/baseliner/tasks/run-container.yml for node1, node2, node3
2018-06-05 20:46:44,095 p=10 u=root |  TASK [baseliner : check if image already exists] *******************************
2018-06-05 20:46:44,095 p=10 u=root |  Tuesday 05 June 2018  20:46:44 +0000 (0:00:00.146)       0:00:12.272 ********** 
2018-06-05 20:46:44,957 p=10 u=root |  changed: [node1]
2018-06-05 20:46:44,959 p=10 u=root |  changed: [node2]
2018-06-05 20:46:44,961 p=10 u=root |  changed: [node3]
2018-06-05 20:46:44,970 p=10 u=root |  TASK [baseliner : pull image] **************************************************
2018-06-05 20:46:44,970 p=10 u=root |  Tuesday 05 June 2018  20:46:44 +0000 (0:00:00.875)       0:00:13.147 ********** 
2018-06-05 20:46:47,394 p=10 u=root |  changed: [node3]
2018-06-05 20:46:47,395 p=10 u=root |  changed: [node1]
2018-06-05 20:46:47,733 p=10 u=root |  changed: [node2]
2018-06-05 20:46:47,739 p=10 u=root |  TASK [baseliner : define name of container] ************************************
2018-06-05 20:46:47,739 p=10 u=root |  Tuesday 05 June 2018  20:46:47 +0000 (0:00:02.768)       0:00:15.916 ********** 
2018-06-05 20:46:47,789 p=10 u=root |  ok: [node1]
2018-06-05 20:46:47,798 p=10 u=root |  ok: [node2]
2018-06-05 20:46:47,808 p=10 u=root |  ok: [node3]
2018-06-05 20:46:47,814 p=10 u=root |  TASK [baseliner : initialize docker_flags variable] ****************************
2018-06-05 20:46:47,814 p=10 u=root |  Tuesday 05 June 2018  20:46:47 +0000 (0:00:00.075)       0:00:15.991 ********** 
2018-06-05 20:46:47,864 p=10 u=root |  ok: [node1]
2018-06-05 20:46:47,871 p=10 u=root |  ok: [node2]
2018-06-05 20:46:47,882 p=10 u=root |  ok: [node3]
2018-06-05 20:46:47,889 p=10 u=root |  TASK [baseliner : add environment to docker_flags] *****************************
2018-06-05 20:46:47,889 p=10 u=root |  Tuesday 05 June 2018  20:46:47 +0000 (0:00:00.074)       0:00:16.066 ********** 
2018-06-05 20:46:47,923 p=10 u=root |  TASK [baseliner : add implicit environment from bench_params variable] *********
2018-06-05 20:46:47,923 p=10 u=root |  Tuesday 05 June 2018  20:46:47 +0000 (0:00:00.033)       0:00:16.100 ********** 
2018-06-05 20:46:47,977 p=10 u=root |  ok: [node1] => (item={'key': u'repetition', 'value': u'1'})
2018-06-05 20:46:47,990 p=10 u=root |  ok: [node2] => (item={'key': u'repetition', 'value': u'1'})
2018-06-05 20:46:47,996 p=10 u=root |  ok: [node3] => (item={'key': u'repetition', 'value': u'1'})
2018-06-05 20:46:48,003 p=10 u=root |  TASK [baseliner : check if we have host-specific ips] **************************
2018-06-05 20:46:48,003 p=10 u=root |  Tuesday 05 June 2018  20:46:48 +0000 (0:00:00.079)       0:00:16.180 ********** 
2018-06-05 20:46:48,054 p=10 u=root |  ok: [node1]
2018-06-05 20:46:48,064 p=10 u=root |  ok: [node2]
2018-06-05 20:46:48,074 p=10 u=root |  ok: [node3]
2018-06-05 20:46:48,080 p=10 u=root |  TASK [baseliner : add host-specific ips to docker_flags] ***********************
2018-06-05 20:46:48,080 p=10 u=root |  Tuesday 05 June 2018  20:46:48 +0000 (0:00:00.077)       0:00:16.257 ********** 
2018-06-05 20:46:48,121 p=10 u=root |  TASK [baseliner : check if we have host-specific environment] ******************
2018-06-05 20:46:48,121 p=10 u=root |  Tuesday 05 June 2018  20:46:48 +0000 (0:00:00.041)       0:00:16.298 ********** 
2018-06-05 20:46:48,172 p=10 u=root |  ok: [node1]
2018-06-05 20:46:48,184 p=10 u=root |  ok: [node2]
2018-06-05 20:46:48,195 p=10 u=root |  ok: [node3]
2018-06-05 20:46:48,202 p=10 u=root |  TASK [baseliner : add host-specific environment to docker_flags] ***************
2018-06-05 20:46:48,202 p=10 u=root |  Tuesday 05 June 2018  20:46:48 +0000 (0:00:00.080)       0:00:16.378 ********** 
2018-06-05 20:46:48,258 p=10 u=root |  ok: [node1] => (item={'key': u'CEPH_PUBLIC_NETWORK', 'value': u'128.110.152.0/22'})
2018-06-05 20:46:48,267 p=10 u=root |  ok: [node2] => (item={'key': u'CEPH_PUBLIC_NETWORK', 'value': u'128.110.152.0/22'})
2018-06-05 20:46:48,281 p=10 u=root |  ok: [node3] => (item={'key': u'CEPH_PUBLIC_NETWORK', 'value': u'128.110.152.0/22'})
2018-06-05 20:46:48,300 p=10 u=root |  ok: [node1] => (item={'key': u'MON_IP', 'value': u'128.110.153.197'})
2018-06-05 20:46:48,310 p=10 u=root |  ok: [node2] => (item={'key': u'MON_IP', 'value': u'128.110.153.197'})
2018-06-05 20:46:48,322 p=10 u=root |  ok: [node3] => (item={'key': u'CLIENT', 'value': True})
2018-06-05 20:46:48,337 p=10 u=root |  ok: [node1] => (item={'key': u'MONITOR', 'value': True})
2018-06-05 20:46:48,348 p=10 u=root |  ok: [node2] => (item={'key': u'OSD', 'value': True})
2018-06-05 20:46:48,357 p=10 u=root |  ok: [node3] => (item={'key': u'MON_IP', 'value': u'128.110.153.197'})
2018-06-05 20:46:48,429 p=10 u=root |  ok: [node2] => (item={'key': u'OSD_DEVICES', 'value': u'/dev/sdb'})
2018-06-05 20:46:48,435 p=10 u=root |  TASK [baseliner : add devices to docker_flags] *********************************
2018-06-05 20:46:48,436 p=10 u=root |  Tuesday 05 June 2018  20:46:48 +0000 (0:00:00.233)       0:00:16.612 ********** 
2018-06-05 20:46:48,476 p=10 u=root |  TASK [baseliner : add network mode to docker_flags] ****************************
2018-06-05 20:46:48,476 p=10 u=root |  Tuesday 05 June 2018  20:46:48 +0000 (0:00:00.040)       0:00:16.653 ********** 
2018-06-05 20:46:48,531 p=10 u=root |  ok: [node1]
2018-06-05 20:46:48,540 p=10 u=root |  ok: [node2]
2018-06-05 20:46:48,549 p=10 u=root |  ok: [node3]
2018-06-05 20:46:48,556 p=10 u=root |  TASK [baseliner : add ipc mode to docker_flags] ********************************
2018-06-05 20:46:48,556 p=10 u=root |  Tuesday 05 June 2018  20:46:48 +0000 (0:00:00.080)       0:00:16.733 ********** 
2018-06-05 20:46:48,595 p=10 u=root |  TASK [baseliner : add ports to docker_flags] ***********************************
2018-06-05 20:46:48,596 p=10 u=root |  Tuesday 05 June 2018  20:46:48 +0000 (0:00:00.039)       0:00:16.772 ********** 
2018-06-05 20:46:48,631 p=10 u=root |  TASK [baseliner : add volumes to docker_flags] *********************************
2018-06-05 20:46:48,631 p=10 u=root |  Tuesday 05 June 2018  20:46:48 +0000 (0:00:00.035)       0:00:16.808 ********** 
2018-06-05 20:46:48,689 p=10 u=root |  ok: [node1] => (item=/tmp/etc:/etc/ceph)
2018-06-05 20:46:48,697 p=10 u=root |  ok: [node2] => (item=/tmp/etc:/etc/ceph)
2018-06-05 20:46:48,705 p=10 u=root |  ok: [node3] => (item=/tmp/etc:/etc/ceph)
2018-06-05 20:46:48,711 p=10 u=root |  TASK [baseliner : add default volumes to docker_flags] *************************
2018-06-05 20:46:48,712 p=10 u=root |  Tuesday 05 June 2018  20:46:48 +0000 (0:00:00.080)       0:00:16.888 ********** 
2018-06-05 20:46:48,762 p=10 u=root |  ok: [node1]
2018-06-05 20:46:48,774 p=10 u=root |  ok: [node2]
2018-06-05 20:46:48,781 p=10 u=root |  ok: [node3]
2018-06-05 20:46:48,787 p=10 u=root |  TASK [baseliner : set entrypoint] **********************************************
2018-06-05 20:46:48,787 p=10 u=root |  Tuesday 05 June 2018  20:46:48 +0000 (0:00:00.075)       0:00:16.964 ********** 
2018-06-05 20:46:48,821 p=10 u=root |  TASK [baseliner : set limits] **************************************************
2018-06-05 20:46:48,822 p=10 u=root |  Tuesday 05 June 2018  20:46:48 +0000 (0:00:00.034)       0:00:16.998 ********** 
2018-06-05 20:46:48,859 p=10 u=root |  TASK [baseliner : add capabilities] ********************************************
2018-06-05 20:46:48,859 p=10 u=root |  Tuesday 05 June 2018  20:46:48 +0000 (0:00:00.037)       0:00:17.036 ********** 
2018-06-05 20:46:48,898 p=10 u=root |  TASK [baseliner : remove capabilities] *****************************************
2018-06-05 20:46:48,898 p=10 u=root |  Tuesday 05 June 2018  20:46:48 +0000 (0:00:00.039)       0:00:17.075 ********** 
2018-06-05 20:46:48,933 p=10 u=root |  TASK [baseliner : set privileged mode] *****************************************
2018-06-05 20:46:48,934 p=10 u=root |  Tuesday 05 June 2018  20:46:48 +0000 (0:00:00.035)       0:00:17.110 ********** 
2018-06-05 20:46:48,969 p=10 u=root |  TASK [baseliner : set memory constraint] ***************************************
2018-06-05 20:46:48,969 p=10 u=root |  Tuesday 05 June 2018  20:46:48 +0000 (0:00:00.035)       0:00:17.146 ********** 
2018-06-05 20:46:49,003 p=10 u=root |  TASK [baseliner : set memory-swap constraint] **********************************
2018-06-05 20:46:49,003 p=10 u=root |  Tuesday 05 June 2018  20:46:49 +0000 (0:00:00.034)       0:00:17.180 ********** 
2018-06-05 20:46:49,037 p=10 u=root |  TASK [baseliner : set parent cgroup] *******************************************
2018-06-05 20:46:49,037 p=10 u=root |  Tuesday 05 June 2018  20:46:49 +0000 (0:00:00.033)       0:00:17.214 ********** 
2018-06-05 20:46:49,073 p=10 u=root |  TASK [baseliner : debug] *******************************************************
2018-06-05 20:46:49,074 p=10 u=root |  Tuesday 05 June 2018  20:46:49 +0000 (0:00:00.036)       0:00:17.250 ********** 
2018-06-05 20:46:49,125 p=10 u=root |  ok: [node1] => {
    "msg": "docker run --rm  -e repetition=\"1\" -e CEPH_PUBLIC_NETWORK=\"128.110.152.0/22\" -e MON_IP=\"128.110.153.197\" -e MONITOR=\"True\" --net=host -v /tmp/etc:/etc/ceph -v /tmp/baseliner_output_node1:/results -v /tmp/baseliner_output_node1:/output mariettesouppe/rados_bench:master-ec8d33f-luminous-ubuntu-16.04-x86_64 "
}
2018-06-05 20:46:49,133 p=10 u=root |  ok: [node2] => {
    "msg": "docker run --rm  -e repetition=\"1\" -e CEPH_PUBLIC_NETWORK=\"128.110.152.0/22\" -e MON_IP=\"128.110.153.197\" -e OSD=\"True\" -e OSD_DEVICES=\"/dev/sdb\" --net=host -v /tmp/etc:/etc/ceph -v /tmp/baseliner_output_node2:/results -v /tmp/baseliner_output_node2:/output mariettesouppe/rados_bench:master-ec8d33f-luminous-ubuntu-16.04-x86_64 "
}
2018-06-05 20:46:49,143 p=10 u=root |  ok: [node3] => {
    "msg": "docker run --rm  -e repetition=\"1\" -e CEPH_PUBLIC_NETWORK=\"128.110.152.0/22\" -e CLIENT=\"True\" -e MON_IP=\"128.110.153.197\" --net=host -v /tmp/etc:/etc/ceph -v /tmp/baseliner_output_node3:/results -v /tmp/baseliner_output_node3:/output mariettesouppe/rados_bench:master-ec8d33f-luminous-ubuntu-16.04-x86_64 "
}
2018-06-05 20:46:49,149 p=10 u=root |  TASK [baseliner : run container] ***********************************************
2018-06-05 20:46:49,149 p=10 u=root |  Tuesday 05 June 2018  20:46:49 +0000 (0:00:00.075)       0:00:17.326 ********** 
2018-06-05 20:46:51,162 p=10 u=root |  changed: [node1]
2018-06-05 20:46:51,165 p=10 u=root |  changed: [node3]
2018-06-05 20:46:51,166 p=10 u=root |  changed: [node2]
2018-06-05 20:46:51,176 p=10 u=root |  TASK [baseliner : wait for containers in parallel mode and stop/kill them if they timeout] ***
2018-06-05 20:46:51,176 p=10 u=root |  Tuesday 05 June 2018  20:46:51 +0000 (0:00:02.026)       0:00:19.352 ********** 
2018-06-05 20:46:51,194 p=10 u=root |  TASK [baseliner : wait for containers in single-node mode and stop/kill them if they timeout] ***
2018-06-05 20:46:51,194 p=10 u=root |  Tuesday 05 June 2018  20:46:51 +0000 (0:00:00.018)       0:00:19.371 ********** 
2018-06-05 22:28:04,436 p=10 u=root |  changed: [node1 -> localhost]
2018-06-05 22:28:04,447 p=10 u=root |  TASK [baseliner : get wait result] *********************************************
2018-06-05 22:28:04,447 p=10 u=root |  Tuesday 05 June 2018  22:28:04 +0000 (1:41:13.252)       1:41:32.623 ********** 
2018-06-05 22:28:04,467 p=10 u=root |  ok: [node1 -> localhost]
2018-06-05 22:28:04,477 p=10 u=root |  TASK [baseliner : debug] *******************************************************
2018-06-05 22:28:04,477 p=10 u=root |  Tuesday 05 June 2018  22:28:04 +0000 (0:00:00.029)       1:41:32.654 ********** 
2018-06-05 22:28:04,496 p=10 u=root |  ok: [node1 -> localhost] => {
    "msg": "WAIT_FOR_RESULT:  WARNING: timeout waiting for 3 hosts"
}
2018-06-05 22:28:04,502 p=10 u=root |  TASK [baseliner : get result of container execution] ***************************
2018-06-05 22:28:04,503 p=10 u=root |  Tuesday 05 June 2018  22:28:04 +0000 (0:00:00.025)       1:41:32.679 ********** 
2018-06-05 22:28:05,443 p=10 u=root |  fatal: [node3]: FAILED! => {"ansible_job_id": "127612489605.15496", "changed": true, "cmd": "docker run --rm --name baseliner_node3  -e repetition=\"1\" -e CEPH_PUBLIC_NETWORK=\"128.110.152.0/22\" -e CLIENT=\"True\" -e MON_IP=\"128.110.153.197\" --net=host -v /tmp/etc:/etc/ceph -v /tmp/baseliner_output_node3:/results -v /tmp/baseliner_output_node3:/output mariettesouppe/rados_bench:master-ec8d33f-luminous-ubuntu-16.04-x86_64 ", "delta": "1:41:12.403127", "end": "2018-06-05 16:28:02.639441", "failed": true, "finished": 1, "msg": "non-zero return code", "rc": 137, "start": "2018-06-05 14:46:50.236314", "stderr": "+ '[' -z '' ']'\n+ '[' -z '' ']'\n+ '[' -z True ']'\n+ '[' -z 128.110.152.0/22 ']'\n+ '[' -z 128.110.153.197 ']'\n+ sed -i -e s@monip@128.110.153.197@ /etc/ceph/ceph.conf\n+ sed -i -e s@cephnet@128.110.152.0/22@ /etc/ceph/ceph.conf\n+ '[' -f /etc/ceph/extra.conf ']'\n+ '[' -n '' ']'\n+ '[' -n '' ']'\n+ '[' -n True ']'\n+ '[' -z '' ']'\n+ echo 'No CLIENT_SCRIPT defined, running built-in radosbench script.'\n+ CLIENT_SCRIPT=run_radosbench\n+ ceph_health_ok\n+ exec run_radosbench\n+ ceph osd pool rm rbd rbd --yes-i-really-really-mean-it\npool 'rbd' does not exist\n+ ceph osd pool create scbench 150 150\npool 'scbench' created\n+ ceph osd pool set scbench size 1\nset pool 1 size to 1\n+ rados bench -p scbench 30 write --no-cleanup", "stderr_lines": ["+ '[' -z '' ']'", "+ '[' -z '' ']'", "+ '[' -z True ']'", "+ '[' -z 128.110.152.0/22 ']'", "+ '[' -z 128.110.153.197 ']'", "+ sed -i -e s@monip@128.110.153.197@ /etc/ceph/ceph.conf", "+ sed -i -e s@cephnet@128.110.152.0/22@ /etc/ceph/ceph.conf", "+ '[' -f /etc/ceph/extra.conf ']'", "+ '[' -n '' ']'", "+ '[' -n '' ']'", "+ '[' -n True ']'", "+ '[' -z '' ']'", "+ echo 'No CLIENT_SCRIPT defined, running built-in radosbench script.'", "+ CLIENT_SCRIPT=run_radosbench", "+ ceph_health_ok", "+ exec run_radosbench", "+ ceph osd pool rm rbd rbd --yes-i-really-really-mean-it", "pool 'rbd' does not exist", "+ ceph osd pool create scbench 150 150", "pool 'scbench' created", "+ ceph osd pool set scbench size 1", "set pool 1 size to 1", "+ rados bench -p scbench 30 write --no-cleanup"], "stdout": "No CLIENT_SCRIPT defined, running built-in radosbench script.", "stdout_lines": ["No CLIENT_SCRIPT defined, running built-in radosbench script."]}
2018-06-05 22:28:05,444 p=10 u=root |  ...ignoring
2018-06-05 22:28:05,446 p=10 u=root |  fatal: [node2]: FAILED! => {"ansible_job_id": "553836942878.16582", "changed": true, "cmd": "docker run --rm --name baseliner_node2  -e repetition=\"1\" -e CEPH_PUBLIC_NETWORK=\"128.110.152.0/22\" -e MON_IP=\"128.110.153.197\" -e OSD=\"True\" -e OSD_DEVICES=\"/dev/sdb\" --net=host -v /tmp/etc:/etc/ceph -v /tmp/baseliner_output_node2:/results -v /tmp/baseliner_output_node2:/output mariettesouppe/rados_bench:master-ec8d33f-luminous-ubuntu-16.04-x86_64 ", "delta": "0:00:00.576555", "end": "2018-06-05 14:46:50.820227", "failed": true, "finished": 1, "msg": "non-zero return code", "rc": 1, "start": "2018-06-05 14:46:50.243672", "stderr": "+ '[' -z '' ']'\n+ '[' -z True ']'\n+ '[' -z 128.110.152.0/22 ']'\n+ '[' -z 128.110.153.197 ']'\n+ sed -i -e s@monip@128.110.153.197@ /etc/ceph/ceph.conf\n+ sed -i -e s@cephnet@128.110.152.0/22@ /etc/ceph/ceph.conf\n+ '[' -f /etc/ceph/extra.conf ']'\n+ '[' -n '' ']'\n+ '[' -n True ']'\n+ '[' -n '' ']'\n+ exec run_osds\n+ '[' -z /dev/sdb ']'\n+ devices=(`echo $OSD_DEVICES | sed 's/,/ /g'`)\n++ echo /dev/sdb\n++ sed 's/,/ /g'\n+ for dev in '\"${devices[@]}\"'\n+ export OSD_FORCE_ZAP=1\n+ OSD_FORCE_ZAP=1\n+ export OSD_DEVICE=/dev/sdb\n+ OSD_DEVICE=/dev/sdb\n+ export OSD_TYPE=disk\n+ OSD_TYPE=disk\n+ '[' /dev/sdb == /dev/sdb ']'\n+ /entrypoint.sh osd", "stderr_lines": ["+ '[' -z '' ']'", "+ '[' -z True ']'", "+ '[' -z 128.110.152.0/22 ']'", "+ '[' -z 128.110.153.197 ']'", "+ sed -i -e s@monip@128.110.153.197@ /etc/ceph/ceph.conf", "+ sed -i -e s@cephnet@128.110.152.0/22@ /etc/ceph/ceph.conf", "+ '[' -f /etc/ceph/extra.conf ']'", "+ '[' -n '' ']'", "+ '[' -n True ']'", "+ '[' -n '' ']'", "+ exec run_osds", "+ '[' -z /dev/sdb ']'", "+ devices=(`echo $OSD_DEVICES | sed 's/,/ /g'`)", "++ echo /dev/sdb", "++ sed 's/,/ /g'", "+ for dev in '\"${devices[@]}\"'", "+ export OSD_FORCE_ZAP=1", "+ OSD_FORCE_ZAP=1", "+ export OSD_DEVICE=/dev/sdb", "+ OSD_DEVICE=/dev/sdb", "+ export OSD_TYPE=disk", "+ OSD_TYPE=disk", "+ '[' /dev/sdb == /dev/sdb ']'", "+ /entrypoint.sh osd"], "stdout": "2018-06-05 20:46:50  /entrypoint.sh: static: does not generate config\n2018-06-05 20:46:50  /entrypoint.sh: ERROR- The device pointed by OSD_DEVICE (/dev/sdb) doesn't exist !", "stdout_lines": ["2018-06-05 20:46:50  /entrypoint.sh: static: does not generate config", "2018-06-05 20:46:50  /entrypoint.sh: ERROR- The device pointed by OSD_DEVICE (/dev/sdb) doesn't exist !"]}
2018-06-05 22:28:05,446 p=10 u=root |  ...ignoring
2018-06-05 22:28:05,473 p=10 u=root |  fatal: [node1]: FAILED! => {"ansible_job_id": "944032220654.10714", "changed": true, "cmd": "docker run --rm --name baseliner_node1  -e repetition=\"1\" -e CEPH_PUBLIC_NETWORK=\"128.110.152.0/22\" -e MON_IP=\"128.110.153.197\" -e MONITOR=\"True\" --net=host -v /tmp/etc:/etc/ceph -v /tmp/baseliner_output_node1:/results -v /tmp/baseliner_output_node1:/output mariettesouppe/rados_bench:master-ec8d33f-luminous-ubuntu-16.04-x86_64 ", "delta": "1:41:02.371576", "end": "2018-06-05 16:27:52.611952", "failed": true, "finished": 1, "msg": "non-zero return code", "rc": 1, "start": "2018-06-05 14:46:50.240376", "stderr": "+ '[' -z True ']'\n+ '[' -z 128.110.152.0/22 ']'\n+ '[' -z 128.110.153.197 ']'\n+ sed -i -e s@monip@128.110.153.197@ /etc/ceph/ceph.conf\n+ sed -i -e s@cephnet@128.110.152.0/22@ /etc/ceph/ceph.conf\n+ '[' -f /etc/ceph/extra.conf ']'\n+ '[' -n True ']'\n+ '[' -n '' ']'\n+ exec /entrypoint.sh mon\n2018-06-05 20:46:51.108769 7fa3f3bcbf00  0 set uid:gid to 64045:64045 (ceph:ceph)\n2018-06-05 20:46:51.108785 7fa3f3bcbf00  0 ceph version 12.2.5 (cad919881333ac92274171586c827e01f554a70a) luminous (stable), process (unknown), pid 122\n2018-06-05 20:46:51.108867 7fa3f3bcbf00  0 pidfile_write: ignore empty --pid-file\n2018-06-05 20:46:51.116239 7fa3f3bcbf00  0 load: jerasure load: lrc load: isa \n2018-06-05 20:46:51.116342 7fa3f3bcbf00  0  set rocksdb option compression = kNoCompression\n2018-06-05 20:46:51.116355 7fa3f3bcbf00  0  set rocksdb option write_buffer_size = 33554432\n2018-06-05 20:46:51.116378 7fa3f3bcbf00  0  set rocksdb option compression = kNoCompression\n2018-06-05 20:46:51.116381 7fa3f3bcbf00  0  set rocksdb option write_buffer_size = 33554432\n2018-06-05 20:46:51.116493 7fa3f3bcbf00  4 rocksdb: RocksDB version: 5.4.0\n\n2018-06-05 20:46:51.116503 7fa3f3bcbf00  4 rocksdb: Git sha rocksdb_build_git_sha:@0@\n2018-06-05 20:46:51.116505 7fa3f3bcbf00  4 rocksdb: Compile date Apr 23 2018\n2018-06-05 20:46:51.116506 7fa3f3bcbf00  4 rocksdb: DB SUMMARY\n\n2018-06-05 20:46:51.116549 7fa3f3bcbf00  4 rocksdb: CURRENT file:  CURRENT\n\n2018-06-05 20:46:51.116551 7fa3f3bcbf00  4 rocksdb: IDENTITY file:  IDENTITY\n\n2018-06-05 20:46:51.116560 7fa3f3bcbf00  4 rocksdb: MANIFEST file:  MANIFEST-000001 size: 13 Bytes\n\n2018-06-05 20:46:51.116562 7fa3f3bcbf00  4 rocksdb: SST files in /var/lib/ceph/mon/ceph-node/store.db dir, Total Num: 0, files: \n\n2018-06-05 20:46:51.116564 7fa3f3bcbf00  4 rocksdb: Write Ahead Log file in /var/lib/ceph/mon/ceph-node/store.db: 000003.log size: 454 ; \n\n2018-06-05 20:46:51.116566 7fa3f3bcbf00  4 rocksdb:                         Options.error_if_exists: 0\n2018-06-05 20:46:51.116567 7fa3f3bcbf00  4 rocksdb:                       Options.create_if_missing: 0\n2018-06-05 20:46:51.116568 7fa3f3bcbf00  4 rocksdb:                         Options.paranoid_checks: 1\n2018-06-05 20:46:51.116569 7fa3f3bcbf00  4 rocksdb:                                     Options.env: 0x5562db672020\n2018-06-05 20:46:51.116570 7fa3f3bcbf00  4 rocksdb:                                Options.info_log: 0x5562dd5fcc80\n2018-06-05 20:46:51.116571 7fa3f3bcbf00  4 rocksdb:                          Options.max_open_files: -1\n2018-06-05 20:46:51.116572 7fa3f3bcbf00  4 rocksdb:                Options.max_file_opening_threads: 16\n2018-06-05 20:46:51.116573 7fa3f3bcbf00  4 rocksdb:                               Options.use_fsync: 0\n2018-06-05 20:46:51.116573 7fa3f3bcbf00  4 rocksdb:                       Options.max_log_file_size: 0\n2018-06-05 20:46:51.116574 7fa3f3bcbf00  4 rocksdb:                  Options.max_manifest_file_size: 18446744073709551615\n2018-06-05 20:46:51.116576 7fa3f3bcbf00  4 rocksdb:                   Options.log_file_time_to_roll: 0\n2018-06-05 20:46:51.116576 7fa3f3bcbf00  4 rocksdb:                       Options.keep_log_file_num: 1000\n2018-06-05 20:46:51.116577 7fa3f3bcbf00  4 rocksdb:                    Options.recycle_log_file_num: 0\n2018-06-05 20:46:51.116585 7fa3f3bcbf00  4 rocksdb:                         Options.allow_fallocate: 1\n2018-06-05 20:46:51.116586 7fa3f3bcbf00  4 rocksdb:                        Options.allow_mmap_reads: 0\n2018-06-05 20:46:51.116587 7fa3f3bcbf00  4 rocksdb:                       Options.allow_mmap_writes: 0\n2018-06-05 20:46:51.116587 7fa3f3bcbf00  4 rocksdb:                        Options.use_direct_reads: 0\n2018-06-05 20:46:51.116588 7fa3f3bcbf00  4 rocksdb:                        Options.use_direct_io_for_flush_and_compaction: 0\n2018-06-05 20:46:51.116589 7fa3f3bcbf00  4 rocksdb:          Options.create_missing_column_families: 0\n2018-06-05 20:46:51.116590 7fa3f3bcbf00  4 rocksdb:                              Options.db_log_dir: \n2018-06-05 20:46:51.116591 7fa3f3bcbf00  4 rocksdb:                                 Options.wal_dir: /var/lib/ceph/mon/ceph-node/store.db\n2018-06-05 20:46:51.116592 7fa3f3bcbf00  4 rocksdb:                Options.table_cache_numshardbits: 6\n2018-06-05 20:46:51.116593 7fa3f3bcbf00  4 rocksdb:                      Options.max_subcompactions: 1\n2018-06-05 20:46:51.116594 7fa3f3bcbf00  4 rocksdb:                  Options.max_background_flushes: 1\n2018-06-05 20:46:51.116594 7fa3f3bcbf00  4 rocksdb:                         Options.WAL_ttl_seconds: 0\n2018-06-05 20:46:51.116595 7fa3f3bcbf00  4 rocksdb:                       Options.WAL_size_limit_MB: 0\n2018-06-05 20:46:51.116596 7fa3f3bcbf00  4 rocksdb:             Options.manifest_preallocation_size: 4194304\n2018-06-05 20:46:51.116597 7fa3f3bcbf00  4 rocksdb:                     Options.is_fd_close_on_exec: 1\n2018-06-05 20:46:51.116598 7fa3f3bcbf00  4 rocksdb:                   Options.advise_random_on_open: 1\n2018-06-05 20:46:51.116599 7fa3f3bcbf00  4 rocksdb:                    Options.db_write_buffer_size: 0\n2018-06-05 20:46:51.116599 7fa3f3bcbf00  4 rocksdb:         Options.access_hint_on_compaction_start: 1\n2018-06-05 20:46:51.116601 7fa3f3bcbf00  4 rocksdb:  Options.new_table_reader_for_compaction_inputs: 0\n2018-06-05 20:46:51.116602 7fa3f3bcbf00  4 rocksdb:               Options.compaction_readahead_size: 0\n2018-06-05 20:46:51.116602 7fa3f3bcbf00  4 rocksdb:           Options.random_access_max_buffer_size: 1048576\n2018-06-05 20:46:51.116603 7fa3f3bcbf00  4 rocksdb:           Options.writable_file_max_buffer_size: 1048576\n2018-06-05 20:46:51.116604 7fa3f3bcbf00  4 rocksdb:                      Options.use_adaptive_mutex: 0\n2018-06-05 20:46:51.116605 7fa3f3bcbf00  4 rocksdb:                            Options.rate_limiter: (nil)\n2018-06-05 20:46:51.116606 7fa3f3bcbf00  4 rocksdb:     Options.sst_file_manager.rate_bytes_per_sec: 0\n2018-06-05 20:46:51.116607 7fa3f3bcbf00  4 rocksdb:                          Options.bytes_per_sync: 0\n2018-06-05 20:46:51.116607 7fa3f3bcbf00  4 rocksdb:                      Options.wal_bytes_per_sync: 0\n2018-06-05 20:46:51.116608 7fa3f3bcbf00  4 rocksdb:                       Options.wal_recovery_mode: 2\n2018-06-05 20:46:51.116609 7fa3f3bcbf00  4 rocksdb:                  Options.enable_thread_tracking: 0\n2018-06-05 20:46:51.116610 7fa3f3bcbf00  4 rocksdb:         Options.allow_concurrent_memtable_write: 1\n2018-06-05 20:46:51.116611 7fa3f3bcbf00  4 rocksdb:      Options.enable_write_thread_adaptive_yield: 1\n2018-06-05 20:46:51.116611 7fa3f3bcbf00  4 rocksdb:             Options.write_thread_max_yield_usec: 100\n2018-06-05 20:46:51.116612 7fa3f3bcbf00  4 rocksdb:            Options.write_thread_slow_yield_usec: 3\n2018-06-05 20:46:51.116613 7fa3f3bcbf00  4 rocksdb:                               Options.row_cache: None\n2018-06-05 20:46:51.116614 7fa3f3bcbf00  4 rocksdb:                              Options.wal_filter: None\n2018-06-05 20:46:51.116615 7fa3f3bcbf00  4 rocksdb:             Options.avoid_flush_during_recovery: 0\n2018-06-05 20:46:51.116616 7fa3f3bcbf00  4 rocksdb:             Options.base_background_compactions: 1\n2018-06-05 20:46:51.116616 7fa3f3bcbf00  4 rocksdb:             Options.max_background_compactions: 1\n2018-06-05 20:46:51.116617 7fa3f3bcbf00  4 rocksdb:             Options.avoid_flush_during_shutdown: 0\n2018-06-05 20:46:51.116618 7fa3f3bcbf00  4 rocksdb:             Options.delayed_write_rate : 16777216\n2018-06-05 20:46:51.116619 7fa3f3bcbf00  4 rocksdb:             Options.max_total_wal_size: 0\n2018-06-05 20:46:51.116620 7fa3f3bcbf00  4 rocksdb:             Options.delete_obsolete_files_period_micros: 21600000000\n2018-06-05 20:46:51.116621 7fa3f3bcbf00  4 rocksdb:                   Options.stats_dump_period_sec: 600\n2018-06-05 20:46:51.116622 7fa3f3bcbf00  4 rocksdb: Compression algorithms supported:\n2018-06-05 20:46:51.116623 7fa3f3bcbf00  4 rocksdb: \tSnappy supported: 0\n2018-06-05 20:46:51.116624 7fa3f3bcbf00  4 rocksdb: \tZlib supported: 0\n2018-06-05 20:46:51.116624 7fa3f3bcbf00  4 rocksdb: \tBzip supported: 0\n2018-06-05 20:46:51.116625 7fa3f3bcbf00  4 rocksdb: \tLZ4 supported: 0\n2018-06-05 20:46:51.116626 7fa3f3bcbf00  4 rocksdb: \tZSTD supported: 0\n2018-06-05 20:46:51.116627 7fa3f3bcbf00  4 rocksdb: Fast CRC32 supported: 1\n2018-06-05 20:46:51.116741 7fa3f3bcbf00  4 rocksdb: [/build/ceph-12.2.5/src/rocksdb/db/version_set.cc:2609] Recovering from manifest file: MANIFEST-000001\n\n2018-06-05 20:46:51.116796 7fa3f3bcbf00  4 rocksdb: [/build/ceph-12.2.5/src/rocksdb/db/column_family.cc:407] --------------- Options for column family [default]:\n\n2018-06-05 20:46:51.116798 7fa3f3bcbf00  4 rocksdb:               Options.comparator: leveldb.BytewiseComparator\n2018-06-05 20:46:51.116800 7fa3f3bcbf00  4 rocksdb:           Options.merge_operator: \n2018-06-05 20:46:51.116801 7fa3f3bcbf00  4 rocksdb:        Options.compaction_filter: None\n2018-06-05 20:46:51.116802 7fa3f3bcbf00  4 rocksdb:        Options.compaction_filter_factory: None\n2018-06-05 20:46:51.116803 7fa3f3bcbf00  4 rocksdb:         Options.memtable_factory: SkipListFactory\n2018-06-05 20:46:51.116804 7fa3f3bcbf00  4 rocksdb:            Options.table_factory: BlockBasedTable\n2018-06-05 20:46:51.116823 7fa3f3bcbf00  4 rocksdb:            table_factory options:   flush_block_policy_factory: FlushBlockBySizePolicyFactory (0x5562dd2d80f8)\n  cache_index_and_filter_blocks: 1\n  cache_index_and_filter_blocks_with_high_priority: 1\n  pin_l0_filter_and_index_blocks_in_cache: 1\n  index_type: 0\n  hash_index_allow_collision: 1\n  checksum: 1\n  no_block_cache: 0\n  block_cache: 0x5562dd5e45c0\n  block_cache_name: LRUCache\n  block_cache_options:\n    capacity : 134217728\n    num_shard_bits : 4\n    strict_capacity_limit : 0\n    high_pri_pool_ratio: 0.000\n  block_cache_compressed: (nil)\n  persistent_cache: (nil)\n  block_size: 4096\n  block_size_deviation: 10\n  block_restart_interval: 16\n  index_block_restart_interval: 1\n  filter_policy: rocksdb.BuiltinBloomFilter\n  whole_key_filtering: 1\n  format_version: 2\n\n2018-06-05 20:46:51.116827 7fa3f3bcbf00  4 rocksdb:        Options.write_buffer_size: 33554432\n2018-06-05 20:46:51.116828 7fa3f3bcbf00  4 rocksdb:  Options.max_write_buffer_number: 2\n2018-06-05 20:46:51.116829 7fa3f3bcbf00  4 rocksdb:          Options.compression: NoCompression\n2018-06-05 20:46:51.116830 7fa3f3bcbf00  4 rocksdb:                  Options.bottommost_compression: Disabled\n2018-06-05 20:46:51.116831 7fa3f3bcbf00  4 rocksdb:       Options.prefix_extractor: nullptr\n2018-06-05 20:46:51.116831 7fa3f3bcbf00  4 rocksdb:   Options.memtable_insert_with_hint_prefix_extractor: nullptr\n2018-06-05 20:46:51.116832 7fa3f3bcbf00  4 rocksdb:             Options.num_levels: 7\n2018-06-05 20:46:51.116833 7fa3f3bcbf00  4 rocksdb:        Options.min_write_buffer_number_to_merge: 1\n2018-06-05 20:46:51.116834 7fa3f3bcbf00  4 rocksdb:     Options.max_write_buffer_number_to_maintain: 0\n2018-06-05 20:46:51.116834 7fa3f3bcbf00  4 rocksdb:            Options.compression_opts.window_bits: -14\n2018-06-05 20:46:51.116835 7fa3f3bcbf00  4 rocksdb:                  Options.compression_opts.level: -1\n2018-06-05 20:46:51.116836 7fa3f3bcbf00  4 rocksdb:               Options.compression_opts.strategy: 0\n2018-06-05 20:46:51.116837 7fa3f3bcbf00  4 rocksdb:         Options.compression_opts.max_dict_bytes: 0\n2018-06-05 20:46:51.116838 7fa3f3bcbf00  4 rocksdb:      Options.level0_file_num_compaction_trigger: 4\n2018-06-05 20:46:51.116838 7fa3f3bcbf00  4 rocksdb:          Options.level0_slowdown_writes_trigger: 20\n2018-06-05 20:46:51.116839 7fa3f3bcbf00  4 rocksdb:              Options.level0_stop_writes_trigger: 36\n2018-06-05 20:46:51.116839 7fa3f3bcbf00  4 rocksdb:                   Options.target_file_size_base: 67108864\n2018-06-05 20:46:51.116840 7fa3f3bcbf00  4 rocksdb:             Options.target_file_size_multiplier: 1\n2018-06-05 20:46:51.116841 7fa3f3bcbf00  4 rocksdb:                Options.max_bytes_for_level_base: 268435456\n2018-06-05 20:46:51.116842 7fa3f3bcbf00  4 rocksdb: Options.level_compaction_dynamic_level_bytes: 0\n2018-06-05 20:46:51.116842 7fa3f3bcbf00  4 rocksdb:          Options.max_bytes_for_level_multiplier: 10.000000\n2018-06-05 20:46:51.116845 7fa3f3bcbf00  4 rocksdb: Options.max_bytes_for_level_multiplier_addtl[0]: 1\n2018-06-05 20:46:51.116852 7fa3f3bcbf00  4 rocksdb: Options.max_bytes_for_level_multiplier_addtl[1]: 1\n2018-06-05 20:46:51.116853 7fa3f3bcbf00  4 rocksdb: Options.max_bytes_for_level_multiplier_addtl[2]: 1\n2018-06-05 20:46:51.116853 7fa3f3bcbf00  4 rocksdb: Options.max_bytes_for_level_multiplier_addtl[3]: 1\n2018-06-05 20:46:51.116854 7fa3f3bcbf00  4 rocksdb: Options.max_bytes_for_level_multiplier_addtl[4]: 1\n2018-06-05 20:46:51.116855 7fa3f3bcbf00  4 rocksdb: Options.max_bytes_for_level_multiplier_addtl[5]: 1\n2018-06-05 20:46:51.116855 7fa3f3bcbf00  4 rocksdb: Options.max_bytes_for_level_multiplier_addtl[6]: 1\n2018-06-05 20:46:51.116856 7fa3f3bcbf00  4 rocksdb:       Options.max_sequential_skip_in_iterations: 8\n2018-06-05 20:46:51.116857 7fa3f3bcbf00  4 rocksdb:                    Options.max_compaction_bytes: 1677721600\n2018-06-05 20:46:51.116858 7fa3f3bcbf00  4 rocksdb:                        Options.arena_block_size: 4194304\n2018-06-05 20:46:51.116858 7fa3f3bcbf00  4 rocksdb:   Options.soft_pending_compaction_bytes_limit: 68719476736\n2018-06-05 20:46:51.116859 7fa3f3bcbf00  4 rocksdb:   Options.hard_pending_compaction_bytes_limit: 274877906944\n2018-06-05 20:46:51.116860 7fa3f3bcbf00  4 rocksdb:       Options.rate_limit_delay_max_milliseconds: 100\n2018-06-05 20:46:51.116860 7fa3f3bcbf00  4 rocksdb:                Options.disable_auto_compactions: 0\n2018-06-05 20:46:51.116861 7fa3f3bcbf00  4 rocksdb:                         Options.compaction_style: kCompactionStyleLevel\n2018-06-05 20:46:51.116863 7fa3f3bcbf00  4 rocksdb:                           Options.compaction_pri: kByCompensatedSize\n2018-06-05 20:46:51.116864 7fa3f3bcbf00  4 rocksdb:  Options.compaction_options_universal.size_ratio: 1\n2018-06-05 20:46:51.116864 7fa3f3bcbf00  4 rocksdb: Options.compaction_options_universal.min_merge_width: 2\n2018-06-05 20:46:51.116865 7fa3f3bcbf00  4 rocksdb: Options.compaction_options_universal.max_merge_width: 4294967295\n2018-06-05 20:46:51.116866 7fa3f3bcbf00  4 rocksdb: Options.compaction_options_universal.max_size_amplification_percent: 200\n2018-06-05 20:46:51.116867 7fa3f3bcbf00  4 rocksdb: Options.compaction_options_universal.compression_size_percent: -1\n2018-06-05 20:46:51.116868 7fa3f3bcbf00  4 rocksdb: Options.compaction_options_fifo.max_table_files_size: 1073741824\n2018-06-05 20:46:51.116868 7fa3f3bcbf00  4 rocksdb:                   Options.table_properties_collectors: \n2018-06-05 20:46:51.116869 7fa3f3bcbf00  4 rocksdb:                   Options.inplace_update_support: 0\n2018-06-05 20:46:51.116870 7fa3f3bcbf00  4 rocksdb:                 Options.inplace_update_num_locks: 10000\n2018-06-05 20:46:51.116870 7fa3f3bcbf00  4 rocksdb:               Options.memtable_prefix_bloom_size_ratio: 0.000000\n2018-06-05 20:46:51.116871 7fa3f3bcbf00  4 rocksdb:   Options.memtable_huge_page_size: 0\n2018-06-05 20:46:51.116872 7fa3f3bcbf00  4 rocksdb:                           Options.bloom_locality: 0\n2018-06-05 20:46:51.116873 7fa3f3bcbf00  4 rocksdb:                    Options.max_successive_merges: 0\n2018-06-05 20:46:51.116874 7fa3f3bcbf00  4 rocksdb:                Options.optimize_filters_for_hits: 0\n2018-06-05 20:46:51.116874 7fa3f3bcbf00  4 rocksdb:                Options.paranoid_file_checks: 0\n2018-06-05 20:46:51.116875 7fa3f3bcbf00  4 rocksdb:                Options.force_consistency_checks: 0\n2018-06-05 20:46:51.116876 7fa3f3bcbf00  4 rocksdb:                Options.report_bg_io_stats: 0\n2018-06-05 20:46:51.118430 7fa3f3bcbf00  4 rocksdb: [/build/ceph-12.2.5/src/rocksdb/db/version_set.cc:2859] Recovered from manifest file:/var/lib/ceph/mon/ceph-node/store.db/MANIFEST-000001 succeeded,manifest_file_number is 1, next_file_number is 3, last_sequence is 0, log_number is 0,prev_log_number is 0,max_column_family is 0\n\n2018-06-05 20:46:51.118438 7fa3f3bcbf00  4 rocksdb: [/build/ceph-12.2.5/src/rocksdb/db/version_set.cc:2867] Column family [default] (ID 0), log number is 0\n\n2018-06-05 20:46:51.118497 7fa3f3bcbf00  4 rocksdb: EVENT_LOG_v1 {\"time_micros\": 1528231611118489, \"job\": 1, \"event\": \"recovery_started\", \"log_files\": [3]}\n2018-06-05 20:46:51.118502 7fa3f3bcbf00  4 rocksdb: [/build/ceph-12.2.5/src/rocksdb/db/db_impl_open.cc:482] Recovering log #3 mode 2\n2018-06-05 20:46:51.123487 7fa3f3bcbf00  4 rocksdb: EVENT_LOG_v1 {\"time_micros\": 1528231611123475, \"cf_name\": \"default\", \"job\": 1, \"event\": \"table_file_creation\", \"file_number\": 4, \"file_size\": 1420, \"table_properties\": {\"data_size\": 463, \"index_size\": 28, \"filter_size\": 20, \"raw_key_size\": 95, \"raw_average_key_size\": 23, \"raw_value_size\": 359, \"raw_average_value_size\": 89, \"num_data_blocks\": 1, \"num_entries\": 4, \"filter_policy_name\": \"rocksdb.BuiltinBloomFilter\", \"kDeletedKeys\": \"0\", \"kMergeOperands\": \"0\"}}\n2018-06-05 20:46:51.123513 7fa3f3bcbf00  4 rocksdb: [/build/ceph-12.2.5/src/rocksdb/db/version_set.cc:2395] Creating manifest 5\n\n2018-06-05 20:46:51.132488 7fa3f3bcbf00  4 rocksdb: EVENT_LOG_v1 {\"time_micros\": 1528231611132485, \"job\": 1, \"event\": \"recovery_finished\"}\n2018-06-05 20:46:51.142873 7fa3f3bcbf00  4 rocksdb: [/build/ceph-12.2.5/src/rocksdb/db/db_impl_open.cc:1063] DB pointer 0x5562dd6f8000\n2018-06-05 20:46:51.144169 7fa3f3bcbf00  0 starting mon.node rank 0 at public addr 128.110.153.197:6789/0 at bind addr 128.110.153.197:6789/0 mon_data /var/lib/ceph/mon/ceph-node fsid b2bb6c0b-6cc6-44f1-bf92-1ce81d78fbdc\n2018-06-05 20:46:51.144370 7fa3f3bcbf00  0 starting mon.node rank 0 at 128.110.153.197:6789/0 mon_data /var/lib/ceph/mon/ceph-node fsid b2bb6c0b-6cc6-44f1-bf92-1ce81d78fbdc\n2018-06-05 20:46:51.145368 7fa3f3bcbf00  1 mon.node@-1(probing) e0 preinit fsid b2bb6c0b-6cc6-44f1-bf92-1ce81d78fbdc\n2018-06-05 20:46:51.146062 7fa3f3bcbf00  1 mon.node@-1(probing).mds e0 Unable to load 'last_metadata'\n2018-06-05 20:46:51.146904 7fa3f3bcbf00  0 mon.node@-1(probing) e0  my rank is now 0 (was -1)\n2018-06-05 20:46:51.146941 7fa3f3bcbf00  1 mon.node@0(probing) e0 win_standalone_election\n2018-06-05 20:46:51.146961 7fa3f3bcbf00  1 mon.node@0(probing).elector(0) init, first boot, initializing epoch at 1 \n2018-06-05 20:46:51.151879 7fa3f3bcbf00  0 log_channel(cluster) log [INF] : mon.node is new leader, mons node in quorum (ranks 0)\n2018-06-05 20:46:51.156816 7fa3f3bcbf00  1 mon.node@0(leader).osd e0 create_pending setting backfillfull_ratio = 0.9\n2018-06-05 20:46:51.156839 7fa3f3bcbf00  1 mon.node@0(leader).osd e0 create_pending setting full_ratio = 0.95\n2018-06-05 20:46:51.156844 7fa3f3bcbf00  1 mon.node@0(leader).osd e0 create_pending setting nearfull_ratio = 0.85\n2018-06-05 20:46:51.157098 7fa3f3bcbf00  1 mon.node@0(leader).osd e0 encode_pending skipping prime_pg_temp; mapping job did not start\n2018-06-05 20:46:51.157987 7fa3f3bcbf00  1 mon.node@0(leader) e0 _apply_compatset_features enabling new quorum features: compat={},rocompat={},incompat={4=support erasure code pools,5=new-style osdmap encoding,6=support isa/lrc erasure code,7=support shec erasure code}\n2018-06-05 20:46:51.165120 7fa3e61d5700  1 mon.node@0(leader).paxosservice(pgmap 0..0) refresh upgraded, format 1 -> 0\n2018-06-05 20:46:51.165132 7fa3e61d5700  1 mon.node@0(leader).pg v0 on_upgrade discarding in-core PGMap\n2018-06-05 20:46:51.169633 7fa3e61d5700  1 mon.node@0(leader).paxosservice(auth 0..0) refresh upgraded, format 2 -> 0\n2018-06-05 20:46:51.169810 7fa3e61d5700  1 mon.node@0(probing) e1 win_standalone_election\n2018-06-05 20:46:51.169829 7fa3e61d5700  1 mon.node@0(probing).elector(2) init, last seen epoch 2\n2018-06-05 20:46:51.174339 7fa3e61d5700  0 log_channel(cluster) log [INF] : mon.node is new leader, mons node in quorum (ranks 0)\n2018-06-05 20:46:51.178730 7fa3e61d5700  0 log_channel(cluster) log [DBG] : monmap e1: 1 mons at {node=128.110.153.197:6789/0}\n2018-06-05 20:46:51.182745 7fa3e61d5700  1 mon.node@0(leader).osd e0 create_pending setting backfillfull_ratio = 0.9\n2018-06-05 20:46:51.182765 7fa3e61d5700  1 mon.node@0(leader).osd e0 create_pending setting full_ratio = 0.95\n2018-06-05 20:46:51.182771 7fa3e61d5700  1 mon.node@0(leader).osd e0 create_pending setting nearfull_ratio = 0.85\n2018-06-05 20:46:51.183004 7fa3e61d5700  1 mon.node@0(leader).osd e0 encode_pending skipping prime_pg_temp; mapping job did not start\n2018-06-05 20:46:51.183496 7fa3e61d5700  1 mon.node@0(leader) e1 _apply_compatset_features enabling new quorum features: compat={},rocompat={},incompat={8=support monmap features,9=luminous ondisk layout}\n2018-06-05 20:46:51.192055 7fa3e61d5700  1 mon.node@0(leader).paxosservice(auth 0..0) refresh upgraded, format 2 -> 0\n2018-06-05 20:46:51.192171 7fa3e61d5700  0 log_channel(cluster) log [INF] : pgmap 0 pgs: ; 0 bytes data, 0 kB used, 0 kB / 0 kB avail\n2018-06-05 20:46:51.201992 7fa3e61d5700  0 mon.node@0(leader).mds e1 print_map\ne1\nenable_multiple, ever_enabled_multiple: 0,0\ncompat: compat={},rocompat={},incompat={1=base v0.20,2=client writeable ranges,3=default file layouts on dirs,4=dir inode in separate object,5=mds uses versioned encoding,6=dirfrag is stored in omap,8=no anchor table,9=file layout v2}\nlegacy client fscid: -1\n \nNo filesystems configured\n\n2018-06-05 20:46:51.202098 7fa3e61d5700  1 mon.node@0(leader).osd e1 e1: 0 total, 0 up, 0 in\n2018-06-05 20:46:51.206194 7fa3e61d5700  0 mon.node@0(leader).osd e1 crush map has features 288514050185494528, adjusting msgr requires\n2018-06-05 20:46:51.206204 7fa3e61d5700  0 mon.node@0(leader).osd e1 crush map has features 288514050185494528, adjusting msgr requires\n2018-06-05 20:46:51.206211 7fa3e61d5700  0 mon.node@0(leader).osd e1 crush map has features 1009089990564790272, adjusting msgr requires\n2018-06-05 20:46:51.206214 7fa3e61d5700  0 mon.node@0(leader).osd e1 crush map has features 288514050185494528, adjusting msgr requires\n2018-06-05 20:46:51.206440 7fa3e61d5700  1 mon.node@0(leader).log v1 unable to write to '/var/log/ceph/ceph.log' for channel 'cluster': (2) No such file or directory\n2018-06-05 20:46:51.206502 7fa3e61d5700  1 mon.node@0(leader).paxosservice(auth 1..1) refresh upgraded, format 0 -> 2\n2018-06-05 20:46:51.207103 7fa3e61d5700  0 log_channel(cluster) log [DBG] : fsmap \n2018-06-05 20:46:51.207184 7fa3e61d5700  1 mon.node@0(leader).pg v1 check_osd_map will clear pg_map data\n2018-06-05 20:46:51.207187 7fa3e61d5700  1 mon.node@0(leader).pg v1 encode_pending clearing pgmap data at v2\n2018-06-05 20:46:51.211449 7fa3e61d5700  0 log_channel(cluster) log [DBG] : osdmap e1: 0 total, 0 up, 0 in\n2018-06-05 20:46:51.211646 7fa3e61d5700  0 log_channel(cluster) log [DBG] : mgrmap e1: no daemons active\n2018-06-05 20:46:51.225999 7fa3e61d5700  1 mon.node@0(leader).log v2 unable to write to '/var/log/ceph/ceph.log' for channel 'cluster': (2) No such file or directory\n2018-06-05 20:46:51.420409 7fa3ea9de700  0 mon.node@0(leader) e1 handle_command mon_command({\"prefix\": \"status\"} v 0) v1\n2018-06-05 20:46:51.420448 7fa3ea9de700  0 log_channel(audit) log [DBG] : from='client.? 128.110.153.138:0/3019500679' entity='client.admin' cmd=[{\"prefix\": \"status\"}]: dispatch\n2018-06-05 20:46:51.710839 7fa3ea9de700  0 mon.node@0(leader) e1 handle_command mon_command({\"pool2\": \"rbd\", \"prefix\": \"osd pool rm\", \"sure\": \"--yes-i-really-really-mean-it\", \"pool\": \"rbd\"} v 0) v1\n2018-06-05 20:46:51.710889 7fa3ea9de700  0 log_channel(audit) log [INF] : from='client.? 128.110.153.138:0/3040884711' entity='client.admin' cmd=[{\"pool2\": \"rbd\", \"prefix\": \"osd pool rm\", \"sure\": \"--yes-i-really-really-mean-it\", \"pool\": \"rbd\"}]: dispatch\n2018-06-05 20:46:52.003364 7fa3ea9de700  0 mon.node@0(leader) e1 handle_command mon_command({\"prefix\": \"osd pool create\", \"pg_num\": 150, \"pool\": \"scbench\", \"pgp_num\": 150} v 0) v1\n2018-06-05 20:46:52.003420 7fa3ea9de700  0 log_channel(audit) log [INF] : from='client.? 128.110.153.138:0/2449834230' entity='client.admin' cmd=[{\"prefix\": \"osd pool create\", \"pg_num\": 150, \"pool\": \"scbench\", \"pgp_num\": 150}]: dispatch\n2018-06-05 20:46:52.188370 7fa3ea9de700  1 mon.node@0(leader).osd e1 encode_pending skipping prime_pg_temp; mapping job did not start\n2018-06-05 20:46:52.198258 7fa3e61d5700  1 mon.node@0(leader).osd e2 e2: 0 total, 0 up, 0 in\n2018-06-05 20:46:52.202873 7fa3e61d5700  0 mon.node@0(leader).osd e2 crush map has features 288514051259236352, adjusting msgr requires\n2018-06-05 20:46:52.202902 7fa3e61d5700  0 mon.node@0(leader).osd e2 crush map has features 288514051259236352, adjusting msgr requires\n2018-06-05 20:46:52.202908 7fa3e61d5700  0 mon.node@0(leader).osd e2 crush map has features 1009089991638532096, adjusting msgr requires\n2018-06-05 20:46:52.202912 7fa3e61d5700  0 mon.node@0(leader).osd e2 crush map has features 288514051259236352, adjusting msgr requires\n2018-06-05 20:46:52.203251 7fa3e61d5700  0 log_channel(audit) log [INF] : from='client.? 128.110.153.138:0/2449834230' entity='client.admin' cmd='[{\"prefix\": \"osd pool create\", \"pg_num\": 150, \"pool\": \"scbench\", \"pgp_num\": 150}]': finished\n2018-06-05 20:46:52.203433 7fa3e61d5700  0 log_channel(cluster) log [DBG] : osdmap e2: 0 total, 0 up, 0 in\n2018-06-05 20:46:52.235121 7fa3e61d5700  1 mon.node@0(leader).log v3 unable to write to '/var/log/ceph/ceph.audit.log' for channel 'audit': (2) No such file or directory\n2018-06-05 20:46:52.235153 7fa3e61d5700  1 mon.node@0(leader).log v3 unable to write to '/var/log/ceph/ceph.log' for channel 'cluster': (2) No such file or directory\n2018-06-05 20:46:52.519482 7fa3ea9de700  0 mon.node@0(leader) e1 handle_command mon_command({\"var\": \"size\", \"prefix\": \"osd pool set\", \"pool\": \"scbench\", \"val\": \"1\"} v 0) v1\n2018-06-05 20:46:52.519560 7fa3ea9de700  0 log_channel(audit) log [INF] : from='client.? 128.110.153.138:0/1066102251' entity='client.admin' cmd=[{\"var\": \"size\", \"prefix\": \"osd pool set\", \"pool\": \"scbench\", \"val\": \"1\"}]: dispatch\n2018-06-05 20:46:53.244865 7fa3e61d5700  1 mon.node@0(leader).log v4 unable to write to '/var/log/ceph/ceph.audit.log' for channel 'audit': (2) No such file or directory\n2018-06-05 20:46:53.254684 7fa3e61d5700  1 mon.node@0(leader).osd e3 e3: 0 total, 0 up, 0 in\n2018-06-05 20:46:53.259708 7fa3e61d5700  0 log_channel(audit) log [INF] : from='client.? 128.110.153.138:0/1066102251' entity='client.admin' cmd='[{\"var\": \"size\", \"prefix\": \"osd pool set\", \"pool\": \"scbench\", \"val\": \"1\"}]': finished\n2018-06-05 20:46:53.259888 7fa3e61d5700  0 log_channel(cluster) log [DBG] : osdmap e3: 0 total, 0 up, 0 in\n2018-06-05 20:46:54.264143 7fa3e61d5700  1 mon.node@0(leader).log v5 unable to write to '/var/log/ceph/ceph.audit.log' for channel 'audit': (2) No such file or directory\n2018-06-05 20:47:51.146455 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB\n2018-06-05 20:48:51.146894 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB\n2018-06-05 20:49:51.147295 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB\n2018-06-05 20:50:51.147738 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB\n2018-06-05 20:51:51.148184 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB\n2018-06-05 20:52:51.148586 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB\n2018-06-05 20:53:51.149009 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB\n2018-06-05 20:54:51.149427 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB\n2018-06-05 20:55:51.149744 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB\n2018-06-05 20:56:51.150182 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB\n2018-06-05 20:57:51.150596 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB\n2018-06-05 20:58:51.151036 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB\n2018-06-05 20:59:51.151455 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB\n2018-06-05 21:00:51.151862 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB\n2018-06-05 21:01:51.152272 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB\n2018-06-05 21:02:51.152670 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB\n2018-06-05 21:03:51.153176 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB\n2018-06-05 21:04:51.153577 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB\n2018-06-05 21:05:51.154043 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB\n2018-06-05 21:06:51.154486 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB\n2018-06-05 21:07:51.154907 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB\n2018-06-05 21:08:51.155322 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB\n2018-06-05 21:09:51.155786 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB\n2018-06-05 21:10:51.156239 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB\n2018-06-05 21:11:51.156697 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB\n2018-06-05 21:12:51.157235 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB\n2018-06-05 21:13:51.157578 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB\n2018-06-05 21:14:51.157995 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB\n2018-06-05 21:15:51.158413 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB\n2018-06-05 21:16:51.159008 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB\n2018-06-05 21:17:51.159403 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB\n2018-06-05 21:18:51.159836 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB\n2018-06-05 21:19:51.160240 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB\n2018-06-05 21:20:51.160755 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB\n2018-06-05 21:21:51.161225 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB\n2018-06-05 21:22:51.161818 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB\n2018-06-05 21:23:51.162213 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB\n2018-06-05 21:24:51.162648 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB\n2018-06-05 21:25:51.163040 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB\n2018-06-05 21:26:51.163439 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB\n2018-06-05 21:27:51.163834 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB\n2018-06-05 21:28:51.164233 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB\n2018-06-05 21:29:51.164623 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB\n2018-06-05 21:30:51.165050 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB\n2018-06-05 21:31:51.165667 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB\n2018-06-05 21:32:51.166088 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB\n2018-06-05 21:33:51.166524 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB\n2018-06-05 21:34:51.166909 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB\n2018-06-05 21:35:51.167298 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB\n2018-06-05 21:36:51.167844 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB\n2018-06-05 21:37:51.168251 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12380 MB\n2018-06-05 21:38:51.168873 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB\n2018-06-05 21:39:51.169280 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB\n2018-06-05 21:40:51.169738 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB\n2018-06-05 21:41:51.170135 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB\n2018-06-05 21:42:51.170577 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB\n2018-06-05 21:43:51.170974 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB\n2018-06-05 21:44:51.171402 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB\n2018-06-05 21:45:51.171847 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB\n2018-06-05 21:46:51.172492 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB\n2018-06-05 21:47:51.172897 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB\n2018-06-05 21:48:51.173296 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB\n2018-06-05 21:49:51.173728 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB\n2018-06-05 21:50:51.174158 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB\n2018-06-05 21:51:51.174553 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB\n2018-06-05 21:52:51.174945 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB\n2018-06-05 21:53:51.175346 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB\n2018-06-05 21:54:51.175960 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB\n2018-06-05 21:55:51.176364 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB\n2018-06-05 21:56:51.176756 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB\n2018-06-05 21:57:51.177148 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB\n2018-06-05 21:58:51.177540 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB\n2018-06-05 21:59:51.177950 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB\n2018-06-05 22:00:51.178360 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB\n2018-06-05 22:01:51.178830 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB\n2018-06-05 22:02:51.179298 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB\n2018-06-05 22:03:51.179713 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB\n2018-06-05 22:04:51.180152 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12380 MB\n2018-06-05 22:05:51.180557 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12380 MB\n2018-06-05 22:06:51.180938 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12380 MB\n2018-06-05 22:07:51.181332 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12380 MB\n2018-06-05 22:08:51.181789 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12380 MB\n2018-06-05 22:09:51.182156 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12380 MB\n2018-06-05 22:10:51.182584 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12380 MB\n2018-06-05 22:11:51.182988 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12380 MB\n2018-06-05 22:12:51.183383 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12380 MB\n2018-06-05 22:13:51.183786 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12380 MB\n2018-06-05 22:14:51.184188 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12380 MB\n2018-06-05 22:15:51.184631 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12380 MB\n2018-06-05 22:16:51.185046 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12380 MB\n2018-06-05 22:17:51.185486 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12380 MB\n2018-06-05 22:18:51.185918 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12380 MB\n2018-06-05 22:19:51.186320 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12380 MB\n2018-06-05 22:20:51.186733 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12380 MB\n2018-06-05 22:21:51.187168 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12380 MB\n2018-06-05 22:22:51.187597 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12380 MB\n2018-06-05 22:23:51.187977 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12380 MB\n2018-06-05 22:24:51.188388 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12380 MB\n2018-06-05 22:25:51.188851 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12380 MB\n2018-06-05 22:26:51.189284 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12380 MB\n2018-06-05 22:27:51.189762 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12380 MB\n2018-06-05 22:27:52.459108 7fa3e81d9700 -1 received  signal: Terminated from  PID: 1 task name:  UID: 0\n2018-06-05 22:27:52.459137 7fa3e81d9700 -1 mon.node@0(leader) e1 *** Got Signal Terminated ***\n2018-06-05 22:27:52.459144 7fa3e81d9700  1 mon.node@0(leader) e1 shutdown", "stderr_lines": ["+ '[' -z True ']'", "+ '[' -z 128.110.152.0/22 ']'", "+ '[' -z 128.110.153.197 ']'", "+ sed -i -e s@monip@128.110.153.197@ /etc/ceph/ceph.conf", "+ sed -i -e s@cephnet@128.110.152.0/22@ /etc/ceph/ceph.conf", "+ '[' -f /etc/ceph/extra.conf ']'", "+ '[' -n True ']'", "+ '[' -n '' ']'", "+ exec /entrypoint.sh mon", "2018-06-05 20:46:51.108769 7fa3f3bcbf00  0 set uid:gid to 64045:64045 (ceph:ceph)", "2018-06-05 20:46:51.108785 7fa3f3bcbf00  0 ceph version 12.2.5 (cad919881333ac92274171586c827e01f554a70a) luminous (stable), process (unknown), pid 122", "2018-06-05 20:46:51.108867 7fa3f3bcbf00  0 pidfile_write: ignore empty --pid-file", "2018-06-05 20:46:51.116239 7fa3f3bcbf00  0 load: jerasure load: lrc load: isa ", "2018-06-05 20:46:51.116342 7fa3f3bcbf00  0  set rocksdb option compression = kNoCompression", "2018-06-05 20:46:51.116355 7fa3f3bcbf00  0  set rocksdb option write_buffer_size = 33554432", "2018-06-05 20:46:51.116378 7fa3f3bcbf00  0  set rocksdb option compression = kNoCompression", "2018-06-05 20:46:51.116381 7fa3f3bcbf00  0  set rocksdb option write_buffer_size = 33554432", "2018-06-05 20:46:51.116493 7fa3f3bcbf00  4 rocksdb: RocksDB version: 5.4.0", "", "2018-06-05 20:46:51.116503 7fa3f3bcbf00  4 rocksdb: Git sha rocksdb_build_git_sha:@0@", "2018-06-05 20:46:51.116505 7fa3f3bcbf00  4 rocksdb: Compile date Apr 23 2018", "2018-06-05 20:46:51.116506 7fa3f3bcbf00  4 rocksdb: DB SUMMARY", "", "2018-06-05 20:46:51.116549 7fa3f3bcbf00  4 rocksdb: CURRENT file:  CURRENT", "", "2018-06-05 20:46:51.116551 7fa3f3bcbf00  4 rocksdb: IDENTITY file:  IDENTITY", "", "2018-06-05 20:46:51.116560 7fa3f3bcbf00  4 rocksdb: MANIFEST file:  MANIFEST-000001 size: 13 Bytes", "", "2018-06-05 20:46:51.116562 7fa3f3bcbf00  4 rocksdb: SST files in /var/lib/ceph/mon/ceph-node/store.db dir, Total Num: 0, files: ", "", "2018-06-05 20:46:51.116564 7fa3f3bcbf00  4 rocksdb: Write Ahead Log file in /var/lib/ceph/mon/ceph-node/store.db: 000003.log size: 454 ; ", "", "2018-06-05 20:46:51.116566 7fa3f3bcbf00  4 rocksdb:                         Options.error_if_exists: 0", "2018-06-05 20:46:51.116567 7fa3f3bcbf00  4 rocksdb:                       Options.create_if_missing: 0", "2018-06-05 20:46:51.116568 7fa3f3bcbf00  4 rocksdb:                         Options.paranoid_checks: 1", "2018-06-05 20:46:51.116569 7fa3f3bcbf00  4 rocksdb:                                     Options.env: 0x5562db672020", "2018-06-05 20:46:51.116570 7fa3f3bcbf00  4 rocksdb:                                Options.info_log: 0x5562dd5fcc80", "2018-06-05 20:46:51.116571 7fa3f3bcbf00  4 rocksdb:                          Options.max_open_files: -1", "2018-06-05 20:46:51.116572 7fa3f3bcbf00  4 rocksdb:                Options.max_file_opening_threads: 16", "2018-06-05 20:46:51.116573 7fa3f3bcbf00  4 rocksdb:                               Options.use_fsync: 0", "2018-06-05 20:46:51.116573 7fa3f3bcbf00  4 rocksdb:                       Options.max_log_file_size: 0", "2018-06-05 20:46:51.116574 7fa3f3bcbf00  4 rocksdb:                  Options.max_manifest_file_size: 18446744073709551615", "2018-06-05 20:46:51.116576 7fa3f3bcbf00  4 rocksdb:                   Options.log_file_time_to_roll: 0", "2018-06-05 20:46:51.116576 7fa3f3bcbf00  4 rocksdb:                       Options.keep_log_file_num: 1000", "2018-06-05 20:46:51.116577 7fa3f3bcbf00  4 rocksdb:                    Options.recycle_log_file_num: 0", "2018-06-05 20:46:51.116585 7fa3f3bcbf00  4 rocksdb:                         Options.allow_fallocate: 1", "2018-06-05 20:46:51.116586 7fa3f3bcbf00  4 rocksdb:                        Options.allow_mmap_reads: 0", "2018-06-05 20:46:51.116587 7fa3f3bcbf00  4 rocksdb:                       Options.allow_mmap_writes: 0", "2018-06-05 20:46:51.116587 7fa3f3bcbf00  4 rocksdb:                        Options.use_direct_reads: 0", "2018-06-05 20:46:51.116588 7fa3f3bcbf00  4 rocksdb:                        Options.use_direct_io_for_flush_and_compaction: 0", "2018-06-05 20:46:51.116589 7fa3f3bcbf00  4 rocksdb:          Options.create_missing_column_families: 0", "2018-06-05 20:46:51.116590 7fa3f3bcbf00  4 rocksdb:                              Options.db_log_dir: ", "2018-06-05 20:46:51.116591 7fa3f3bcbf00  4 rocksdb:                                 Options.wal_dir: /var/lib/ceph/mon/ceph-node/store.db", "2018-06-05 20:46:51.116592 7fa3f3bcbf00  4 rocksdb:                Options.table_cache_numshardbits: 6", "2018-06-05 20:46:51.116593 7fa3f3bcbf00  4 rocksdb:                      Options.max_subcompactions: 1", "2018-06-05 20:46:51.116594 7fa3f3bcbf00  4 rocksdb:                  Options.max_background_flushes: 1", "2018-06-05 20:46:51.116594 7fa3f3bcbf00  4 rocksdb:                         Options.WAL_ttl_seconds: 0", "2018-06-05 20:46:51.116595 7fa3f3bcbf00  4 rocksdb:                       Options.WAL_size_limit_MB: 0", "2018-06-05 20:46:51.116596 7fa3f3bcbf00  4 rocksdb:             Options.manifest_preallocation_size: 4194304", "2018-06-05 20:46:51.116597 7fa3f3bcbf00  4 rocksdb:                     Options.is_fd_close_on_exec: 1", "2018-06-05 20:46:51.116598 7fa3f3bcbf00  4 rocksdb:                   Options.advise_random_on_open: 1", "2018-06-05 20:46:51.116599 7fa3f3bcbf00  4 rocksdb:                    Options.db_write_buffer_size: 0", "2018-06-05 20:46:51.116599 7fa3f3bcbf00  4 rocksdb:         Options.access_hint_on_compaction_start: 1", "2018-06-05 20:46:51.116601 7fa3f3bcbf00  4 rocksdb:  Options.new_table_reader_for_compaction_inputs: 0", "2018-06-05 20:46:51.116602 7fa3f3bcbf00  4 rocksdb:               Options.compaction_readahead_size: 0", "2018-06-05 20:46:51.116602 7fa3f3bcbf00  4 rocksdb:           Options.random_access_max_buffer_size: 1048576", "2018-06-05 20:46:51.116603 7fa3f3bcbf00  4 rocksdb:           Options.writable_file_max_buffer_size: 1048576", "2018-06-05 20:46:51.116604 7fa3f3bcbf00  4 rocksdb:                      Options.use_adaptive_mutex: 0", "2018-06-05 20:46:51.116605 7fa3f3bcbf00  4 rocksdb:                            Options.rate_limiter: (nil)", "2018-06-05 20:46:51.116606 7fa3f3bcbf00  4 rocksdb:     Options.sst_file_manager.rate_bytes_per_sec: 0", "2018-06-05 20:46:51.116607 7fa3f3bcbf00  4 rocksdb:                          Options.bytes_per_sync: 0", "2018-06-05 20:46:51.116607 7fa3f3bcbf00  4 rocksdb:                      Options.wal_bytes_per_sync: 0", "2018-06-05 20:46:51.116608 7fa3f3bcbf00  4 rocksdb:                       Options.wal_recovery_mode: 2", "2018-06-05 20:46:51.116609 7fa3f3bcbf00  4 rocksdb:                  Options.enable_thread_tracking: 0", "2018-06-05 20:46:51.116610 7fa3f3bcbf00  4 rocksdb:         Options.allow_concurrent_memtable_write: 1", "2018-06-05 20:46:51.116611 7fa3f3bcbf00  4 rocksdb:      Options.enable_write_thread_adaptive_yield: 1", "2018-06-05 20:46:51.116611 7fa3f3bcbf00  4 rocksdb:             Options.write_thread_max_yield_usec: 100", "2018-06-05 20:46:51.116612 7fa3f3bcbf00  4 rocksdb:            Options.write_thread_slow_yield_usec: 3", "2018-06-05 20:46:51.116613 7fa3f3bcbf00  4 rocksdb:                               Options.row_cache: None", "2018-06-05 20:46:51.116614 7fa3f3bcbf00  4 rocksdb:                              Options.wal_filter: None", "2018-06-05 20:46:51.116615 7fa3f3bcbf00  4 rocksdb:             Options.avoid_flush_during_recovery: 0", "2018-06-05 20:46:51.116616 7fa3f3bcbf00  4 rocksdb:             Options.base_background_compactions: 1", "2018-06-05 20:46:51.116616 7fa3f3bcbf00  4 rocksdb:             Options.max_background_compactions: 1", "2018-06-05 20:46:51.116617 7fa3f3bcbf00  4 rocksdb:             Options.avoid_flush_during_shutdown: 0", "2018-06-05 20:46:51.116618 7fa3f3bcbf00  4 rocksdb:             Options.delayed_write_rate : 16777216", "2018-06-05 20:46:51.116619 7fa3f3bcbf00  4 rocksdb:             Options.max_total_wal_size: 0", "2018-06-05 20:46:51.116620 7fa3f3bcbf00  4 rocksdb:             Options.delete_obsolete_files_period_micros: 21600000000", "2018-06-05 20:46:51.116621 7fa3f3bcbf00  4 rocksdb:                   Options.stats_dump_period_sec: 600", "2018-06-05 20:46:51.116622 7fa3f3bcbf00  4 rocksdb: Compression algorithms supported:", "2018-06-05 20:46:51.116623 7fa3f3bcbf00  4 rocksdb: \tSnappy supported: 0", "2018-06-05 20:46:51.116624 7fa3f3bcbf00  4 rocksdb: \tZlib supported: 0", "2018-06-05 20:46:51.116624 7fa3f3bcbf00  4 rocksdb: \tBzip supported: 0", "2018-06-05 20:46:51.116625 7fa3f3bcbf00  4 rocksdb: \tLZ4 supported: 0", "2018-06-05 20:46:51.116626 7fa3f3bcbf00  4 rocksdb: \tZSTD supported: 0", "2018-06-05 20:46:51.116627 7fa3f3bcbf00  4 rocksdb: Fast CRC32 supported: 1", "2018-06-05 20:46:51.116741 7fa3f3bcbf00  4 rocksdb: [/build/ceph-12.2.5/src/rocksdb/db/version_set.cc:2609] Recovering from manifest file: MANIFEST-000001", "", "2018-06-05 20:46:51.116796 7fa3f3bcbf00  4 rocksdb: [/build/ceph-12.2.5/src/rocksdb/db/column_family.cc:407] --------------- Options for column family [default]:", "", "2018-06-05 20:46:51.116798 7fa3f3bcbf00  4 rocksdb:               Options.comparator: leveldb.BytewiseComparator", "2018-06-05 20:46:51.116800 7fa3f3bcbf00  4 rocksdb:           Options.merge_operator: ", "2018-06-05 20:46:51.116801 7fa3f3bcbf00  4 rocksdb:        Options.compaction_filter: None", "2018-06-05 20:46:51.116802 7fa3f3bcbf00  4 rocksdb:        Options.compaction_filter_factory: None", "2018-06-05 20:46:51.116803 7fa3f3bcbf00  4 rocksdb:         Options.memtable_factory: SkipListFactory", "2018-06-05 20:46:51.116804 7fa3f3bcbf00  4 rocksdb:            Options.table_factory: BlockBasedTable", "2018-06-05 20:46:51.116823 7fa3f3bcbf00  4 rocksdb:            table_factory options:   flush_block_policy_factory: FlushBlockBySizePolicyFactory (0x5562dd2d80f8)", "  cache_index_and_filter_blocks: 1", "  cache_index_and_filter_blocks_with_high_priority: 1", "  pin_l0_filter_and_index_blocks_in_cache: 1", "  index_type: 0", "  hash_index_allow_collision: 1", "  checksum: 1", "  no_block_cache: 0", "  block_cache: 0x5562dd5e45c0", "  block_cache_name: LRUCache", "  block_cache_options:", "    capacity : 134217728", "    num_shard_bits : 4", "    strict_capacity_limit : 0", "    high_pri_pool_ratio: 0.000", "  block_cache_compressed: (nil)", "  persistent_cache: (nil)", "  block_size: 4096", "  block_size_deviation: 10", "  block_restart_interval: 16", "  index_block_restart_interval: 1", "  filter_policy: rocksdb.BuiltinBloomFilter", "  whole_key_filtering: 1", "  format_version: 2", "", "2018-06-05 20:46:51.116827 7fa3f3bcbf00  4 rocksdb:        Options.write_buffer_size: 33554432", "2018-06-05 20:46:51.116828 7fa3f3bcbf00  4 rocksdb:  Options.max_write_buffer_number: 2", "2018-06-05 20:46:51.116829 7fa3f3bcbf00  4 rocksdb:          Options.compression: NoCompression", "2018-06-05 20:46:51.116830 7fa3f3bcbf00  4 rocksdb:                  Options.bottommost_compression: Disabled", "2018-06-05 20:46:51.116831 7fa3f3bcbf00  4 rocksdb:       Options.prefix_extractor: nullptr", "2018-06-05 20:46:51.116831 7fa3f3bcbf00  4 rocksdb:   Options.memtable_insert_with_hint_prefix_extractor: nullptr", "2018-06-05 20:46:51.116832 7fa3f3bcbf00  4 rocksdb:             Options.num_levels: 7", "2018-06-05 20:46:51.116833 7fa3f3bcbf00  4 rocksdb:        Options.min_write_buffer_number_to_merge: 1", "2018-06-05 20:46:51.116834 7fa3f3bcbf00  4 rocksdb:     Options.max_write_buffer_number_to_maintain: 0", "2018-06-05 20:46:51.116834 7fa3f3bcbf00  4 rocksdb:            Options.compression_opts.window_bits: -14", "2018-06-05 20:46:51.116835 7fa3f3bcbf00  4 rocksdb:                  Options.compression_opts.level: -1", "2018-06-05 20:46:51.116836 7fa3f3bcbf00  4 rocksdb:               Options.compression_opts.strategy: 0", "2018-06-05 20:46:51.116837 7fa3f3bcbf00  4 rocksdb:         Options.compression_opts.max_dict_bytes: 0", "2018-06-05 20:46:51.116838 7fa3f3bcbf00  4 rocksdb:      Options.level0_file_num_compaction_trigger: 4", "2018-06-05 20:46:51.116838 7fa3f3bcbf00  4 rocksdb:          Options.level0_slowdown_writes_trigger: 20", "2018-06-05 20:46:51.116839 7fa3f3bcbf00  4 rocksdb:              Options.level0_stop_writes_trigger: 36", "2018-06-05 20:46:51.116839 7fa3f3bcbf00  4 rocksdb:                   Options.target_file_size_base: 67108864", "2018-06-05 20:46:51.116840 7fa3f3bcbf00  4 rocksdb:             Options.target_file_size_multiplier: 1", "2018-06-05 20:46:51.116841 7fa3f3bcbf00  4 rocksdb:                Options.max_bytes_for_level_base: 268435456", "2018-06-05 20:46:51.116842 7fa3f3bcbf00  4 rocksdb: Options.level_compaction_dynamic_level_bytes: 0", "2018-06-05 20:46:51.116842 7fa3f3bcbf00  4 rocksdb:          Options.max_bytes_for_level_multiplier: 10.000000", "2018-06-05 20:46:51.116845 7fa3f3bcbf00  4 rocksdb: Options.max_bytes_for_level_multiplier_addtl[0]: 1", "2018-06-05 20:46:51.116852 7fa3f3bcbf00  4 rocksdb: Options.max_bytes_for_level_multiplier_addtl[1]: 1", "2018-06-05 20:46:51.116853 7fa3f3bcbf00  4 rocksdb: Options.max_bytes_for_level_multiplier_addtl[2]: 1", "2018-06-05 20:46:51.116853 7fa3f3bcbf00  4 rocksdb: Options.max_bytes_for_level_multiplier_addtl[3]: 1", "2018-06-05 20:46:51.116854 7fa3f3bcbf00  4 rocksdb: Options.max_bytes_for_level_multiplier_addtl[4]: 1", "2018-06-05 20:46:51.116855 7fa3f3bcbf00  4 rocksdb: Options.max_bytes_for_level_multiplier_addtl[5]: 1", "2018-06-05 20:46:51.116855 7fa3f3bcbf00  4 rocksdb: Options.max_bytes_for_level_multiplier_addtl[6]: 1", "2018-06-05 20:46:51.116856 7fa3f3bcbf00  4 rocksdb:       Options.max_sequential_skip_in_iterations: 8", "2018-06-05 20:46:51.116857 7fa3f3bcbf00  4 rocksdb:                    Options.max_compaction_bytes: 1677721600", "2018-06-05 20:46:51.116858 7fa3f3bcbf00  4 rocksdb:                        Options.arena_block_size: 4194304", "2018-06-05 20:46:51.116858 7fa3f3bcbf00  4 rocksdb:   Options.soft_pending_compaction_bytes_limit: 68719476736", "2018-06-05 20:46:51.116859 7fa3f3bcbf00  4 rocksdb:   Options.hard_pending_compaction_bytes_limit: 274877906944", "2018-06-05 20:46:51.116860 7fa3f3bcbf00  4 rocksdb:       Options.rate_limit_delay_max_milliseconds: 100", "2018-06-05 20:46:51.116860 7fa3f3bcbf00  4 rocksdb:                Options.disable_auto_compactions: 0", "2018-06-05 20:46:51.116861 7fa3f3bcbf00  4 rocksdb:                         Options.compaction_style: kCompactionStyleLevel", "2018-06-05 20:46:51.116863 7fa3f3bcbf00  4 rocksdb:                           Options.compaction_pri: kByCompensatedSize", "2018-06-05 20:46:51.116864 7fa3f3bcbf00  4 rocksdb:  Options.compaction_options_universal.size_ratio: 1", "2018-06-05 20:46:51.116864 7fa3f3bcbf00  4 rocksdb: Options.compaction_options_universal.min_merge_width: 2", "2018-06-05 20:46:51.116865 7fa3f3bcbf00  4 rocksdb: Options.compaction_options_universal.max_merge_width: 4294967295", "2018-06-05 20:46:51.116866 7fa3f3bcbf00  4 rocksdb: Options.compaction_options_universal.max_size_amplification_percent: 200", "2018-06-05 20:46:51.116867 7fa3f3bcbf00  4 rocksdb: Options.compaction_options_universal.compression_size_percent: -1", "2018-06-05 20:46:51.116868 7fa3f3bcbf00  4 rocksdb: Options.compaction_options_fifo.max_table_files_size: 1073741824", "2018-06-05 20:46:51.116868 7fa3f3bcbf00  4 rocksdb:                   Options.table_properties_collectors: ", "2018-06-05 20:46:51.116869 7fa3f3bcbf00  4 rocksdb:                   Options.inplace_update_support: 0", "2018-06-05 20:46:51.116870 7fa3f3bcbf00  4 rocksdb:                 Options.inplace_update_num_locks: 10000", "2018-06-05 20:46:51.116870 7fa3f3bcbf00  4 rocksdb:               Options.memtable_prefix_bloom_size_ratio: 0.000000", "2018-06-05 20:46:51.116871 7fa3f3bcbf00  4 rocksdb:   Options.memtable_huge_page_size: 0", "2018-06-05 20:46:51.116872 7fa3f3bcbf00  4 rocksdb:                           Options.bloom_locality: 0", "2018-06-05 20:46:51.116873 7fa3f3bcbf00  4 rocksdb:                    Options.max_successive_merges: 0", "2018-06-05 20:46:51.116874 7fa3f3bcbf00  4 rocksdb:                Options.optimize_filters_for_hits: 0", "2018-06-05 20:46:51.116874 7fa3f3bcbf00  4 rocksdb:                Options.paranoid_file_checks: 0", "2018-06-05 20:46:51.116875 7fa3f3bcbf00  4 rocksdb:                Options.force_consistency_checks: 0", "2018-06-05 20:46:51.116876 7fa3f3bcbf00  4 rocksdb:                Options.report_bg_io_stats: 0", "2018-06-05 20:46:51.118430 7fa3f3bcbf00  4 rocksdb: [/build/ceph-12.2.5/src/rocksdb/db/version_set.cc:2859] Recovered from manifest file:/var/lib/ceph/mon/ceph-node/store.db/MANIFEST-000001 succeeded,manifest_file_number is 1, next_file_number is 3, last_sequence is 0, log_number is 0,prev_log_number is 0,max_column_family is 0", "", "2018-06-05 20:46:51.118438 7fa3f3bcbf00  4 rocksdb: [/build/ceph-12.2.5/src/rocksdb/db/version_set.cc:2867] Column family [default] (ID 0), log number is 0", "", "2018-06-05 20:46:51.118497 7fa3f3bcbf00  4 rocksdb: EVENT_LOG_v1 {\"time_micros\": 1528231611118489, \"job\": 1, \"event\": \"recovery_started\", \"log_files\": [3]}", "2018-06-05 20:46:51.118502 7fa3f3bcbf00  4 rocksdb: [/build/ceph-12.2.5/src/rocksdb/db/db_impl_open.cc:482] Recovering log #3 mode 2", "2018-06-05 20:46:51.123487 7fa3f3bcbf00  4 rocksdb: EVENT_LOG_v1 {\"time_micros\": 1528231611123475, \"cf_name\": \"default\", \"job\": 1, \"event\": \"table_file_creation\", \"file_number\": 4, \"file_size\": 1420, \"table_properties\": {\"data_size\": 463, \"index_size\": 28, \"filter_size\": 20, \"raw_key_size\": 95, \"raw_average_key_size\": 23, \"raw_value_size\": 359, \"raw_average_value_size\": 89, \"num_data_blocks\": 1, \"num_entries\": 4, \"filter_policy_name\": \"rocksdb.BuiltinBloomFilter\", \"kDeletedKeys\": \"0\", \"kMergeOperands\": \"0\"}}", "2018-06-05 20:46:51.123513 7fa3f3bcbf00  4 rocksdb: [/build/ceph-12.2.5/src/rocksdb/db/version_set.cc:2395] Creating manifest 5", "", "2018-06-05 20:46:51.132488 7fa3f3bcbf00  4 rocksdb: EVENT_LOG_v1 {\"time_micros\": 1528231611132485, \"job\": 1, \"event\": \"recovery_finished\"}", "2018-06-05 20:46:51.142873 7fa3f3bcbf00  4 rocksdb: [/build/ceph-12.2.5/src/rocksdb/db/db_impl_open.cc:1063] DB pointer 0x5562dd6f8000", "2018-06-05 20:46:51.144169 7fa3f3bcbf00  0 starting mon.node rank 0 at public addr 128.110.153.197:6789/0 at bind addr 128.110.153.197:6789/0 mon_data /var/lib/ceph/mon/ceph-node fsid b2bb6c0b-6cc6-44f1-bf92-1ce81d78fbdc", "2018-06-05 20:46:51.144370 7fa3f3bcbf00  0 starting mon.node rank 0 at 128.110.153.197:6789/0 mon_data /var/lib/ceph/mon/ceph-node fsid b2bb6c0b-6cc6-44f1-bf92-1ce81d78fbdc", "2018-06-05 20:46:51.145368 7fa3f3bcbf00  1 mon.node@-1(probing) e0 preinit fsid b2bb6c0b-6cc6-44f1-bf92-1ce81d78fbdc", "2018-06-05 20:46:51.146062 7fa3f3bcbf00  1 mon.node@-1(probing).mds e0 Unable to load 'last_metadata'", "2018-06-05 20:46:51.146904 7fa3f3bcbf00  0 mon.node@-1(probing) e0  my rank is now 0 (was -1)", "2018-06-05 20:46:51.146941 7fa3f3bcbf00  1 mon.node@0(probing) e0 win_standalone_election", "2018-06-05 20:46:51.146961 7fa3f3bcbf00  1 mon.node@0(probing).elector(0) init, first boot, initializing epoch at 1 ", "2018-06-05 20:46:51.151879 7fa3f3bcbf00  0 log_channel(cluster) log [INF] : mon.node is new leader, mons node in quorum (ranks 0)", "2018-06-05 20:46:51.156816 7fa3f3bcbf00  1 mon.node@0(leader).osd e0 create_pending setting backfillfull_ratio = 0.9", "2018-06-05 20:46:51.156839 7fa3f3bcbf00  1 mon.node@0(leader).osd e0 create_pending setting full_ratio = 0.95", "2018-06-05 20:46:51.156844 7fa3f3bcbf00  1 mon.node@0(leader).osd e0 create_pending setting nearfull_ratio = 0.85", "2018-06-05 20:46:51.157098 7fa3f3bcbf00  1 mon.node@0(leader).osd e0 encode_pending skipping prime_pg_temp; mapping job did not start", "2018-06-05 20:46:51.157987 7fa3f3bcbf00  1 mon.node@0(leader) e0 _apply_compatset_features enabling new quorum features: compat={},rocompat={},incompat={4=support erasure code pools,5=new-style osdmap encoding,6=support isa/lrc erasure code,7=support shec erasure code}", "2018-06-05 20:46:51.165120 7fa3e61d5700  1 mon.node@0(leader).paxosservice(pgmap 0..0) refresh upgraded, format 1 -> 0", "2018-06-05 20:46:51.165132 7fa3e61d5700  1 mon.node@0(leader).pg v0 on_upgrade discarding in-core PGMap", "2018-06-05 20:46:51.169633 7fa3e61d5700  1 mon.node@0(leader).paxosservice(auth 0..0) refresh upgraded, format 2 -> 0", "2018-06-05 20:46:51.169810 7fa3e61d5700  1 mon.node@0(probing) e1 win_standalone_election", "2018-06-05 20:46:51.169829 7fa3e61d5700  1 mon.node@0(probing).elector(2) init, last seen epoch 2", "2018-06-05 20:46:51.174339 7fa3e61d5700  0 log_channel(cluster) log [INF] : mon.node is new leader, mons node in quorum (ranks 0)", "2018-06-05 20:46:51.178730 7fa3e61d5700  0 log_channel(cluster) log [DBG] : monmap e1: 1 mons at {node=128.110.153.197:6789/0}", "2018-06-05 20:46:51.182745 7fa3e61d5700  1 mon.node@0(leader).osd e0 create_pending setting backfillfull_ratio = 0.9", "2018-06-05 20:46:51.182765 7fa3e61d5700  1 mon.node@0(leader).osd e0 create_pending setting full_ratio = 0.95", "2018-06-05 20:46:51.182771 7fa3e61d5700  1 mon.node@0(leader).osd e0 create_pending setting nearfull_ratio = 0.85", "2018-06-05 20:46:51.183004 7fa3e61d5700  1 mon.node@0(leader).osd e0 encode_pending skipping prime_pg_temp; mapping job did not start", "2018-06-05 20:46:51.183496 7fa3e61d5700  1 mon.node@0(leader) e1 _apply_compatset_features enabling new quorum features: compat={},rocompat={},incompat={8=support monmap features,9=luminous ondisk layout}", "2018-06-05 20:46:51.192055 7fa3e61d5700  1 mon.node@0(leader).paxosservice(auth 0..0) refresh upgraded, format 2 -> 0", "2018-06-05 20:46:51.192171 7fa3e61d5700  0 log_channel(cluster) log [INF] : pgmap 0 pgs: ; 0 bytes data, 0 kB used, 0 kB / 0 kB avail", "2018-06-05 20:46:51.201992 7fa3e61d5700  0 mon.node@0(leader).mds e1 print_map", "e1", "enable_multiple, ever_enabled_multiple: 0,0", "compat: compat={},rocompat={},incompat={1=base v0.20,2=client writeable ranges,3=default file layouts on dirs,4=dir inode in separate object,5=mds uses versioned encoding,6=dirfrag is stored in omap,8=no anchor table,9=file layout v2}", "legacy client fscid: -1", " ", "No filesystems configured", "", "2018-06-05 20:46:51.202098 7fa3e61d5700  1 mon.node@0(leader).osd e1 e1: 0 total, 0 up, 0 in", "2018-06-05 20:46:51.206194 7fa3e61d5700  0 mon.node@0(leader).osd e1 crush map has features 288514050185494528, adjusting msgr requires", "2018-06-05 20:46:51.206204 7fa3e61d5700  0 mon.node@0(leader).osd e1 crush map has features 288514050185494528, adjusting msgr requires", "2018-06-05 20:46:51.206211 7fa3e61d5700  0 mon.node@0(leader).osd e1 crush map has features 1009089990564790272, adjusting msgr requires", "2018-06-05 20:46:51.206214 7fa3e61d5700  0 mon.node@0(leader).osd e1 crush map has features 288514050185494528, adjusting msgr requires", "2018-06-05 20:46:51.206440 7fa3e61d5700  1 mon.node@0(leader).log v1 unable to write to '/var/log/ceph/ceph.log' for channel 'cluster': (2) No such file or directory", "2018-06-05 20:46:51.206502 7fa3e61d5700  1 mon.node@0(leader).paxosservice(auth 1..1) refresh upgraded, format 0 -> 2", "2018-06-05 20:46:51.207103 7fa3e61d5700  0 log_channel(cluster) log [DBG] : fsmap ", "2018-06-05 20:46:51.207184 7fa3e61d5700  1 mon.node@0(leader).pg v1 check_osd_map will clear pg_map data", "2018-06-05 20:46:51.207187 7fa3e61d5700  1 mon.node@0(leader).pg v1 encode_pending clearing pgmap data at v2", "2018-06-05 20:46:51.211449 7fa3e61d5700  0 log_channel(cluster) log [DBG] : osdmap e1: 0 total, 0 up, 0 in", "2018-06-05 20:46:51.211646 7fa3e61d5700  0 log_channel(cluster) log [DBG] : mgrmap e1: no daemons active", "2018-06-05 20:46:51.225999 7fa3e61d5700  1 mon.node@0(leader).log v2 unable to write to '/var/log/ceph/ceph.log' for channel 'cluster': (2) No such file or directory", "2018-06-05 20:46:51.420409 7fa3ea9de700  0 mon.node@0(leader) e1 handle_command mon_command({\"prefix\": \"status\"} v 0) v1", "2018-06-05 20:46:51.420448 7fa3ea9de700  0 log_channel(audit) log [DBG] : from='client.? 128.110.153.138:0/3019500679' entity='client.admin' cmd=[{\"prefix\": \"status\"}]: dispatch", "2018-06-05 20:46:51.710839 7fa3ea9de700  0 mon.node@0(leader) e1 handle_command mon_command({\"pool2\": \"rbd\", \"prefix\": \"osd pool rm\", \"sure\": \"--yes-i-really-really-mean-it\", \"pool\": \"rbd\"} v 0) v1", "2018-06-05 20:46:51.710889 7fa3ea9de700  0 log_channel(audit) log [INF] : from='client.? 128.110.153.138:0/3040884711' entity='client.admin' cmd=[{\"pool2\": \"rbd\", \"prefix\": \"osd pool rm\", \"sure\": \"--yes-i-really-really-mean-it\", \"pool\": \"rbd\"}]: dispatch", "2018-06-05 20:46:52.003364 7fa3ea9de700  0 mon.node@0(leader) e1 handle_command mon_command({\"prefix\": \"osd pool create\", \"pg_num\": 150, \"pool\": \"scbench\", \"pgp_num\": 150} v 0) v1", "2018-06-05 20:46:52.003420 7fa3ea9de700  0 log_channel(audit) log [INF] : from='client.? 128.110.153.138:0/2449834230' entity='client.admin' cmd=[{\"prefix\": \"osd pool create\", \"pg_num\": 150, \"pool\": \"scbench\", \"pgp_num\": 150}]: dispatch", "2018-06-05 20:46:52.188370 7fa3ea9de700  1 mon.node@0(leader).osd e1 encode_pending skipping prime_pg_temp; mapping job did not start", "2018-06-05 20:46:52.198258 7fa3e61d5700  1 mon.node@0(leader).osd e2 e2: 0 total, 0 up, 0 in", "2018-06-05 20:46:52.202873 7fa3e61d5700  0 mon.node@0(leader).osd e2 crush map has features 288514051259236352, adjusting msgr requires", "2018-06-05 20:46:52.202902 7fa3e61d5700  0 mon.node@0(leader).osd e2 crush map has features 288514051259236352, adjusting msgr requires", "2018-06-05 20:46:52.202908 7fa3e61d5700  0 mon.node@0(leader).osd e2 crush map has features 1009089991638532096, adjusting msgr requires", "2018-06-05 20:46:52.202912 7fa3e61d5700  0 mon.node@0(leader).osd e2 crush map has features 288514051259236352, adjusting msgr requires", "2018-06-05 20:46:52.203251 7fa3e61d5700  0 log_channel(audit) log [INF] : from='client.? 128.110.153.138:0/2449834230' entity='client.admin' cmd='[{\"prefix\": \"osd pool create\", \"pg_num\": 150, \"pool\": \"scbench\", \"pgp_num\": 150}]': finished", "2018-06-05 20:46:52.203433 7fa3e61d5700  0 log_channel(cluster) log [DBG] : osdmap e2: 0 total, 0 up, 0 in", "2018-06-05 20:46:52.235121 7fa3e61d5700  1 mon.node@0(leader).log v3 unable to write to '/var/log/ceph/ceph.audit.log' for channel 'audit': (2) No such file or directory", "2018-06-05 20:46:52.235153 7fa3e61d5700  1 mon.node@0(leader).log v3 unable to write to '/var/log/ceph/ceph.log' for channel 'cluster': (2) No such file or directory", "2018-06-05 20:46:52.519482 7fa3ea9de700  0 mon.node@0(leader) e1 handle_command mon_command({\"var\": \"size\", \"prefix\": \"osd pool set\", \"pool\": \"scbench\", \"val\": \"1\"} v 0) v1", "2018-06-05 20:46:52.519560 7fa3ea9de700  0 log_channel(audit) log [INF] : from='client.? 128.110.153.138:0/1066102251' entity='client.admin' cmd=[{\"var\": \"size\", \"prefix\": \"osd pool set\", \"pool\": \"scbench\", \"val\": \"1\"}]: dispatch", "2018-06-05 20:46:53.244865 7fa3e61d5700  1 mon.node@0(leader).log v4 unable to write to '/var/log/ceph/ceph.audit.log' for channel 'audit': (2) No such file or directory", "2018-06-05 20:46:53.254684 7fa3e61d5700  1 mon.node@0(leader).osd e3 e3: 0 total, 0 up, 0 in", "2018-06-05 20:46:53.259708 7fa3e61d5700  0 log_channel(audit) log [INF] : from='client.? 128.110.153.138:0/1066102251' entity='client.admin' cmd='[{\"var\": \"size\", \"prefix\": \"osd pool set\", \"pool\": \"scbench\", \"val\": \"1\"}]': finished", "2018-06-05 20:46:53.259888 7fa3e61d5700  0 log_channel(cluster) log [DBG] : osdmap e3: 0 total, 0 up, 0 in", "2018-06-05 20:46:54.264143 7fa3e61d5700  1 mon.node@0(leader).log v5 unable to write to '/var/log/ceph/ceph.audit.log' for channel 'audit': (2) No such file or directory", "2018-06-05 20:47:51.146455 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB", "2018-06-05 20:48:51.146894 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB", "2018-06-05 20:49:51.147295 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB", "2018-06-05 20:50:51.147738 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB", "2018-06-05 20:51:51.148184 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB", "2018-06-05 20:52:51.148586 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB", "2018-06-05 20:53:51.149009 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB", "2018-06-05 20:54:51.149427 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB", "2018-06-05 20:55:51.149744 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB", "2018-06-05 20:56:51.150182 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB", "2018-06-05 20:57:51.150596 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB", "2018-06-05 20:58:51.151036 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB", "2018-06-05 20:59:51.151455 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB", "2018-06-05 21:00:51.151862 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB", "2018-06-05 21:01:51.152272 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB", "2018-06-05 21:02:51.152670 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB", "2018-06-05 21:03:51.153176 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB", "2018-06-05 21:04:51.153577 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB", "2018-06-05 21:05:51.154043 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB", "2018-06-05 21:06:51.154486 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB", "2018-06-05 21:07:51.154907 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB", "2018-06-05 21:08:51.155322 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB", "2018-06-05 21:09:51.155786 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB", "2018-06-05 21:10:51.156239 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB", "2018-06-05 21:11:51.156697 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB", "2018-06-05 21:12:51.157235 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB", "2018-06-05 21:13:51.157578 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB", "2018-06-05 21:14:51.157995 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB", "2018-06-05 21:15:51.158413 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB", "2018-06-05 21:16:51.159008 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB", "2018-06-05 21:17:51.159403 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB", "2018-06-05 21:18:51.159836 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB", "2018-06-05 21:19:51.160240 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB", "2018-06-05 21:20:51.160755 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB", "2018-06-05 21:21:51.161225 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB", "2018-06-05 21:22:51.161818 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB", "2018-06-05 21:23:51.162213 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB", "2018-06-05 21:24:51.162648 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB", "2018-06-05 21:25:51.163040 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB", "2018-06-05 21:26:51.163439 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB", "2018-06-05 21:27:51.163834 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB", "2018-06-05 21:28:51.164233 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB", "2018-06-05 21:29:51.164623 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB", "2018-06-05 21:30:51.165050 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB", "2018-06-05 21:31:51.165667 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB", "2018-06-05 21:32:51.166088 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB", "2018-06-05 21:33:51.166524 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB", "2018-06-05 21:34:51.166909 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB", "2018-06-05 21:35:51.167298 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB", "2018-06-05 21:36:51.167844 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB", "2018-06-05 21:37:51.168251 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12380 MB", "2018-06-05 21:38:51.168873 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB", "2018-06-05 21:39:51.169280 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB", "2018-06-05 21:40:51.169738 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB", "2018-06-05 21:41:51.170135 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB", "2018-06-05 21:42:51.170577 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB", "2018-06-05 21:43:51.170974 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB", "2018-06-05 21:44:51.171402 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB", "2018-06-05 21:45:51.171847 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB", "2018-06-05 21:46:51.172492 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB", "2018-06-05 21:47:51.172897 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB", "2018-06-05 21:48:51.173296 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB", "2018-06-05 21:49:51.173728 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB", "2018-06-05 21:50:51.174158 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB", "2018-06-05 21:51:51.174553 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB", "2018-06-05 21:52:51.174945 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB", "2018-06-05 21:53:51.175346 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB", "2018-06-05 21:54:51.175960 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB", "2018-06-05 21:55:51.176364 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB", "2018-06-05 21:56:51.176756 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB", "2018-06-05 21:57:51.177148 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB", "2018-06-05 21:58:51.177540 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB", "2018-06-05 21:59:51.177950 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB", "2018-06-05 22:00:51.178360 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB", "2018-06-05 22:01:51.178830 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB", "2018-06-05 22:02:51.179298 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB", "2018-06-05 22:03:51.179713 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12381 MB", "2018-06-05 22:04:51.180152 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12380 MB", "2018-06-05 22:05:51.180557 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12380 MB", "2018-06-05 22:06:51.180938 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12380 MB", "2018-06-05 22:07:51.181332 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12380 MB", "2018-06-05 22:08:51.181789 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12380 MB", "2018-06-05 22:09:51.182156 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12380 MB", "2018-06-05 22:10:51.182584 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12380 MB", "2018-06-05 22:11:51.182988 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12380 MB", "2018-06-05 22:12:51.183383 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12380 MB", "2018-06-05 22:13:51.183786 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12380 MB", "2018-06-05 22:14:51.184188 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12380 MB", "2018-06-05 22:15:51.184631 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12380 MB", "2018-06-05 22:16:51.185046 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12380 MB", "2018-06-05 22:17:51.185486 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12380 MB", "2018-06-05 22:18:51.185918 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12380 MB", "2018-06-05 22:19:51.186320 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12380 MB", "2018-06-05 22:20:51.186733 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12380 MB", "2018-06-05 22:21:51.187168 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12380 MB", "2018-06-05 22:22:51.187597 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12380 MB", "2018-06-05 22:23:51.187977 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12380 MB", "2018-06-05 22:24:51.188388 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12380 MB", "2018-06-05 22:25:51.188851 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12380 MB", "2018-06-05 22:26:51.189284 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12380 MB", "2018-06-05 22:27:51.189762 7fa3ed1e3700  0 mon.node@0(leader).data_health(3) update_stats avail 77% total 15998 MB, used 2798 MB, avail 12380 MB", "2018-06-05 22:27:52.459108 7fa3e81d9700 -1 received  signal: Terminated from  PID: 1 task name:  UID: 0", "2018-06-05 22:27:52.459137 7fa3e81d9700 -1 mon.node@0(leader) e1 *** Got Signal Terminated ***", "2018-06-05 22:27:52.459144 7fa3e81d9700  1 mon.node@0(leader) e1 shutdown"], "stdout": "creating /var/lib/ceph/bootstrap-osd/ceph.keyring\ncreating /var/lib/ceph/bootstrap-mds/ceph.keyring\ncreating /var/lib/ceph/bootstrap-rgw/ceph.keyring\ncreating /var/lib/ceph/bootstrap-rbd/ceph.keyring\nmonmaptool: monmap file /etc/ceph/monmap-ceph\nmonmaptool: set fsid to b2bb6c0b-6cc6-44f1-bf92-1ce81d78fbdc\nmonmaptool: writing epoch 0 to /etc/ceph/monmap-ceph (1 monitors)\nimporting contents of /var/lib/ceph/bootstrap-osd/ceph.keyring into /etc/ceph/ceph.mon.keyring\nimporting contents of /var/lib/ceph/bootstrap-mds/ceph.keyring into /etc/ceph/ceph.mon.keyring\nimporting contents of /var/lib/ceph/bootstrap-rgw/ceph.keyring into /etc/ceph/ceph.mon.keyring\nimporting contents of /var/lib/ceph/bootstrap-rbd/ceph.keyring into /etc/ceph/ceph.mon.keyring\nimporting contents of /etc/ceph/ceph.client.admin.keyring into /etc/ceph/ceph.mon.keyring\n2018-06-05 20:46:51  /entrypoint.sh: SUCCESS\nexec: PID 122: spawning /usr/bin/ceph-mon --cluster ceph --setuser ceph --setgroup ceph -d -i node --mon-data /var/lib/ceph/mon/ceph-node --public-addr 128.110.153.197:6789\nSending SIGTERM to PID 122", "stdout_lines": ["creating /var/lib/ceph/bootstrap-osd/ceph.keyring", "creating /var/lib/ceph/bootstrap-mds/ceph.keyring", "creating /var/lib/ceph/bootstrap-rgw/ceph.keyring", "creating /var/lib/ceph/bootstrap-rbd/ceph.keyring", "monmaptool: monmap file /etc/ceph/monmap-ceph", "monmaptool: set fsid to b2bb6c0b-6cc6-44f1-bf92-1ce81d78fbdc", "monmaptool: writing epoch 0 to /etc/ceph/monmap-ceph (1 monitors)", "importing contents of /var/lib/ceph/bootstrap-osd/ceph.keyring into /etc/ceph/ceph.mon.keyring", "importing contents of /var/lib/ceph/bootstrap-mds/ceph.keyring into /etc/ceph/ceph.mon.keyring", "importing contents of /var/lib/ceph/bootstrap-rgw/ceph.keyring into /etc/ceph/ceph.mon.keyring", "importing contents of /var/lib/ceph/bootstrap-rbd/ceph.keyring into /etc/ceph/ceph.mon.keyring", "importing contents of /etc/ceph/ceph.client.admin.keyring into /etc/ceph/ceph.mon.keyring", "2018-06-05 20:46:51  /entrypoint.sh: SUCCESS", "exec: PID 122: spawning /usr/bin/ceph-mon --cluster ceph --setuser ceph --setgroup ceph -d -i node --mon-data /var/lib/ceph/mon/ceph-node --public-addr 128.110.153.197:6789", "Sending SIGTERM to PID 122"]}
2018-06-05 22:28:05,474 p=10 u=root |  ...ignoring
2018-06-05 22:28:05,485 p=10 u=root |  TASK [baseliner : set fact to store result of bench execution] *****************
2018-06-05 22:28:05,486 p=10 u=root |  Tuesday 05 June 2018  22:28:05 +0000 (0:00:00.982)       1:41:33.662 ********** 
2018-06-05 22:28:05,503 p=10 u=root |  ok: [node1 -> localhost]
2018-06-05 22:28:05,513 p=10 u=root |  TASK [baseliner : record elapsed time in output folder] ************************
2018-06-05 22:28:05,514 p=10 u=root |  Tuesday 05 June 2018  22:28:05 +0000 (0:00:00.028)       1:41:33.690 ********** 
2018-06-05 22:28:05,733 p=10 u=root |  changed: [node2 -> localhost]
2018-06-05 22:28:05,740 p=10 u=root |  changed: [node1 -> localhost]
2018-06-05 22:28:05,750 p=10 u=root |  changed: [node3 -> localhost]
2018-06-05 22:28:05,758 p=10 u=root |  TASK [baseliner : capture stdout] **********************************************
2018-06-05 22:28:05,759 p=10 u=root |  Tuesday 05 June 2018  22:28:05 +0000 (0:00:00.244)       1:41:33.935 ********** 
2018-06-05 22:28:05,974 p=10 u=root |  changed: [node1 -> localhost]
2018-06-05 22:28:05,980 p=10 u=root |  changed: [node2 -> localhost]
2018-06-05 22:28:05,981 p=10 u=root |  changed: [node3 -> localhost]
2018-06-05 22:28:05,991 p=10 u=root |  TASK [baseliner : capture stderr] **********************************************
2018-06-05 22:28:05,991 p=10 u=root |  Tuesday 05 June 2018  22:28:05 +0000 (0:00:00.232)       1:41:34.168 ********** 
2018-06-05 22:28:06,215 p=10 u=root |  changed: [node3 -> localhost]
2018-06-05 22:28:06,222 p=10 u=root |  changed: [node2 -> localhost]
2018-06-05 22:28:06,222 p=10 u=root |  changed: [node1 -> localhost]
2018-06-05 22:28:06,229 p=10 u=root |  TASK [baseliner : debug] *******************************************************
2018-06-05 22:28:06,229 p=10 u=root |  Tuesday 05 June 2018  22:28:06 +0000 (0:00:00.237)       1:41:34.406 ********** 
2018-06-05 22:28:06,264 p=10 u=root |  TASK [baseliner : run compose benchmark] ***************************************
2018-06-05 22:28:06,264 p=10 u=root |  Tuesday 05 June 2018  22:28:06 +0000 (0:00:00.035)       1:41:34.441 ********** 
2018-06-05 22:28:06,300 p=10 u=root |  TASK [baseliner : run script benchmark] ****************************************
2018-06-05 22:28:06,301 p=10 u=root |  Tuesday 05 June 2018  22:28:06 +0000 (0:00:00.036)       1:41:34.477 ********** 
2018-06-05 22:28:06,339 p=10 u=root |  TASK [baseliner : download results] ********************************************
2018-06-05 22:28:06,339 p=10 u=root |  Tuesday 05 June 2018  22:28:06 +0000 (0:00:00.038)       1:41:34.516 ********** 
2018-06-05 22:28:06,394 p=10 u=root |  included: /etc/ansible/roles/baseliner/tasks/download-output.yml for node1, node2, node3
2018-06-05 22:28:06,481 p=10 u=root |  TASK [baseliner : get any bench-specific files] ********************************
2018-06-05 22:28:06,482 p=10 u=root |  Tuesday 05 June 2018  22:28:06 +0000 (0:00:00.142)       1:41:34.658 ********** 
2018-06-05 22:28:07,826 p=10 u=root |  changed: [node1] => (item=/tmp/baseliner_output_node1)
2018-06-05 22:28:07,860 p=10 u=root |  changed: [node2] => (item=/tmp/baseliner_output_node2)
2018-06-05 22:28:07,870 p=10 u=root |  changed: [node3] => (item=/tmp/baseliner_output_node3)
2018-06-05 22:28:07,877 p=10 u=root |  TASK [baseliner : delete remote folder after transfer] *************************
2018-06-05 22:28:07,877 p=10 u=root |  Tuesday 05 June 2018  22:28:07 +0000 (0:00:01.395)       1:41:36.054 ********** 
2018-06-05 22:28:08,715 p=10 u=root |  changed: [node1] => (item=/tmp/baseliner_output_node1)
2018-06-05 22:28:08,720 p=10 u=root |  changed: [node3] => (item=/tmp/baseliner_output_node3)
2018-06-05 22:28:08,721 p=10 u=root |  changed: [node2] => (item=/tmp/baseliner_output_node2)
2018-06-05 22:28:08,728 p=10 u=root |  TASK [baseliner : check if we should fail fast] ********************************
2018-06-05 22:28:08,729 p=10 u=root |  Tuesday 05 June 2018  22:28:08 +0000 (0:00:00.851)       1:41:36.905 ********** 
2018-06-05 22:28:08,784 p=10 u=root |  included: /etc/ansible/roles/baseliner/tasks/fail-fast.yml for node1, node2, node3
2018-06-05 22:28:08,879 p=10 u=root |  TASK [baseliner : failfast for single-node mode] *******************************
2018-06-05 22:28:08,879 p=10 u=root |  Tuesday 05 June 2018  22:28:08 +0000 (0:00:00.150)       1:41:37.056 ********** 
2018-06-05 22:28:08,933 p=10 u=root |  fatal: [node1]: FAILED! => {"changed": false, "failed": true, "msg": "benchmark failed"}
2018-06-05 22:28:08,934 p=10 u=root |  NO MORE HOSTS LEFT *************************************************************
2018-06-05 22:28:08,934 p=10 u=root |  PLAY RECAP *********************************************************************
2018-06-05 22:28:08,934 p=10 u=root |  node1                      : ok=50   changed=17   unreachable=0    failed=1   
2018-06-05 22:28:08,935 p=10 u=root |  node2                      : ok=43   changed=14   unreachable=0    failed=0   
2018-06-05 22:28:08,935 p=10 u=root |  node3                      : ok=43   changed=14   unreachable=0    failed=0   
2018-06-05 22:28:08,935 p=10 u=root |  Tuesday 05 June 2018  22:28:08 +0000 (0:00:00.055)       1:41:37.112 ********** 
2018-06-05 22:28:08,935 p=10 u=root |  =============================================================================== 
2018-06-05 22:28:08,936 p=10 u=root |  baseliner : wait for containers in single-node mode and stop/kill them if they timeout  6073.25s
2018-06-05 22:28:08,936 p=10 u=root |  Gathering Facts --------------------------------------------------------- 2.97s
2018-06-05 22:28:08,936 p=10 u=root |  baseliner : pull image -------------------------------------------------- 2.77s
2018-06-05 22:28:08,936 p=10 u=root |  baseliner : run container ----------------------------------------------- 2.03s
2018-06-05 22:28:08,936 p=10 u=root |  baseliner : install facter ---------------------------------------------- 1.48s
2018-06-05 22:28:08,936 p=10 u=root |  baseliner : upload files ------------------------------------------------ 1.48s
2018-06-05 22:28:08,936 p=10 u=root |  baseliner : get any bench-specific files -------------------------------- 1.40s
2018-06-05 22:28:08,936 p=10 u=root |  baseliner : stop any running container ---------------------------------- 1.01s
2018-06-05 22:28:08,936 p=10 u=root |  baseliner : get result of container execution --------------------------- 0.98s
2018-06-05 22:28:08,936 p=10 u=root |  baseliner : check if image already exists ------------------------------- 0.88s
2018-06-05 22:28:08,937 p=10 u=root |  baseliner : delete remote folder after transfer ------------------------- 0.85s
2018-06-05 22:28:08,937 p=10 u=root |  baseliner : remove containers to avoid name clashes --------------------- 0.84s
2018-06-05 22:28:08,937 p=10 u=root |  baseliner : create remote results folder -------------------------------- 0.84s
2018-06-05 22:28:08,937 p=10 u=root |  baseliner : remove remote results folder -------------------------------- 0.83s
2018-06-05 22:28:08,937 p=10 u=root |  baseliner : store facts about remotes ----------------------------------- 0.76s
2018-06-05 22:28:08,937 p=10 u=root |  baseliner : ensure results folder exists -------------------------------- 0.29s
2018-06-05 22:28:08,937 p=10 u=root |  baseliner : record elapsed time in output folder ------------------------ 0.24s
2018-06-05 22:28:08,937 p=10 u=root |  baseliner : capture stderr ---------------------------------------------- 0.24s
2018-06-05 22:28:08,937 p=10 u=root |  baseliner : add host-specific environment to docker_flags --------------- 0.23s
2018-06-05 22:28:08,937 p=10 u=root |  baseliner : capture stdout ---------------------------------------------- 0.23s
2018-06-05 22:28:08,937 p=10 u=root |  Playbook run took 0 days, 1 hours, 41 minutes, 37 seconds
